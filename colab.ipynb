{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYaNir-4TuJk",
        "outputId": "42a7c961-43d4-4c1c-aaee-27ba0f8e0144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHTmcGmTT4eC",
        "outputId": "7f74b73c-88b3-40d2-bc0e-6b6d4833e7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=\n"
          ]
        }
      ],
      "source": [
        "%env PYTHONPATH="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8_hwtDiYz2_"
      },
      "source": [
        "Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ag4asCLzT9ap",
        "outputId": "8c60bf5d-2cd3-458b-9d53-d53874d2c3e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.27.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.3.6)\n",
            "Downloading virtualenv-20.27.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.9 virtualenv-20.27.1\n",
            "created virtual environment CPython3.10.12.final.0-64 in 1026ms\n",
            "  creator CPython3Posix(dest=/content/dassl, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==24.3.1, setuptools==75.2.0, wheel==0.44.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "--2024-11-08 01:44:37--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.191.158, 104.16.32.241, 2606:4700::6810:20f1, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.191.158|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 148337011 (141M) [application/octet-stream]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>] 141.46M   313MB/s    in 0.5s    \n",
            "\n",
            "2024-11-08 01:44:37 (313 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [148337011/148337011]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "installation finished.\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/dassl\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    pip-24.2                   |   py38h06a4308_0         2.2 MB\n",
            "    python-3.8.20              |       he870216_0        23.8 MB\n",
            "    setuptools-75.1.0          |   py38h06a4308_0         1.7 MB\n",
            "    wheel-0.44.0               |   py38h06a4308_0         108 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        27.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2024.9.24-h06a4308_0 \n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0 \n",
            "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1 \n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n",
            "  openssl            pkgs/main/linux-64::openssl-3.0.15-h5eee18b_0 \n",
            "  pip                pkgs/main/linux-64::pip-24.2-py38h06a4308_0 \n",
            "  python             pkgs/main/linux-64::python-3.8.20-he870216_0 \n",
            "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n",
            "  setuptools         pkgs/main/linux-64::setuptools-75.1.0-py38h06a4308_0 \n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.45.3-h5eee18b_0 \n",
            "  tk                 pkgs/main/linux-64::tk-8.6.14-h39e8969_0 \n",
            "  wheel              pkgs/main/linux-64::wheel-0.44.0-py38h06a4308_0 \n",
            "  xz                 pkgs/main/linux-64::xz-5.4.6-h5eee18b_1 \n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.8.20        | 23.8 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "pip-24.2             | 2.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "setuptools-75.1.0    | 1.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "python-3.8.20        | 23.8 MB   | :  11% 0.10646653275825588/1 [00:00<00:00,  1.06it/s]\n",
            "pip-24.2             | 2.2 MB    | : 100% 0.99731992374216/1 [00:00<00:00,  9.94it/s]\u001b[A\n",
            "\n",
            "setuptools-75.1.0    | 1.7 MB    | :  80% 0.7954358903141382/1 [00:00<00:00,  7.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "wheel-0.44.0         | 108 KB    | : 100% 1.0/1 [00:00<00:00,  5.68it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "python-3.8.20        | 23.8 MB   | :  82% 0.8175578194522859/1 [00:00<00:00,  2.11it/s]\n",
            "pip-24.2             | 2.2 MB    | : 100% 1.0/1 [00:00<00:00,  9.94it/s]             \u001b[A\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate dassl\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Python 3.12.7\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!pip install virtualenv\n",
        "!virtualenv dassl\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!/usr/local/bin/conda create -n dassl python=3.8 -y\n",
        "# !conda install -q -y --prefix /usr/local python=3.8 ujson\n",
        "import os\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local/envs/dassl'\n",
        "import sys\n",
        "sys.path.append('/usr/local/envs/dassl/python3.8/site-packages/')\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1ieJE8jGhK8k",
        "outputId": "664e6d6e-4910-4fdd-dfa7-8d2329744fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Capstone-ood'...\n",
            "remote: Enumerating objects: 121194, done.\u001b[K\n",
            "remote: Counting objects: 100% (576/576), done.\u001b[K\n",
            "remote: Compressing objects: 100% (441/441), done.\u001b[K\n",
            "remote: Total 121194 (delta 123), reused 550 (delta 102), pack-reused 120618 (from 1)\u001b[K\n",
            "Receiving objects: 100% (121194/121194), 495.23 MiB | 16.30 MiB/s, done.\n",
            "Resolving deltas: 100% (1231/1231), done.\n",
            "Updating files: 100% (120630/120630), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ql2501/Capstone-ood.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qh9GEYbNZYmN",
        "outputId": "b2d7150a-4165-49c0-bf67-196db556cd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - pytorch\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/dassl\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit=10.2\n",
            "    - pytorch\n",
            "    - torchvision\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    blas-1.0                   |              mkl           6 KB\n",
            "    brotli-python-1.0.9        |   py38h6a678d5_8         356 KB\n",
            "    certifi-2024.8.30          |   py38h06a4308_0         162 KB\n",
            "    cudatoolkit-10.2.89        |       hfd86e86_1       365.1 MB\n",
            "    ffmpeg-4.3                 |       hf484d3e_0         9.9 MB  pytorch\n",
            "    filelock-3.13.1            |   py38h06a4308_0          21 KB\n",
            "    freetype-2.12.1            |       h4a9f257_0         626 KB\n",
            "    giflib-5.2.2               |       h5eee18b_0          80 KB\n",
            "    gmp-6.2.1                  |       h295c915_3         544 KB\n",
            "    gmpy2-2.1.2                |   py38heeb90bb_0         191 KB\n",
            "    gnutls-3.6.15              |       he1e5248_0         1.0 MB\n",
            "    idna-3.7                   |   py38h06a4308_0         113 KB\n",
            "    intel-openmp-2023.1.0      |   hdb19cb5_46306        17.2 MB\n",
            "    jinja2-3.1.4               |   py38h06a4308_0         275 KB\n",
            "    jpeg-9e                    |       h5eee18b_3         262 KB\n",
            "    lame-3.100                 |       h7b6447c_0         323 KB\n",
            "    lcms2-2.12                 |       h3be6417_0         312 KB\n",
            "    lerc-3.0                   |       h295c915_0         196 KB\n",
            "    libdeflate-1.17            |       h5eee18b_1          64 KB\n",
            "    libiconv-1.16              |       h5eee18b_3         759 KB\n",
            "    libidn2-2.3.4              |       h5eee18b_0         146 KB\n",
            "    libjpeg-turbo-2.0.0        |       h9bf148f_0         950 KB  pytorch\n",
            "    libpng-1.6.39              |       h5eee18b_0         304 KB\n",
            "    libtasn1-4.19.0            |       h5eee18b_0          63 KB\n",
            "    libtiff-4.5.1              |       h6a678d5_0         533 KB\n",
            "    libunistring-0.9.10        |       h27cfd23_0         536 KB\n",
            "    libwebp-1.3.2              |       h11a3e52_0          87 KB\n",
            "    libwebp-base-1.3.2         |       h5eee18b_1         425 KB\n",
            "    llvm-openmp-14.0.6         |       h9e868ea_0         4.4 MB\n",
            "    markupsafe-2.1.3           |   py38h5eee18b_0          22 KB\n",
            "    mkl-2023.1.0               |   h213fc3f_46344       171.5 MB\n",
            "    mkl-service-2.4.0          |   py38h5eee18b_1          54 KB\n",
            "    mkl_fft-1.3.8              |   py38h5eee18b_0         221 KB\n",
            "    mkl_random-1.2.4           |   py38hdb19cb5_0         327 KB\n",
            "    mpc-1.1.0                  |       h10f8cd9_1          90 KB\n",
            "    mpfr-4.0.2                 |       hb69a4c5_1         487 KB\n",
            "    mpmath-1.3.0               |   py38h06a4308_0         832 KB\n",
            "    nettle-3.7.3               |       hbbd107a_1         809 KB\n",
            "    networkx-3.1               |   py38h06a4308_0         2.7 MB\n",
            "    numpy-1.24.3               |   py38hf6e8229_1          10 KB\n",
            "    numpy-base-1.24.3          |   py38h060ed82_1         6.2 MB\n",
            "    openh264-2.1.1             |       h4ff587b_0         711 KB\n",
            "    openjpeg-2.5.2             |       he7f1fd0_0         371 KB\n",
            "    pillow-10.4.0              |   py38h5eee18b_0         795 KB\n",
            "    pysocks-1.7.1              |   py38h06a4308_0          31 KB\n",
            "    pytorch-2.4.1              |      py3.8_cpu_0        85.1 MB  pytorch\n",
            "    pytorch-mutex-1.0          |              cpu           3 KB  pytorch\n",
            "    pyyaml-6.0.2               |   py38h5eee18b_0         196 KB\n",
            "    requests-2.32.3            |   py38h06a4308_0         100 KB\n",
            "    sympy-1.13.2               |   py38h06a4308_0        11.2 MB\n",
            "    tbb-2021.8.0               |       hdb19cb5_0         1.6 MB\n",
            "    torchvision-0.20.0         |         py38_cpu        11.8 MB  pytorch\n",
            "    typing_extensions-4.11.0   |   py38h06a4308_0          59 KB\n",
            "    urllib3-2.2.3              |   py38h06a4308_0         181 KB\n",
            "    yaml-0.2.5                 |       h7b6447c_0          75 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       700.3 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               pkgs/main/linux-64::blas-1.0-mkl \n",
            "  brotli-python      pkgs/main/linux-64::brotli-python-1.0.9-py38h6a678d5_8 \n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6 \n",
            "  certifi            pkgs/main/linux-64::certifi-2024.8.30-py38h06a4308_0 \n",
            "  charset-normalizer pkgs/main/noarch::charset-normalizer-3.3.2-pyhd3eb1b0_0 \n",
            "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.2.89-hfd86e86_1 \n",
            "  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0 \n",
            "  filelock           pkgs/main/linux-64::filelock-3.13.1-py38h06a4308_0 \n",
            "  freetype           pkgs/main/linux-64::freetype-2.12.1-h4a9f257_0 \n",
            "  giflib             pkgs/main/linux-64::giflib-5.2.2-h5eee18b_0 \n",
            "  gmp                pkgs/main/linux-64::gmp-6.2.1-h295c915_3 \n",
            "  gmpy2              pkgs/main/linux-64::gmpy2-2.1.2-py38heeb90bb_0 \n",
            "  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0 \n",
            "  idna               pkgs/main/linux-64::idna-3.7-py38h06a4308_0 \n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2023.1.0-hdb19cb5_46306 \n",
            "  jinja2             pkgs/main/linux-64::jinja2-3.1.4-py38h06a4308_0 \n",
            "  jpeg               pkgs/main/linux-64::jpeg-9e-h5eee18b_3 \n",
            "  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0 \n",
            "  lcms2              pkgs/main/linux-64::lcms2-2.12-h3be6417_0 \n",
            "  lerc               pkgs/main/linux-64::lerc-3.0-h295c915_0 \n",
            "  libdeflate         pkgs/main/linux-64::libdeflate-1.17-h5eee18b_1 \n",
            "  libiconv           pkgs/main/linux-64::libiconv-1.16-h5eee18b_3 \n",
            "  libidn2            pkgs/main/linux-64::libidn2-2.3.4-h5eee18b_0 \n",
            "  libjpeg-turbo      pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0 \n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0 \n",
            "  libtasn1           pkgs/main/linux-64::libtasn1-4.19.0-h5eee18b_0 \n",
            "  libtiff            pkgs/main/linux-64::libtiff-4.5.1-h6a678d5_0 \n",
            "  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0 \n",
            "  libwebp            pkgs/main/linux-64::libwebp-1.3.2-h11a3e52_0 \n",
            "  libwebp-base       pkgs/main/linux-64::libwebp-base-1.3.2-h5eee18b_1 \n",
            "  llvm-openmp        pkgs/main/linux-64::llvm-openmp-14.0.6-h9e868ea_0 \n",
            "  lz4-c              pkgs/main/linux-64::lz4-c-1.9.4-h6a678d5_1 \n",
            "  markupsafe         pkgs/main/linux-64::markupsafe-2.1.3-py38h5eee18b_0 \n",
            "  mkl                pkgs/main/linux-64::mkl-2023.1.0-h213fc3f_46344 \n",
            "  mkl-service        pkgs/main/linux-64::mkl-service-2.4.0-py38h5eee18b_1 \n",
            "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.8-py38h5eee18b_0 \n",
            "  mkl_random         pkgs/main/linux-64::mkl_random-1.2.4-py38hdb19cb5_0 \n",
            "  mpc                pkgs/main/linux-64::mpc-1.1.0-h10f8cd9_1 \n",
            "  mpfr               pkgs/main/linux-64::mpfr-4.0.2-hb69a4c5_1 \n",
            "  mpmath             pkgs/main/linux-64::mpmath-1.3.0-py38h06a4308_0 \n",
            "  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1 \n",
            "  networkx           pkgs/main/linux-64::networkx-3.1-py38h06a4308_0 \n",
            "  numpy              pkgs/main/linux-64::numpy-1.24.3-py38hf6e8229_1 \n",
            "  numpy-base         pkgs/main/linux-64::numpy-base-1.24.3-py38h060ed82_1 \n",
            "  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0 \n",
            "  openjpeg           pkgs/main/linux-64::openjpeg-2.5.2-he7f1fd0_0 \n",
            "  pillow             pkgs/main/linux-64::pillow-10.4.0-py38h5eee18b_0 \n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py38h06a4308_0 \n",
            "  pytorch            pytorch/linux-64::pytorch-2.4.1-py3.8_cpu_0 \n",
            "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cpu \n",
            "  pyyaml             pkgs/main/linux-64::pyyaml-6.0.2-py38h5eee18b_0 \n",
            "  requests           pkgs/main/linux-64::requests-2.32.3-py38h06a4308_0 \n",
            "  sympy              pkgs/main/linux-64::sympy-1.13.2-py38h06a4308_0 \n",
            "  tbb                pkgs/main/linux-64::tbb-2021.8.0-hdb19cb5_0 \n",
            "  torchvision        pytorch/linux-64::torchvision-0.20.0-py38_cpu \n",
            "  typing_extensions  pkgs/main/linux-64::typing_extensions-4.11.0-py38h06a4308_0 \n",
            "  urllib3            pkgs/main/linux-64::urllib3-2.2.3-py38h06a4308_0 \n",
            "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0 \n",
            "  zstd               pkgs/main/linux-64::zstd-1.5.6-hc292b87_0 \n",
            "\n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   0% 0/1 [00:00<?, ?it/s]\n",
            "mkl-2023.1.0         | 171.5 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "intel-openmp-2023.1. | 17.2 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "torchvision-0.20.0   | 11.8 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.2         | 11.2 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ffmpeg-4.3           | 9.9 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "numpy-base-1.24.3    | 6.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-14.0.6   | 4.4 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.1         | 2.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tbb-2021.8.0         | 1.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.6.15        | 1.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libjpeg-turbo-2.0.0  | 950 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mpmath-1.3.0         | 832 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "nettle-3.7.3         | 809 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.4.0        | 795 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libiconv-1.16        | 759 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openh264-2.1.1       | 711 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "freetype-2.12.1      | 626 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2023.1.0         | 171.5 MB  | :   3% 0.03242711343137378/1 [00:00<00:02,  3.08s/it]\u001b[A\n",
            "\n",
            "\n",
            "intel-openmp-2023.1. | 17.2 MB   | :  18% 0.18374087400676065/1 [00:00<00:00,  1.84it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "torchvision-0.20.0   | 11.8 MB   | :   0% 0.001320028524346848/1 [00:00<01:26, 86.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :   0% 0.00018354434229774102/1 [00:00<15:48, 949.08s/it]\u001b[A\u001b[A\n",
            "mkl-2023.1.0         | 171.5 MB  | :   7% 0.07469166576889466/1 [00:00<00:02,  2.62s/it]\u001b[A\n",
            "\n",
            "\n",
            "intel-openmp-2023.1. | 17.2 MB   | :  56% 0.5630475297533903/1 [00:00<00:00,  2.99it/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   0% 4.280133806742674e-05/1 [00:00<1:46:36, 6396.95s/it]\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :   3% 0.026430385290874707/1 [00:00<00:08,  8.50s/it]   \u001b[A\u001b[Ay\n",
            "\n",
            "\n",
            "\n",
            "intel-openmp-2023.1. | 17.2 MB   | :  88% 0.8786816053986674/1 [00:00<00:00,  3.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "torchvision-0.20.0   | 11.8 MB   | :  94% 0.9398603093349559/1 [00:00<00:00,  3.56it/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   0% 0.004151729792540394/1 [00:00<01:10, 70.67s/it]     \n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :   8% 0.07855697850343316/1 [00:00<00:03,  3.67s/it] \u001b[A\u001b[A\n",
            "mkl-2023.1.0         | 171.5 MB  | :  15% 0.15402878879902546/1 [00:00<00:02,  2.68s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   1% 0.008132254232811081/1 [00:00<00:45, 45.60s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  19% 0.1916478838753102/1 [00:00<00:02,  2.79s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.2         | 11.2 MB   | :   0% 0.0013900384873693488/1 [00:00<06:34, 394.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   1% 0.011984374658879488/1 [00:00<00:36, 37.24s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   2% 0.016392912479824443/1 [00:00<00:31, 31.52s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ffmpeg-4.3           | 9.9 MB    | :   0% 0.0015713236502815128/1 [00:00<07:19, 440.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2023.1.0         | 171.5 MB  | :  23% 0.2278095805951287/1 [00:00<00:02,  3.87s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.2         | 11.2 MB   | :  46% 0.4642728547813625/1 [00:00<00:00,  1.28s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   2% 0.02152907304791565/1 [00:00<00:26, 27.45s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ffmpeg-4.3           | 9.9 MB    | :  25% 0.245126489443916/1 [00:00<00:01,  2.47s/it]     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   3% 0.027392856363153115/1 [00:00<00:23, 24.24s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ffmpeg-4.3           | 9.9 MB    | :  50% 0.5043948917403657/1 [00:00<00:00,  1.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  30% 0.30394943084505915/1 [00:00<00:01,  2.81s/it]\u001b[A\u001b[A\n",
            "mkl-2023.1.0         | 171.5 MB  | :  26% 0.2572308099162909/1 [00:00<00:03,  4.51s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.2         | 11.2 MB   | :  94% 0.9382759789743104/1 [00:01<00:00,  1.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   3% 0.033984262425536836/1 [00:01<00:20, 21.17s/it]\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  34% 0.3439620974659667/1 [00:01<00:01,  2.84s/it] \u001b[A\u001b[A\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   4% 0.0417741059538085/1 [00:01<00:17, 18.31s/it]  \n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  38% 0.38213932066389683/1 [00:01<00:01,  2.86s/it]\u001b[A\u001b[A\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   5% 0.0473810812406414/1 [00:01<00:18, 19.75s/it]\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  42% 0.4192152778080405/1 [00:01<00:01,  3.00s/it] \u001b[A\u001b[A\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   6% 0.058894641180779195/1 [00:01<00:14, 15.28s/it]\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  46% 0.45555705758299325/1 [00:01<00:01,  2.98s/it]\u001b[A\u001b[A\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   7% 0.06582845794770233/1 [00:01<00:14, 15.20s/it] \n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  50% 0.496120357230794/1 [00:01<00:01,  2.91s/it]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "numpy-base-1.24.3    | 6.2 MB    | :   0% 0.002510203062312911/1 [00:01<10:00, 601.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   7% 0.0725054666862209/1 [00:01<00:14, 15.54s/it] \n",
            "mkl-2023.1.0         | 171.5 MB  | :  37% 0.3659891622675838/1 [00:01<00:03,  5.86s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "numpy-base-1.24.3    | 6.2 MB    | :  38% 0.3790406624092495/1 [00:01<00:01,  3.07s/it]   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  53% 0.5313608709519603/1 [00:01<00:01,  3.18s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   8% 0.07901127007246976/1 [00:01<00:14, 15.68s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  38% 0.38338685514789955/1 [00:01<00:03,  6.14s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "numpy-base-1.24.3    | 6.2 MB    | :  81% 0.8107955891270702/1 [00:01<00:00,  1.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   9% 0.08654430557233687/1 [00:01<00:14, 15.43s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  40% 0.39987367405542384/1 [00:01<00:03,  6.22s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :   9% 0.09484776515741766/1 [00:01<00:13, 14.70s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  42% 0.4192752896758806/1 [00:01<00:03,  6.00s/it] \u001b[A\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  63% 0.6328608922426111/1 [00:01<00:01,  3.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  10% 0.10212399262888021/1 [00:02<00:13, 14.78s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tbb-2021.8.0         | 1.6 MB    | :   1% 0.009480285473244695/1 [00:02<03:34, 216.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2023.1.0         | 171.5 MB  | :  44% 0.4361264581725215/1 [00:02<00:03,  6.60s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  11% 0.10957142545261246/1 [00:02<00:12, 14.45s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  45% 0.4515202283126961/1 [00:02<00:03,  6.67s/it]\u001b[A\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  71% 0.7068292621886008/1 [00:02<00:00,  3.25s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.6.15        | 1.0 MB    | :   2% 0.015533582935054567/1 [00:02<02:18, 141.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  12% 0.11689045426214244/1 [00:02<00:12, 14.63s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  47% 0.4679159598229413/1 [00:02<00:03,  6.51s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mpmath-1.3.0         | 832 KB    | :   2% 0.01923855978525709/1 [00:02<01:57, 119.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  13% 0.12673476201765058/1 [00:02<00:11, 13.48s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  49% 0.4866799636624441/1 [00:02<00:03,  6.18s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.4.0        | 795 KB    | :   2% 0.02013308183365385/1 [00:02<01:55, 118.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  14% 0.13833392463392322/1 [00:02<00:10, 11.87s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libiconv-1.16        | 759 KB    | :   2% 0.021080233292718544/1 [00:02<01:53, 116.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2023.1.0         | 171.5 MB  | :  51% 0.5094518129822291/1 [00:02<00:02,  5.63s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openh264-2.1.1       | 711 KB    | :   2% 0.022519043050626613/1 [00:02<01:48, 111.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  15% 0.14809262971329654/1 [00:02<00:09, 11.46s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  53% 0.5325880118911306/1 [00:02<00:02,  5.38s/it]\u001b[A\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  88% 0.881747020398348/1 [00:02<00:00,  2.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "freetype-2.12.1      | 626 KB    | :   3% 0.025568319254423814/1 [00:02<01:40, 103.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  16% 0.156866904017119/1 [00:02<00:10, 12.45s/it]  \n",
            "mkl-2023.1.0         | 171.5 MB  | :  55% 0.5512609283333543/1 [00:02<00:02,  5.38s/it]\u001b[A\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  17% 0.1659835890254809/1 [00:02<00:10, 12.05s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  57% 0.569933844775578/1 [00:02<00:02,  5.56s/it] \u001b[A\n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | :  97% 0.968196405620584/1 [00:02<00:00,  2.73s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  17% 0.17441545262476396/1 [00:02<00:09, 12.08s/it]\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  18% 0.18276171354791218/1 [00:03<00:10, 13.03s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  61% 0.6080083768382584/1 [00:03<00:02,  5.71s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  19% 0.19149318651366726/1 [00:03<00:10, 12.86s/it]\n",
            "mkl-2023.1.0         | 171.5 MB  | :  63% 0.6280476042396691/1 [00:03<00:02,  5.58s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "nettle-3.7.3         | 809 KB    | :  16% 0.15813794118569674/1 [00:03<00:10, 12.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "mkl-2023.1.0         | 171.5 MB  | :  65% 0.6513659779431289/1 [00:03<00:01,  5.22s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  20% 0.19971104342261317/1 [00:03<00:10, 13.12s/it]\n",
            "\n",
            "\n",
            "intel-openmp-2023.1. | 17.2 MB   | : 100% 1.0/1 [00:03<00:00,  3.05it/s]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  21% 0.2080573043457614/1 [00:03<00:10, 12.81s/it] \n",
            "mkl-2023.1.0         | 171.5 MB  | :  67% 0.6710408557554232/1 [00:03<00:01,  5.35s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "nettle-3.7.3         | 809 KB    | :  59% 0.5930172794463627/1 [00:03<00:00,  2.09s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  22% 0.21824402280580896/1 [00:03<00:09, 12.27s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "nettle-3.7.3         | 809 KB    | :  87% 0.8697586765213321/1 [00:03<00:00,  1.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  23% 0.22817393323745197/1 [00:03<00:09, 12.45s/it]\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  24% 0.2362633861321956/1 [00:03<00:09, 12.43s/it] \n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  25% 0.24525166712635524/1 [00:03<00:09, 12.10s/it]\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  25% 0.25355512671143604/1 [00:04<00:33, 45.44s/it]\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  26% 0.2596329167170106/1 [00:05<00:33, 44.79s/it] \n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  28% 0.27683905462011615/1 [00:05<00:18, 26.04s/it]\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  29% 0.2949868219607051/1 [00:05<00:12, 17.43s/it] \n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  31% 0.3113797344405295/1 [00:05<00:09, 13.45s/it]\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  34% 0.3429243205962231/1 [00:05<00:05,  8.40s/it]\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  94% 0.9356372501539486/1 [00:08<00:00,  4.77s/it]\n",
            "\n",
            "\n",
            "\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | :  98% 0.9806642578008815/1 [00:09<00:00,  4.59s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.13.2         | 11.2 MB   | : 100% 1.0/1 [00:09<00:00,  1.44it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "llvm-openmp-14.0.6   | 4.4 MB    | : 100% 1.0/1 [00:09<00:00,  1.77s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "numpy-base-1.24.3    | 6.2 MB    | : 100% 1.0/1 [00:10<00:00,  1.31s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tbb-2021.8.0         | 1.6 MB    | : 100% 1.0/1 [00:10<00:00, 10.32s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tbb-2021.8.0         | 1.6 MB    | : 100% 1.0/1 [00:10<00:00, 10.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ffmpeg-4.3           | 9.9 MB    | : 100% 1.0/1 [00:11<00:00,  1.31it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.6.15        | 1.0 MB    | : 100% 1.0/1 [00:11<00:00, 10.96s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "gnutls-3.6.15        | 1.0 MB    | : 100% 1.0/1 [00:11<00:00, 10.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libjpeg-turbo-2.0.0  | 950 KB    | : 100% 1.0/1 [00:11<00:00, 11.25s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libjpeg-turbo-2.0.0  | 950 KB    | : 100% 1.0/1 [00:11<00:00, 11.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.1         | 2.7 MB    | : 100% 1.0/1 [00:12<00:00, 11.48s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-3.1         | 2.7 MB    | : 100% 1.0/1 [00:12<00:00, 11.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mpmath-1.3.0         | 832 KB    | : 100% 1.0/1 [00:12<00:00, 11.42s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mpmath-1.3.0         | 832 KB    | : 100% 1.0/1 [00:12<00:00, 11.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libiconv-1.16        | 759 KB    | : 100% 1.0/1 [00:12<00:00, 11.46s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libiconv-1.16        | 759 KB    | : 100% 1.0/1 [00:12<00:00, 11.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openh264-2.1.1       | 711 KB    | : 100% 1.0/1 [00:12<00:00, 11.53s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openh264-2.1.1       | 711 KB    | : 100% 1.0/1 [00:12<00:00, 11.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.4.0        | 795 KB    | : 100% 1.0/1 [00:12<00:00, 11.58s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pillow-10.4.0        | 795 KB    | : 100% 1.0/1 [00:12<00:00, 11.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "freetype-2.12.1      | 626 KB    | : 100% 1.0/1 [00:12<00:00, 11.60s/it]                  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "freetype-2.12.1      | 626 KB    | : 100% 1.0/1 [00:12<00:00, 11.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "nettle-3.7.3         | 809 KB    | : 100% 1.0/1 [00:12<00:00,  1.22s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "cudatoolkit-10.2.89  | 365.1 MB  | : 100% 1.0/1 [00:31<00:00,  4.59s/it]               \n",
            "\n",
            "pytorch-2.4.1        | 85.1 MB   | : 100% 1.0/1 [00:43<00:00,  2.73s/it]              \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n"
          ]
        }
      ],
      "source": [
        "!conda install pytorch torchvision cudatoolkit=10.2 -c pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ISLGcGXZhVRw",
        "outputId": "92c3e44b-b494-4a81-f207-d039cc567353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flake8==3.7.9 (from -r requirements.txt (line 1))\n",
            "  Downloading flake8-3.7.9-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting yapf==0.29.0 (from -r requirements.txt (line 2))\n",
            "  Downloading yapf-0.29.0-py2.py3-none-any.whl.metadata (30 kB)\n",
            "Collecting isort==4.3.21 (from -r requirements.txt (line 3))\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting yacs (from -r requirements.txt (line 4))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Collecting gdown (from -r requirements.txt (line 5))\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tb-nightly (from -r requirements.txt (line 6))\n",
            "  Downloading tb_nightly-2.19.0a20241107-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting future (from -r requirements.txt (line 7))\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting scipy (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (4.66.5)\n",
            "Collecting ftfy (from -r requirements.txt (line 11))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting regex (from -r requirements.txt (line 12))\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting wilds==1.2.2 (from -r requirements.txt (line 13))\n",
            "  Downloading wilds-1.2.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting tabulate (from -r requirements.txt (line 14))\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting entrypoints<0.4.0,>=0.3.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyflakes<2.2.0,>=2.1.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pycodestyle<2.6.0,>=2.5.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting numpy>=1.19.1 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting ogb>=1.2.6 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting outdated>=0.2.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pandas>=1.1.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting pillow>=7.2.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting pytz>=2020.4 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting torch>=1.7.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision>=0.8.2 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting PyYAML (from yacs->-r requirements.txt (line 4))\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting beautifulsoup4 (from gdown->-r requirements.txt (line 5))\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting filelock (from gdown->-r requirements.txt (line 5))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/site-packages (from gdown->-r requirements.txt (line 5)) (2.32.3)\n",
            "Collecting absl-py>=0.4 (from tb-nightly->-r requirements.txt (line 6))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tb-nightly->-r requirements.txt (line 6))\n",
            "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting markdown>=2.6.8 (from tb-nightly->-r requirements.txt (line 6))\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from tb-nightly->-r requirements.txt (line 6)) (24.1)\n",
            "Collecting protobuf!=4.24.0,>=3.19.6 (from tb-nightly->-r requirements.txt (line 6))\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/site-packages (from tb-nightly->-r requirements.txt (line 6)) (75.1.0)\n",
            "Collecting six>1.9 (from tb-nightly->-r requirements.txt (line 6))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tb-nightly->-r requirements.txt (line 6))\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tb-nightly->-r requirements.txt (line 6))\n",
            "  Downloading werkzeug-3.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting wcwidth (from ftfy->-r requirements.txt (line 11))\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.12/site-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (2.2.3)\n",
            "Collecting littleutils (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6))\n",
            "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->gdown->-r requirements.txt (line 5))\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
            "Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
            "Downloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n",
            "Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "Downloading wilds-1.2.2-py3-none-any.whl (92 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Downloading tb_nightly-2.19.0a20241107-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m167.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
            "Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m172.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m146.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
            "Downloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
            "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m144.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.2-py3-none-any.whl (224 kB)\n",
            "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yapf, wcwidth, pytz, mpmath, mccabe, tzdata, typing-extensions, threadpoolctl, tensorboard-data-server, tabulate, sympy, soupsieve, six, regex, PyYAML, pyflakes, pycodestyle, protobuf, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, markdown, littleutils, joblib, isort, grpcio, future, ftfy, fsspec, filelock, entrypoints, absl-py, yacs, werkzeug, triton, scipy, python-dateutil, outdated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, flake8, beautifulsoup4, tb-nightly, scikit-learn, pandas, nvidia-cusolver-cu12, gdown, torch, torchvision, ogb, wilds\n",
            "Successfully installed MarkupSafe-3.0.2 PyYAML-6.0.2 absl-py-2.1.0 beautifulsoup4-4.12.3 entrypoints-0.3 filelock-3.16.1 flake8-3.7.9 fsspec-2024.10.0 ftfy-6.3.1 future-1.0.0 gdown-5.2.0 grpcio-1.67.1 isort-4.3.21 jinja2-3.1.4 joblib-1.4.2 littleutils-0.2.4 markdown-3.7 mccabe-0.6.1 mpmath-1.3.0 networkx-3.4.2 numpy-2.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 ogb-1.3.6 outdated-0.2.2 pandas-2.2.3 pillow-11.0.0 protobuf-5.28.3 pycodestyle-2.5.0 pyflakes-2.1.1 python-dateutil-2.9.0.post0 pytz-2024.2 regex-2024.11.6 scikit-learn-1.5.2 scipy-1.14.1 six-1.16.0 soupsieve-2.6 sympy-1.13.1 tabulate-0.9.0 tb-nightly-2.19.0a20241107 tensorboard-data-server-0.7.2 threadpoolctl-3.5.0 torch-2.5.1 torchvision-0.20.1 triton-3.1.0 typing-extensions-4.12.2 tzdata-2024.2 wcwidth-0.2.13 werkzeug-3.1.2 wilds-1.2.2 yacs-0.1.8 yapf-0.29.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six",
                  "wcwidth"
                ]
              },
              "id": "7760ae3775eb453ba5a3ccaa1c544248"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running develop\n",
            "/usr/local/lib/python3.12/site-packages/setuptools/command/develop.py:41: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.12/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "writing dassl.egg-info/PKG-INFO\n",
            "writing dependency_links to dassl.egg-info/dependency_links.txt\n",
            "writing requirements to dassl.egg-info/requires.txt\n",
            "writing top-level names to dassl.egg-info/top_level.txt\n",
            "reading manifest file 'dassl.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'dassl.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.12/site-packages/dassl.egg-link (link to .)\n",
            "Adding dassl 0.6.3 to easy-install.pth file\n",
            "\n",
            "Installed /content/Capstone-ood/CoOp_works/Dassl.pytorch\n",
            "Processing dependencies for dassl==0.6.3\n",
            "Searching for tabulate==0.9.0\n",
            "Best match: tabulate 0.9.0\n",
            "Adding tabulate 0.9.0 to easy-install.pth file\n",
            "Installing tabulate script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for wilds==1.2.2\n",
            "Best match: wilds 1.2.2\n",
            "Adding wilds 1.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for regex==2024.11.6\n",
            "Best match: regex 2024.11.6\n",
            "Adding regex 2024.11.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for ftfy==6.3.1\n",
            "Best match: ftfy 6.3.1\n",
            "Adding ftfy 6.3.1 to easy-install.pth file\n",
            "Installing ftfy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for tqdm==4.66.5\n",
            "Best match: tqdm 4.66.5\n",
            "Adding tqdm 4.66.5 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for scikit-learn==1.5.2\n",
            "Best match: scikit-learn 1.5.2\n",
            "Adding scikit-learn 1.5.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for scipy==1.14.1\n",
            "Best match: scipy 1.14.1\n",
            "Adding scipy 1.14.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for future==1.0.0\n",
            "Best match: future 1.0.0\n",
            "Adding future 1.0.0 to easy-install.pth file\n",
            "Installing futurize script to /usr/local/bin\n",
            "Installing pasteurize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for tb-nightly==2.19.0a20241107\n",
            "Best match: tb-nightly 2.19.0a20241107\n",
            "Adding tb-nightly 2.19.0a20241107 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for gdown==5.2.0\n",
            "Best match: gdown 5.2.0\n",
            "Adding gdown 5.2.0 to easy-install.pth file\n",
            "Installing gdown script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for yacs==0.1.8\n",
            "Best match: yacs 0.1.8\n",
            "Adding yacs 0.1.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for isort==4.3.21\n",
            "Best match: isort 4.3.21\n",
            "Adding isort 4.3.21 to easy-install.pth file\n",
            "Installing isort script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for yapf==0.29.0\n",
            "Best match: yapf 0.29.0\n",
            "Adding yapf 0.29.0 to easy-install.pth file\n",
            "Installing yapf script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for flake8==3.7.9\n",
            "Best match: flake8 3.7.9\n",
            "Adding flake8 3.7.9 to easy-install.pth file\n",
            "Installing flake8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for torchvision==0.20.1\n",
            "Best match: torchvision 0.20.1\n",
            "Adding torchvision 0.20.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for torch==2.5.1\n",
            "Best match: torch 2.5.1\n",
            "Adding torch 2.5.1 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "Installing torchfrtrace script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for pytz==2024.2\n",
            "Best match: pytz 2024.2\n",
            "Adding pytz 2024.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for pillow==11.0.0\n",
            "Best match: pillow 11.0.0\n",
            "Adding pillow 11.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for pandas==2.2.3\n",
            "Best match: pandas 2.2.3\n",
            "Adding pandas 2.2.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for outdated==0.2.2\n",
            "Best match: outdated 0.2.2\n",
            "Adding outdated 0.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for ogb==1.3.6\n",
            "Best match: ogb 1.3.6\n",
            "Adding ogb 1.3.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for numpy==2.1.3\n",
            "Best match: numpy 2.1.3\n",
            "Adding numpy 2.1.3 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing numpy-config script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for wcwidth==0.2.13\n",
            "Best match: wcwidth 0.2.13\n",
            "Adding wcwidth 0.2.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for threadpoolctl==3.5.0\n",
            "Best match: threadpoolctl 3.5.0\n",
            "Adding threadpoolctl 3.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for joblib==1.4.2\n",
            "Best match: joblib 1.4.2\n",
            "Adding joblib 1.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for werkzeug==3.1.2\n",
            "Best match: werkzeug 3.1.2\n",
            "Adding werkzeug 3.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for tensorboard-data-server==0.7.2\n",
            "Best match: tensorboard-data-server 0.7.2\n",
            "Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for six==1.16.0\n",
            "Best match: six 1.16.0\n",
            "Adding six 1.16.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for setuptools==75.1.0\n",
            "Best match: setuptools 75.1.0\n",
            "Adding setuptools 75.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for protobuf==5.28.3\n",
            "Best match: protobuf 5.28.3\n",
            "Adding protobuf 5.28.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for packaging==24.1\n",
            "Best match: packaging 24.1\n",
            "Adding packaging 24.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages/setuptools/_vendor\n",
            "Searching for Markdown==3.7\n",
            "Best match: Markdown 3.7\n",
            "Adding Markdown 3.7 to easy-install.pth file\n",
            "detected new path './setuptools/_vendor'\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for grpcio==1.67.1\n",
            "Best match: grpcio 1.67.1\n",
            "Adding grpcio 1.67.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for absl-py==2.1.0\n",
            "Best match: absl-py 2.1.0\n",
            "Adding absl-py 2.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for requests==2.32.3\n",
            "Best match: requests 2.32.3\n",
            "Adding requests 2.32.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for filelock==3.16.1\n",
            "Best match: filelock 3.16.1\n",
            "Adding filelock 3.16.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for beautifulsoup4==4.12.3\n",
            "Best match: beautifulsoup4 4.12.3\n",
            "Adding beautifulsoup4 4.12.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for PyYAML==6.0.2\n",
            "Best match: PyYAML 6.0.2\n",
            "Adding PyYAML 6.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for mccabe==0.6.1\n",
            "Best match: mccabe 0.6.1\n",
            "Adding mccabe 0.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for pycodestyle==2.5.0\n",
            "Best match: pycodestyle 2.5.0\n",
            "Adding pycodestyle 2.5.0 to easy-install.pth file\n",
            "Installing pycodestyle script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for pyflakes==2.1.1\n",
            "Best match: pyflakes 2.1.1\n",
            "Adding pyflakes 2.1.1 to easy-install.pth file\n",
            "Installing pyflakes script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for entrypoints==0.3\n",
            "Best match: entrypoints 0.3\n",
            "Adding entrypoints 0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for sympy==1.13.1\n",
            "Best match: sympy 1.13.1\n",
            "Adding sympy 1.13.1 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for triton==3.1.0\n",
            "Best match: triton 3.1.0\n",
            "Adding triton 3.1.0 to easy-install.pth file\n",
            "Installing proton script to /usr/local/bin\n",
            "Installing proton-viewer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-nvjitlink-cu12==12.4.127\n",
            "Best match: nvidia-nvjitlink-cu12 12.4.127\n",
            "Adding nvidia-nvjitlink-cu12 12.4.127 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-nvtx-cu12==12.4.127\n",
            "Best match: nvidia-nvtx-cu12 12.4.127\n",
            "Adding nvidia-nvtx-cu12 12.4.127 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-nccl-cu12==2.21.5\n",
            "Best match: nvidia-nccl-cu12 2.21.5\n",
            "Adding nvidia-nccl-cu12 2.21.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-cusparse-cu12==12.3.1.170\n",
            "Best match: nvidia-cusparse-cu12 12.3.1.170\n",
            "Adding nvidia-cusparse-cu12 12.3.1.170 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-cusolver-cu12==11.6.1.9\n",
            "Best match: nvidia-cusolver-cu12 11.6.1.9\n",
            "Adding nvidia-cusolver-cu12 11.6.1.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-curand-cu12==10.3.5.147\n",
            "Best match: nvidia-curand-cu12 10.3.5.147\n",
            "Adding nvidia-curand-cu12 10.3.5.147 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-cufft-cu12==11.2.1.3\n",
            "Best match: nvidia-cufft-cu12 11.2.1.3\n",
            "Adding nvidia-cufft-cu12 11.2.1.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-cublas-cu12==12.4.5.8\n",
            "Best match: nvidia-cublas-cu12 12.4.5.8\n",
            "Adding nvidia-cublas-cu12 12.4.5.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-cudnn-cu12==9.1.0.70\n",
            "Best match: nvidia-cudnn-cu12 9.1.0.70\n",
            "Adding nvidia-cudnn-cu12 9.1.0.70 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-cuda-cupti-cu12==12.4.127\n",
            "Best match: nvidia-cuda-cupti-cu12 12.4.127\n",
            "Adding nvidia-cuda-cupti-cu12 12.4.127 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-cuda-runtime-cu12==12.4.127\n",
            "Best match: nvidia-cuda-runtime-cu12 12.4.127\n",
            "Adding nvidia-cuda-runtime-cu12 12.4.127 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "Best match: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "Adding nvidia-cuda-nvrtc-cu12 12.4.127 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for fsspec==2024.10.0\n",
            "Best match: fsspec 2024.10.0\n",
            "Adding fsspec 2024.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for jinja2==3.1.4\n",
            "Best match: jinja2 3.1.4\n",
            "Adding jinja2 3.1.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for networkx==3.4.2\n",
            "Best match: networkx 3.4.2\n",
            "Adding networkx 3.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for typing-extensions==4.12.2\n",
            "Best match: typing-extensions 4.12.2\n",
            "typing-extensions 4.12.2 is already the active version in easy-install.pth\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages/setuptools/_vendor\n",
            "Searching for tzdata==2024.2\n",
            "Best match: tzdata 2024.2\n",
            "Adding tzdata 2024.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for python-dateutil==2.9.0.post0\n",
            "Best match: python-dateutil 2.9.0.post0\n",
            "Adding python-dateutil 2.9.0.post0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for littleutils==0.2.4\n",
            "Best match: littleutils 0.2.4\n",
            "Adding littleutils 0.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for urllib3==2.2.3\n",
            "Best match: urllib3 2.2.3\n",
            "Adding urllib3 2.2.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for MarkupSafe==3.0.2\n",
            "Best match: MarkupSafe 3.0.2\n",
            "Adding MarkupSafe 3.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for PySocks==1.7.1\n",
            "Best match: PySocks 1.7.1\n",
            "Adding PySocks 1.7.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for certifi==2024.8.30\n",
            "Best match: certifi 2024.8.30\n",
            "Adding certifi 2024.8.30 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for idna==3.7\n",
            "Best match: idna 3.7\n",
            "Adding idna 3.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for charset-normalizer==3.3.2\n",
            "Best match: charset-normalizer 3.3.2\n",
            "Adding charset-normalizer 3.3.2 to easy-install.pth file\n",
            "Installing normalizer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for soupsieve==2.6\n",
            "Best match: soupsieve 2.6\n",
            "Adding soupsieve 2.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.12/site-packages\n",
            "Finished processing dependencies for dassl==0.6.3\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/Capstone-ood/CoOp_works/Dassl.pytorch')\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc22snIjbXOC",
        "outputId": "c016dbbc-fc4a-4ed2-f212-d2a34f68b26e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.66.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/site-packages (from ftfy->-r requirements.txt (line 1)) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/Capstone-ood/CoOp_works/CoOp')\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "IMvk8G53gXqJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjxSdozrcfLs",
        "outputId": "a0121360-470b-4cd2-891f-483c051ccaae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed1\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Creating a 1-shot dataset\n",
            "Saving preprocessed few-shot data to /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  200\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "100%|███████████████████████████████████████| 256M/256M [00:03<00:00, 78.0MiB/s]\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed1/tensorboard)\n",
            "epoch [1/50] batch [5/6] time 0.339 (1.317) data 0.001 (0.248) loss 4.5898 (4.6453) acc 18.7500 (13.1250) lr 1.0000e-05 eta 0:06:28\n",
            "epoch [2/50] batch [5/6] time 0.338 (0.487) data 0.000 (0.151) loss 3.4160 (3.8289) acc 28.1250 (25.0000) lr 2.0000e-03 eta 0:02:20\n",
            "epoch [3/50] batch [5/6] time 0.337 (0.459) data 0.001 (0.122) loss 3.8184 (3.5840) acc 9.3750 (19.3750) lr 1.9980e-03 eta 0:02:09\n",
            "epoch [4/50] batch [5/6] time 0.334 (0.482) data 0.000 (0.145) loss 3.3809 (3.5027) acc 28.1250 (23.7500) lr 1.9921e-03 eta 0:02:13\n",
            "epoch [5/50] batch [5/6] time 0.339 (0.478) data 0.001 (0.135) loss 3.3789 (3.4984) acc 25.0000 (21.2500) lr 1.9823e-03 eta 0:02:09\n",
            "epoch [6/50] batch [5/6] time 0.338 (0.583) data 0.001 (0.244) loss 3.5938 (3.2961) acc 31.2500 (28.1250) lr 1.9686e-03 eta 0:02:34\n",
            "epoch [7/50] batch [5/6] time 0.338 (0.489) data 0.000 (0.150) loss 3.0410 (3.2176) acc 37.5000 (28.1250) lr 1.9511e-03 eta 0:02:06\n",
            "epoch [8/50] batch [5/6] time 0.340 (0.488) data 0.000 (0.149) loss 3.2148 (3.2398) acc 28.1250 (32.5000) lr 1.9298e-03 eta 0:02:03\n",
            "epoch [9/50] batch [5/6] time 0.337 (0.483) data 0.000 (0.143) loss 2.9707 (3.0797) acc 25.0000 (30.6250) lr 1.9048e-03 eta 0:01:59\n",
            "epoch [10/50] batch [5/6] time 0.342 (0.492) data 0.001 (0.152) loss 3.0527 (3.2418) acc 28.1250 (24.3750) lr 1.8763e-03 eta 0:01:58\n",
            "epoch [11/50] batch [5/6] time 0.341 (0.574) data 0.001 (0.230) loss 3.5234 (3.2023) acc 21.8750 (30.0000) lr 1.8443e-03 eta 0:02:14\n",
            "epoch [12/50] batch [5/6] time 0.339 (0.476) data 0.000 (0.128) loss 2.6094 (2.8930) acc 46.8750 (37.5000) lr 1.8090e-03 eta 0:01:49\n",
            "epoch [13/50] batch [5/6] time 0.340 (0.491) data 0.000 (0.148) loss 2.5918 (2.8820) acc 37.5000 (36.2500) lr 1.7705e-03 eta 0:01:49\n",
            "epoch [14/50] batch [5/6] time 0.341 (0.483) data 0.000 (0.136) loss 2.6641 (3.0059) acc 43.7500 (30.6250) lr 1.7290e-03 eta 0:01:44\n",
            "epoch [15/50] batch [5/6] time 0.342 (0.486) data 0.001 (0.143) loss 2.9668 (2.9934) acc 34.3750 (33.1250) lr 1.6845e-03 eta 0:01:42\n",
            "epoch [16/50] batch [5/6] time 0.349 (0.590) data 0.001 (0.246) loss 3.3047 (2.7559) acc 25.0000 (37.5000) lr 1.6374e-03 eta 0:02:00\n",
            "epoch [17/50] batch [5/6] time 0.343 (0.497) data 0.000 (0.154) loss 3.1816 (2.6477) acc 31.2500 (37.5000) lr 1.5878e-03 eta 0:01:38\n",
            "epoch [18/50] batch [5/6] time 0.343 (0.481) data 0.000 (0.136) loss 2.2969 (2.7910) acc 53.1250 (41.2500) lr 1.5358e-03 eta 0:01:32\n",
            "epoch [19/50] batch [5/6] time 0.343 (0.496) data 0.000 (0.151) loss 3.0020 (2.7445) acc 40.6250 (38.1250) lr 1.4818e-03 eta 0:01:32\n",
            "epoch [20/50] batch [5/6] time 0.347 (0.477) data 0.001 (0.120) loss 2.4316 (2.7582) acc 43.7500 (40.6250) lr 1.4258e-03 eta 0:01:26\n",
            "epoch [21/50] batch [5/6] time 0.347 (0.588) data 0.001 (0.241) loss 1.9385 (2.5443) acc 59.3750 (45.0000) lr 1.3681e-03 eta 0:01:42\n",
            "epoch [22/50] batch [5/6] time 0.345 (0.498) data 0.000 (0.152) loss 3.0605 (2.5773) acc 31.2500 (39.3750) lr 1.3090e-03 eta 0:01:24\n",
            "epoch [23/50] batch [5/6] time 0.347 (0.488) data 0.000 (0.140) loss 3.1035 (2.3773) acc 28.1250 (45.0000) lr 1.2487e-03 eta 0:01:19\n",
            "epoch [24/50] batch [5/6] time 0.348 (0.474) data 0.000 (0.123) loss 2.6426 (2.6824) acc 43.7500 (38.1250) lr 1.1874e-03 eta 0:01:14\n",
            "epoch [25/50] batch [5/6] time 0.349 (0.502) data 0.001 (0.153) loss 2.1836 (2.2434) acc 46.8750 (48.7500) lr 1.1253e-03 eta 0:01:15\n",
            "epoch [26/50] batch [5/6] time 0.348 (0.608) data 0.001 (0.259) loss 2.1602 (2.2512) acc 59.3750 (46.8750) lr 1.0628e-03 eta 0:01:28\n",
            "epoch [27/50] batch [5/6] time 0.348 (0.509) data 0.000 (0.159) loss 2.2402 (2.2938) acc 46.8750 (47.5000) lr 1.0000e-03 eta 0:01:10\n",
            "epoch [28/50] batch [5/6] time 0.346 (0.476) data 0.000 (0.116) loss 2.8574 (2.3111) acc 43.7500 (43.7500) lr 9.3721e-04 eta 0:01:03\n",
            "epoch [29/50] batch [5/6] time 0.350 (0.445) data 0.000 (0.093) loss 2.5195 (2.2426) acc 46.8750 (50.0000) lr 8.7467e-04 eta 0:00:56\n",
            "epoch [30/50] batch [5/6] time 0.349 (0.501) data 0.001 (0.150) loss 2.3984 (2.2119) acc 46.8750 (48.7500) lr 8.1262e-04 eta 0:01:00\n",
            "epoch [31/50] batch [5/6] time 0.341 (0.589) data 0.001 (0.240) loss 1.8711 (2.0564) acc 53.1250 (52.5000) lr 7.5131e-04 eta 0:01:07\n",
            "epoch [32/50] batch [5/6] time 0.352 (0.509) data 0.000 (0.156) loss 1.9932 (1.9561) acc 56.2500 (53.7500) lr 6.9098e-04 eta 0:00:55\n",
            "epoch [33/50] batch [5/6] time 0.352 (0.499) data 0.000 (0.146) loss 2.2129 (2.1082) acc 50.0000 (52.5000) lr 6.3188e-04 eta 0:00:51\n",
            "epoch [34/50] batch [5/6] time 0.352 (0.496) data 0.000 (0.140) loss 2.1289 (1.8896) acc 53.1250 (55.6250) lr 5.7422e-04 eta 0:00:48\n",
            "epoch [35/50] batch [5/6] time 0.353 (0.503) data 0.001 (0.148) loss 2.1855 (2.0494) acc 50.0000 (57.5000) lr 5.1825e-04 eta 0:00:45\n",
            "epoch [36/50] batch [5/6] time 0.352 (0.606) data 0.001 (0.252) loss 2.0938 (1.8100) acc 62.5000 (60.0000) lr 4.6417e-04 eta 0:00:51\n",
            "epoch [37/50] batch [5/6] time 0.352 (0.503) data 0.000 (0.146) loss 1.7500 (1.7715) acc 59.3750 (61.2500) lr 4.1221e-04 eta 0:00:39\n",
            "epoch [38/50] batch [5/6] time 0.352 (0.501) data 0.000 (0.145) loss 2.6445 (2.0145) acc 37.5000 (53.1250) lr 3.6258e-04 eta 0:00:36\n",
            "epoch [39/50] batch [5/6] time 0.352 (0.505) data 0.000 (0.145) loss 1.4609 (1.6162) acc 68.7500 (61.8750) lr 3.1545e-04 eta 0:00:33\n",
            "epoch [40/50] batch [5/6] time 0.353 (0.494) data 0.001 (0.129) loss 1.6211 (1.9670) acc 68.7500 (57.5000) lr 2.7103e-04 eta 0:00:30\n",
            "epoch [41/50] batch [5/6] time 0.356 (0.597) data 0.001 (0.238) loss 2.0566 (1.8686) acc 53.1250 (58.1250) lr 2.2949e-04 eta 0:00:32\n",
            "epoch [42/50] batch [5/6] time 0.354 (0.520) data 0.000 (0.162) loss 1.7725 (1.7098) acc 59.3750 (61.8750) lr 1.9098e-04 eta 0:00:25\n",
            "epoch [43/50] batch [5/6] time 0.354 (0.443) data 0.000 (0.083) loss 1.9941 (1.8512) acc 53.1250 (58.7500) lr 1.5567e-04 eta 0:00:19\n",
            "epoch [44/50] batch [5/6] time 0.354 (0.507) data 0.000 (0.149) loss 1.8379 (2.0371) acc 65.6250 (50.6250) lr 1.2369e-04 eta 0:00:18\n",
            "epoch [45/50] batch [5/6] time 0.354 (0.505) data 0.001 (0.148) loss 1.5537 (1.4604) acc 65.6250 (68.1250) lr 9.5173e-05 eta 0:00:15\n",
            "epoch [46/50] batch [5/6] time 0.352 (0.585) data 0.001 (0.227) loss 1.7275 (1.8600) acc 59.3750 (58.1250) lr 7.0224e-05 eta 0:00:14\n",
            "epoch [47/50] batch [5/6] time 0.352 (0.509) data 0.000 (0.156) loss 1.8633 (1.7646) acc 56.2500 (62.5000) lr 4.8943e-05 eta 0:00:09\n",
            "epoch [48/50] batch [5/6] time 0.351 (0.479) data 0.000 (0.120) loss 1.8857 (1.5820) acc 59.3750 (67.5000) lr 3.1417e-05 eta 0:00:06\n",
            "epoch [49/50] batch [5/6] time 0.352 (0.505) data 0.000 (0.152) loss 1.7510 (1.9178) acc 62.5000 (56.8750) lr 1.7713e-05 eta 0:00:03\n",
            "epoch [50/50] batch [5/6] time 0.352 (0.504) data 0.001 (0.150) loss 1.4580 (1.6607) acc 68.7500 (60.6250) lr 7.8853e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:40<00:00,  2.49it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 44\n",
            "* accuracy: 0.4%\n",
            "* error: 99.6%\n",
            "* macro_f1: 0.9%\n",
            "Elapsed: 0:03:19\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Creating a 1-shot dataset\n",
            "Saving preprocessed few-shot data to /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_1-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  200\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2/tensorboard)\n",
            "epoch [1/50] batch [5/6] time 0.352 (1.162) data 0.003 (0.270) loss 4.7305 (4.7039) acc 9.3750 (9.3750) lr 1.0000e-05 eta 0:05:42\n",
            "epoch [2/50] batch [5/6] time 0.349 (0.516) data 0.000 (0.165) loss 4.0469 (4.1465) acc 21.8750 (13.1250) lr 2.0000e-03 eta 0:02:29\n",
            "epoch [3/50] batch [5/6] time 0.350 (0.487) data 0.000 (0.134) loss 3.6426 (3.6332) acc 25.0000 (23.1250) lr 1.9980e-03 eta 0:02:17\n",
            "epoch [4/50] batch [5/6] time 0.352 (0.497) data 0.000 (0.145) loss 4.1250 (3.7473) acc 25.0000 (28.1250) lr 1.9921e-03 eta 0:02:17\n",
            "epoch [5/50] batch [5/6] time 0.352 (0.486) data 0.001 (0.133) loss 3.8789 (3.4867) acc 21.8750 (28.1250) lr 1.9823e-03 eta 0:02:11\n",
            "epoch [6/50] batch [5/6] time 0.354 (0.600) data 0.001 (0.229) loss 3.7637 (3.4719) acc 21.8750 (26.2500) lr 1.9686e-03 eta 0:02:39\n",
            "epoch [7/50] batch [5/6] time 0.352 (0.508) data 0.000 (0.153) loss 3.4590 (3.2555) acc 25.0000 (26.8750) lr 1.9511e-03 eta 0:02:11\n",
            "epoch [8/50] batch [5/6] time 0.354 (0.498) data 0.000 (0.142) loss 3.3145 (3.2582) acc 18.7500 (30.6250) lr 1.9298e-03 eta 0:02:06\n",
            "epoch [9/50] batch [5/6] time 0.352 (0.504) data 0.001 (0.148) loss 2.6543 (3.1480) acc 34.3750 (28.1250) lr 1.9048e-03 eta 0:02:04\n",
            "epoch [10/50] batch [5/6] time 0.354 (0.504) data 0.001 (0.146) loss 3.0879 (3.5195) acc 25.0000 (22.5000) lr 1.8763e-03 eta 0:02:01\n",
            "epoch [11/50] batch [5/6] time 0.357 (0.609) data 0.001 (0.250) loss 3.6230 (3.4594) acc 25.0000 (26.8750) lr 1.8443e-03 eta 0:02:23\n",
            "epoch [12/50] batch [5/6] time 0.356 (0.506) data 0.000 (0.142) loss 2.9863 (3.1281) acc 37.5000 (31.8750) lr 1.8090e-03 eta 0:01:55\n",
            "epoch [13/50] batch [5/6] time 0.353 (0.489) data 0.000 (0.130) loss 2.9180 (3.3891) acc 28.1250 (23.1250) lr 1.7705e-03 eta 0:01:49\n",
            "epoch [14/50] batch [5/6] time 0.354 (0.475) data 0.000 (0.115) loss 2.7539 (3.1727) acc 40.6250 (26.8750) lr 1.7290e-03 eta 0:01:43\n",
            "epoch [15/50] batch [5/6] time 0.354 (0.484) data 0.001 (0.126) loss 3.2383 (3.1496) acc 28.1250 (28.7500) lr 1.6845e-03 eta 0:01:42\n",
            "epoch [16/50] batch [5/6] time 0.354 (0.584) data 0.001 (0.220) loss 3.5137 (2.9656) acc 18.7500 (30.0000) lr 1.6374e-03 eta 0:01:59\n",
            "epoch [17/50] batch [5/6] time 0.353 (0.507) data 0.002 (0.151) loss 3.7773 (2.9387) acc 15.6250 (34.3750) lr 1.5878e-03 eta 0:01:40\n",
            "epoch [18/50] batch [5/6] time 0.352 (0.500) data 0.000 (0.146) loss 2.1777 (2.8672) acc 56.2500 (38.1250) lr 1.5358e-03 eta 0:01:36\n",
            "epoch [19/50] batch [5/6] time 0.352 (0.505) data 0.000 (0.151) loss 2.5820 (2.7996) acc 37.5000 (36.8750) lr 1.4818e-03 eta 0:01:34\n",
            "epoch [20/50] batch [5/6] time 0.352 (0.492) data 0.001 (0.133) loss 3.1934 (2.7137) acc 34.3750 (38.1250) lr 1.4258e-03 eta 0:01:29\n",
            "epoch [21/50] batch [5/6] time 0.351 (0.616) data 0.001 (0.242) loss 1.9971 (2.7385) acc 50.0000 (41.8750) lr 1.3681e-03 eta 0:01:47\n",
            "epoch [22/50] batch [5/6] time 0.349 (0.483) data 0.000 (0.127) loss 2.3125 (2.7008) acc 50.0000 (39.3750) lr 1.3090e-03 eta 0:01:21\n",
            "epoch [23/50] batch [5/6] time 0.349 (0.488) data 0.001 (0.127) loss 2.5020 (2.8215) acc 37.5000 (39.3750) lr 1.2487e-03 eta 0:01:19\n",
            "epoch [24/50] batch [5/6] time 0.348 (0.502) data 0.000 (0.150) loss 3.1484 (2.7100) acc 34.3750 (39.3750) lr 1.1874e-03 eta 0:01:18\n",
            "epoch [25/50] batch [5/6] time 0.351 (0.471) data 0.001 (0.111) loss 2.5469 (2.6207) acc 40.6250 (42.5000) lr 1.1253e-03 eta 0:01:11\n",
            "epoch [26/50] batch [5/6] time 0.350 (0.604) data 0.001 (0.252) loss 2.6582 (2.5941) acc 31.2500 (45.0000) lr 1.0628e-03 eta 0:01:27\n",
            "epoch [27/50] batch [5/6] time 0.351 (0.510) data 0.001 (0.158) loss 2.0938 (2.3945) acc 56.2500 (48.1250) lr 1.0000e-03 eta 0:01:10\n",
            "epoch [28/50] batch [5/6] time 0.350 (0.469) data 0.000 (0.111) loss 2.4062 (2.3441) acc 46.8750 (48.1250) lr 9.3721e-04 eta 0:01:02\n",
            "epoch [29/50] batch [5/6] time 0.351 (0.498) data 0.000 (0.146) loss 1.8271 (2.5018) acc 62.5000 (44.3750) lr 8.7467e-04 eta 0:01:03\n",
            "epoch [30/50] batch [5/6] time 0.349 (0.475) data 0.001 (0.121) loss 2.1504 (2.3016) acc 53.1250 (46.2500) lr 8.1262e-04 eta 0:00:57\n",
            "epoch [31/50] batch [5/6] time 0.352 (0.601) data 0.001 (0.246) loss 2.4805 (2.3625) acc 50.0000 (50.6250) lr 7.5131e-04 eta 0:01:09\n",
            "epoch [32/50] batch [5/6] time 0.351 (0.513) data 0.000 (0.161) loss 2.0938 (2.1850) acc 50.0000 (52.5000) lr 6.9098e-04 eta 0:00:55\n",
            "epoch [33/50] batch [5/6] time 0.349 (0.496) data 0.000 (0.141) loss 2.6172 (2.2773) acc 43.7500 (51.8750) lr 6.3188e-04 eta 0:00:51\n",
            "epoch [34/50] batch [5/6] time 0.353 (0.450) data 0.000 (0.094) loss 2.1465 (2.2699) acc 53.1250 (53.7500) lr 5.7422e-04 eta 0:00:43\n",
            "epoch [35/50] batch [5/6] time 0.354 (0.508) data 0.001 (0.152) loss 2.6309 (2.4229) acc 43.7500 (48.7500) lr 5.1825e-04 eta 0:00:46\n",
            "epoch [36/50] batch [5/6] time 0.353 (0.610) data 0.001 (0.255) loss 2.4395 (2.1367) acc 37.5000 (53.1250) lr 4.6417e-04 eta 0:00:51\n",
            "epoch [37/50] batch [5/6] time 0.354 (0.517) data 0.002 (0.162) loss 1.8770 (2.2023) acc 50.0000 (51.8750) lr 4.1221e-04 eta 0:00:40\n",
            "epoch [38/50] batch [5/6] time 0.352 (0.486) data 0.000 (0.128) loss 2.0820 (2.5086) acc 43.7500 (45.0000) lr 3.6258e-04 eta 0:00:35\n",
            "epoch [39/50] batch [5/6] time 0.354 (0.506) data 0.000 (0.151) loss 2.2812 (2.3039) acc 53.1250 (53.1250) lr 3.1545e-04 eta 0:00:33\n",
            "epoch [40/50] batch [5/6] time 0.352 (0.507) data 0.001 (0.152) loss 2.3125 (2.3176) acc 53.1250 (47.5000) lr 2.7103e-04 eta 0:00:30\n",
            "epoch [41/50] batch [5/6] time 0.351 (0.607) data 0.001 (0.251) loss 2.2148 (2.1766) acc 46.8750 (53.1250) lr 2.2949e-04 eta 0:00:33\n",
            "epoch [42/50] batch [5/6] time 0.352 (0.510) data 0.000 (0.156) loss 1.9424 (2.0777) acc 59.3750 (58.7500) lr 1.9098e-04 eta 0:00:24\n",
            "epoch [43/50] batch [5/6] time 0.350 (0.508) data 0.000 (0.153) loss 2.4414 (2.2541) acc 46.8750 (51.8750) lr 1.5567e-04 eta 0:00:21\n",
            "epoch [44/50] batch [5/6] time 0.352 (0.486) data 0.000 (0.123) loss 2.1328 (2.1328) acc 50.0000 (52.5000) lr 1.2369e-04 eta 0:00:17\n",
            "epoch [45/50] batch [5/6] time 0.351 (0.511) data 0.001 (0.156) loss 1.6221 (1.9576) acc 68.7500 (60.6250) lr 9.5173e-05 eta 0:00:15\n",
            "epoch [46/50] batch [5/6] time 0.354 (0.617) data 0.001 (0.262) loss 2.1172 (1.9297) acc 46.8750 (55.6250) lr 7.0224e-05 eta 0:00:15\n",
            "epoch [47/50] batch [5/6] time 0.351 (0.490) data 0.001 (0.128) loss 2.5039 (2.3775) acc 46.8750 (46.8750) lr 4.8943e-05 eta 0:00:09\n",
            "epoch [48/50] batch [5/6] time 0.351 (0.512) data 0.000 (0.156) loss 2.4043 (2.0729) acc 43.7500 (52.5000) lr 3.1417e-05 eta 0:00:06\n",
            "epoch [49/50] batch [5/6] time 0.353 (0.476) data 0.000 (0.117) loss 1.9502 (2.0877) acc 59.3750 (52.5000) lr 1.7713e-05 eta 0:00:03\n",
            "epoch [50/50] batch [5/6] time 0.354 (0.477) data 0.001 (0.121) loss 2.2852 (2.1479) acc 53.1250 (56.8750) lr 7.8853e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:38<00:00,  2.57it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 50\n",
            "* accuracy: 0.5%\n",
            "* error: 99.5%\n",
            "* macro_f1: 1.0%\n",
            "Elapsed: 0:03:19\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Creating a 1-shot dataset\n",
            "Saving preprocessed few-shot data to /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_1-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  200\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3/tensorboard)\n",
            "epoch [1/50] batch [5/6] time 0.350 (1.026) data 0.001 (0.168) loss 4.9961 (4.8539) acc 6.2500 (11.8750) lr 1.0000e-05 eta 0:05:02\n",
            "epoch [2/50] batch [5/6] time 0.349 (0.509) data 0.000 (0.160) loss 4.2734 (4.1332) acc 9.3750 (16.2500) lr 2.0000e-03 eta 0:02:27\n",
            "epoch [3/50] batch [5/6] time 0.348 (0.497) data 0.000 (0.148) loss 3.6973 (4.0715) acc 34.3750 (18.7500) lr 1.9980e-03 eta 0:02:20\n",
            "epoch [4/50] batch [5/6] time 0.350 (0.492) data 0.000 (0.142) loss 3.2148 (3.7996) acc 40.6250 (23.7500) lr 1.9921e-03 eta 0:02:16\n",
            "epoch [5/50] batch [5/6] time 0.350 (0.477) data 0.001 (0.117) loss 3.3164 (3.5953) acc 34.3750 (25.0000) lr 1.9823e-03 eta 0:02:09\n",
            "epoch [6/50] batch [5/6] time 0.354 (0.591) data 0.001 (0.237) loss 3.4453 (3.5039) acc 31.2500 (31.2500) lr 1.9686e-03 eta 0:02:36\n",
            "epoch [7/50] batch [5/6] time 0.351 (0.491) data 0.000 (0.136) loss 3.5938 (3.6836) acc 25.0000 (21.2500) lr 1.9511e-03 eta 0:02:07\n",
            "epoch [8/50] batch [5/6] time 0.351 (0.499) data 0.000 (0.146) loss 3.6094 (3.3992) acc 21.8750 (24.3750) lr 1.9298e-03 eta 0:02:06\n",
            "epoch [9/50] batch [5/6] time 0.352 (0.502) data 0.000 (0.148) loss 2.6289 (3.3617) acc 40.6250 (28.7500) lr 1.9048e-03 eta 0:02:03\n",
            "epoch [10/50] batch [5/6] time 0.351 (0.502) data 0.001 (0.147) loss 3.6543 (3.5488) acc 18.7500 (23.7500) lr 1.8763e-03 eta 0:02:01\n",
            "epoch [11/50] batch [5/6] time 0.355 (0.592) data 0.001 (0.236) loss 3.4707 (3.5676) acc 28.1250 (24.3750) lr 1.8443e-03 eta 0:02:19\n",
            "epoch [12/50] batch [5/6] time 0.352 (0.531) data 0.000 (0.174) loss 3.2285 (3.2742) acc 28.1250 (30.6250) lr 1.8090e-03 eta 0:02:01\n",
            "epoch [13/50] batch [5/6] time 0.356 (0.493) data 0.000 (0.135) loss 3.3477 (3.5488) acc 31.2500 (24.3750) lr 1.7705e-03 eta 0:01:49\n",
            "epoch [14/50] batch [5/6] time 0.355 (0.501) data 0.000 (0.146) loss 3.5527 (3.3121) acc 28.1250 (28.7500) lr 1.7290e-03 eta 0:01:48\n",
            "epoch [15/50] batch [5/6] time 0.354 (0.480) data 0.001 (0.116) loss 3.0625 (3.2437) acc 28.1250 (28.1250) lr 1.6845e-03 eta 0:01:41\n",
            "epoch [16/50] batch [5/6] time 0.355 (0.559) data 0.001 (0.183) loss 3.2734 (3.2039) acc 28.1250 (27.5000) lr 1.6374e-03 eta 0:01:54\n",
            "epoch [17/50] batch [5/6] time 0.354 (0.484) data 0.000 (0.118) loss 3.4121 (3.3984) acc 18.7500 (25.0000) lr 1.5878e-03 eta 0:01:36\n",
            "epoch [18/50] batch [5/6] time 0.353 (0.498) data 0.001 (0.141) loss 2.9199 (3.1809) acc 28.1250 (28.1250) lr 1.5358e-03 eta 0:01:36\n",
            "epoch [19/50] batch [5/6] time 0.354 (0.493) data 0.000 (0.136) loss 2.7363 (3.1691) acc 40.6250 (28.7500) lr 1.4818e-03 eta 0:01:32\n",
            "epoch [20/50] batch [5/6] time 0.355 (0.511) data 0.001 (0.157) loss 2.7520 (3.2156) acc 37.5000 (31.8750) lr 1.4258e-03 eta 0:01:32\n",
            "epoch [21/50] batch [5/6] time 0.354 (0.589) data 0.001 (0.234) loss 2.7969 (3.0527) acc 46.8750 (36.8750) lr 1.3681e-03 eta 0:01:43\n",
            "epoch [22/50] batch [5/6] time 0.352 (0.517) data 0.000 (0.163) loss 3.4160 (3.1059) acc 18.7500 (31.8750) lr 1.3090e-03 eta 0:01:27\n",
            "epoch [23/50] batch [5/6] time 0.351 (0.491) data 0.000 (0.138) loss 3.0840 (3.1313) acc 37.5000 (31.8750) lr 1.2487e-03 eta 0:01:20\n",
            "epoch [24/50] batch [5/6] time 0.351 (0.455) data 0.000 (0.100) loss 2.4043 (3.0055) acc 43.7500 (28.1250) lr 1.1874e-03 eta 0:01:11\n",
            "epoch [25/50] batch [5/6] time 0.349 (0.490) data 0.001 (0.138) loss 2.9414 (3.3254) acc 31.2500 (25.0000) lr 1.1253e-03 eta 0:01:14\n",
            "epoch [26/50] batch [5/6] time 0.352 (0.592) data 0.001 (0.239) loss 2.9453 (3.0328) acc 40.6250 (33.1250) lr 1.0628e-03 eta 0:01:25\n",
            "epoch [27/50] batch [5/6] time 0.351 (0.509) data 0.000 (0.158) loss 2.4805 (2.8785) acc 43.7500 (35.6250) lr 1.0000e-03 eta 0:01:10\n",
            "epoch [28/50] batch [5/6] time 0.349 (0.497) data 0.000 (0.145) loss 3.4375 (3.0477) acc 21.8750 (31.8750) lr 9.3721e-04 eta 0:01:06\n",
            "epoch [29/50] batch [5/6] time 0.352 (0.497) data 0.000 (0.145) loss 3.0938 (3.0008) acc 34.3750 (34.3750) lr 8.7467e-04 eta 0:01:03\n",
            "epoch [30/50] batch [5/6] time 0.350 (0.492) data 0.001 (0.137) loss 2.7031 (2.9914) acc 28.1250 (30.6250) lr 8.1262e-04 eta 0:00:59\n",
            "epoch [31/50] batch [5/6] time 0.350 (0.582) data 0.001 (0.225) loss 2.7402 (3.0277) acc 40.6250 (35.0000) lr 7.5131e-04 eta 0:01:06\n",
            "epoch [32/50] batch [5/6] time 0.352 (0.503) data 0.001 (0.146) loss 3.1348 (2.7344) acc 34.3750 (41.2500) lr 6.9098e-04 eta 0:00:54\n",
            "epoch [33/50] batch [5/6] time 0.352 (0.487) data 0.000 (0.134) loss 2.8770 (2.8008) acc 37.5000 (35.6250) lr 6.3188e-04 eta 0:00:50\n",
            "epoch [34/50] batch [5/6] time 0.351 (0.499) data 0.000 (0.147) loss 2.7461 (2.7965) acc 34.3750 (33.1250) lr 5.7422e-04 eta 0:00:48\n",
            "epoch [35/50] batch [5/6] time 0.352 (0.477) data 0.001 (0.124) loss 2.3086 (2.9832) acc 53.1250 (35.0000) lr 5.1825e-04 eta 0:00:43\n",
            "epoch [36/50] batch [5/6] time 0.352 (0.616) data 0.001 (0.260) loss 2.8555 (2.7543) acc 25.0000 (33.1250) lr 4.6417e-04 eta 0:00:52\n",
            "epoch [37/50] batch [5/6] time 0.351 (0.508) data 0.000 (0.153) loss 2.8535 (2.7875) acc 28.1250 (35.0000) lr 4.1221e-04 eta 0:00:40\n",
            "epoch [38/50] batch [5/6] time 0.352 (0.494) data 0.000 (0.140) loss 2.8770 (2.9172) acc 37.5000 (38.1250) lr 3.6258e-04 eta 0:00:36\n",
            "epoch [39/50] batch [5/6] time 0.351 (0.503) data 0.000 (0.148) loss 2.6055 (2.8246) acc 37.5000 (37.5000) lr 3.1545e-04 eta 0:00:33\n",
            "epoch [40/50] batch [5/6] time 0.354 (0.490) data 0.001 (0.132) loss 2.9297 (3.0961) acc 37.5000 (33.7500) lr 2.7103e-04 eta 0:00:29\n",
            "epoch [41/50] batch [5/6] time 0.353 (0.579) data 0.001 (0.224) loss 2.3652 (2.7516) acc 31.2500 (35.6250) lr 2.2949e-04 eta 0:00:31\n",
            "epoch [42/50] batch [5/6] time 0.354 (0.510) data 0.000 (0.146) loss 2.2949 (3.0027) acc 50.0000 (36.8750) lr 1.9098e-04 eta 0:00:24\n",
            "epoch [43/50] batch [5/6] time 0.352 (0.512) data 0.000 (0.157) loss 3.1270 (2.9414) acc 21.8750 (37.5000) lr 1.5567e-04 eta 0:00:22\n",
            "epoch [44/50] batch [5/6] time 0.352 (0.495) data 0.000 (0.134) loss 2.9219 (2.6238) acc 40.6250 (43.1250) lr 1.2369e-04 eta 0:00:18\n",
            "epoch [45/50] batch [5/6] time 0.353 (0.498) data 0.001 (0.143) loss 3.3008 (2.8402) acc 21.8750 (36.8750) lr 9.5173e-05 eta 0:00:15\n",
            "epoch [46/50] batch [5/6] time 0.354 (0.591) data 0.001 (0.236) loss 3.6777 (3.1191) acc 18.7500 (30.6250) lr 7.0224e-05 eta 0:00:14\n",
            "epoch [47/50] batch [5/6] time 0.352 (0.501) data 0.000 (0.147) loss 3.4414 (3.0637) acc 31.2500 (32.5000) lr 4.8943e-05 eta 0:00:09\n",
            "epoch [48/50] batch [5/6] time 0.355 (0.504) data 0.000 (0.149) loss 2.4922 (2.9402) acc 46.8750 (35.6250) lr 3.1417e-05 eta 0:00:06\n",
            "epoch [49/50] batch [5/6] time 0.352 (0.494) data 0.000 (0.141) loss 3.0996 (2.8473) acc 40.6250 (36.2500) lr 1.7713e-05 eta 0:00:03\n",
            "epoch [50/50] batch [5/6] time 0.352 (0.504) data 0.001 (0.150) loss 2.3398 (2.6281) acc 50.0000 (41.2500) lr 7.8853e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:39<00:00,  2.56it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 61\n",
            "* accuracy: 0.6%\n",
            "* error: 99.4%\n",
            "* macro_f1: 1.2%\n",
            "Elapsed: 0:03:18\n"
          ]
        }
      ],
      "source": [
        "# CLIP + CoOp (M=16, end) + 1 shot\n",
        "!rm -rf output/imagenet\n",
        "!bash scripts/coop/main.sh imagenet rn50_ep50 end 16 1 False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIP + CoOp (M=16, end) + 8 shot\n",
        "!bash scripts/coop/main.sh imagenet rn50_ep50 end 16 8 False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9k1dAUwgO84",
        "outputId": "c123c1a7-22e3-4be9-d5b5-490c36f34cb9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed1\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Creating a 8-shot dataset\n",
            "Saving preprocessed few-shot data to /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_8-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  1,600\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed1/tensorboard)\n",
            "epoch [1/50] batch [5/50] time 0.341 (1.098) data 0.001 (0.329) loss 4.4375 (4.6281) acc 21.8750 (15.6250) lr 1.0000e-05 eta 0:45:39\n",
            "epoch [1/50] batch [10/50] time 0.342 (0.720) data 0.001 (0.165) loss 4.7422 (4.6320) acc 15.6250 (13.7500) lr 1.0000e-05 eta 0:29:52\n",
            "epoch [1/50] batch [15/50] time 0.348 (0.595) data 0.001 (0.110) loss 4.2734 (4.5971) acc 15.6250 (13.3333) lr 1.0000e-05 eta 0:24:37\n",
            "epoch [1/50] batch [20/50] time 0.361 (0.534) data 0.007 (0.083) loss 4.4219 (4.5723) acc 21.8750 (13.2812) lr 1.0000e-05 eta 0:22:03\n",
            "epoch [1/50] batch [25/50] time 0.356 (0.497) data 0.002 (0.067) loss 4.4844 (4.5395) acc 12.5000 (14.2500) lr 1.0000e-05 eta 0:20:29\n",
            "epoch [1/50] batch [30/50] time 0.354 (0.472) data 0.001 (0.056) loss 4.6602 (4.5418) acc 9.3750 (14.1667) lr 1.0000e-05 eta 0:19:26\n",
            "epoch [1/50] batch [35/50] time 0.345 (0.455) data 0.001 (0.048) loss 4.1836 (4.5038) acc 12.5000 (14.5536) lr 1.0000e-05 eta 0:18:40\n",
            "epoch [1/50] batch [40/50] time 0.336 (0.441) data 0.001 (0.042) loss 4.0781 (4.4671) acc 15.6250 (14.5312) lr 1.0000e-05 eta 0:18:04\n",
            "epoch [1/50] batch [45/50] time 0.349 (0.430) data 0.000 (0.037) loss 4.4648 (4.4375) acc 9.3750 (14.7222) lr 1.0000e-05 eta 0:17:36\n",
            "epoch [1/50] batch [50/50] time 0.346 (0.422) data 0.000 (0.034) loss 3.8066 (4.3762) acc 28.1250 (15.9375) lr 2.0000e-03 eta 0:17:13\n",
            "epoch [2/50] batch [5/50] time 0.346 (0.584) data 0.001 (0.195) loss 4.1055 (3.7793) acc 25.0000 (24.3750) lr 2.0000e-03 eta 0:23:47\n",
            "epoch [2/50] batch [10/50] time 0.348 (0.465) data 0.000 (0.098) loss 3.6016 (3.7822) acc 21.8750 (23.1250) lr 2.0000e-03 eta 0:18:55\n",
            "epoch [2/50] batch [15/50] time 0.347 (0.426) data 0.001 (0.065) loss 4.2461 (3.7230) acc 12.5000 (23.7500) lr 2.0000e-03 eta 0:17:18\n",
            "epoch [2/50] batch [20/50] time 0.358 (0.408) data 0.007 (0.050) loss 3.0391 (3.6159) acc 31.2500 (25.7812) lr 2.0000e-03 eta 0:16:30\n",
            "epoch [2/50] batch [25/50] time 0.353 (0.397) data 0.001 (0.040) loss 3.3262 (3.5893) acc 21.8750 (25.0000) lr 2.0000e-03 eta 0:16:02\n",
            "epoch [2/50] batch [30/50] time 0.349 (0.390) data 0.001 (0.034) loss 2.8945 (3.5457) acc 25.0000 (24.3750) lr 2.0000e-03 eta 0:15:42\n",
            "epoch [2/50] batch [35/50] time 0.355 (0.385) data 0.001 (0.030) loss 3.5742 (3.5868) acc 31.2500 (24.3750) lr 2.0000e-03 eta 0:15:29\n",
            "epoch [2/50] batch [40/50] time 0.349 (0.380) data 0.001 (0.026) loss 3.9277 (3.6030) acc 21.8750 (24.1406) lr 2.0000e-03 eta 0:15:16\n",
            "epoch [2/50] batch [45/50] time 0.350 (0.377) data 0.000 (0.023) loss 3.4941 (3.5847) acc 21.8750 (23.8194) lr 2.0000e-03 eta 0:15:07\n",
            "epoch [2/50] batch [50/50] time 0.351 (0.375) data 0.000 (0.021) loss 3.2578 (3.5910) acc 25.0000 (23.5000) lr 1.9980e-03 eta 0:14:58\n",
            "epoch [3/50] batch [5/50] time 0.353 (0.515) data 0.000 (0.137) loss 3.3828 (3.3340) acc 31.2500 (21.8750) lr 1.9980e-03 eta 0:20:33\n",
            "epoch [3/50] batch [10/50] time 0.356 (0.436) data 0.001 (0.070) loss 2.7734 (3.2469) acc 31.2500 (25.0000) lr 1.9980e-03 eta 0:17:22\n",
            "epoch [3/50] batch [15/50] time 0.353 (0.408) data 0.001 (0.047) loss 3.3047 (3.3440) acc 25.0000 (25.0000) lr 1.9980e-03 eta 0:16:13\n",
            "epoch [3/50] batch [20/50] time 0.358 (0.395) data 0.003 (0.035) loss 3.3262 (3.3327) acc 31.2500 (25.4688) lr 1.9980e-03 eta 0:15:39\n",
            "epoch [3/50] batch [25/50] time 0.361 (0.388) data 0.007 (0.029) loss 3.4395 (3.3482) acc 25.0000 (25.7500) lr 1.9980e-03 eta 0:15:20\n",
            "epoch [3/50] batch [30/50] time 0.366 (0.383) data 0.008 (0.025) loss 3.5879 (3.3577) acc 25.0000 (25.8333) lr 1.9980e-03 eta 0:15:08\n",
            "epoch [3/50] batch [35/50] time 0.355 (0.380) data 0.001 (0.022) loss 3.3984 (3.3617) acc 25.0000 (26.1607) lr 1.9980e-03 eta 0:14:58\n",
            "epoch [3/50] batch [40/50] time 0.354 (0.377) data 0.001 (0.020) loss 3.8438 (3.3670) acc 12.5000 (26.1719) lr 1.9980e-03 eta 0:14:49\n",
            "epoch [3/50] batch [45/50] time 0.355 (0.374) data 0.000 (0.018) loss 3.1309 (3.3438) acc 34.3750 (26.8750) lr 1.9980e-03 eta 0:14:41\n",
            "epoch [3/50] batch [50/50] time 0.353 (0.372) data 0.000 (0.016) loss 3.1016 (3.3514) acc 40.6250 (26.9375) lr 1.9921e-03 eta 0:14:35\n",
            "epoch [4/50] batch [5/50] time 0.351 (0.577) data 0.001 (0.187) loss 3.6348 (3.2805) acc 21.8750 (25.6250) lr 1.9921e-03 eta 0:22:32\n",
            "epoch [4/50] batch [10/50] time 0.358 (0.467) data 0.001 (0.094) loss 3.1641 (3.1918) acc 28.1250 (26.8750) lr 1.9921e-03 eta 0:18:13\n",
            "epoch [4/50] batch [15/50] time 0.361 (0.431) data 0.005 (0.063) loss 3.0801 (3.2148) acc 37.5000 (27.9167) lr 1.9921e-03 eta 0:16:46\n",
            "epoch [4/50] batch [20/50] time 0.368 (0.415) data 0.011 (0.049) loss 3.7324 (3.2860) acc 21.8750 (27.1875) lr 1.9921e-03 eta 0:16:08\n",
            "epoch [4/50] batch [25/50] time 0.371 (0.406) data 0.007 (0.040) loss 3.3516 (3.3197) acc 31.2500 (26.1250) lr 1.9921e-03 eta 0:15:43\n",
            "epoch [4/50] batch [30/50] time 0.363 (0.399) data 0.005 (0.034) loss 3.7090 (3.3343) acc 15.6250 (25.9375) lr 1.9921e-03 eta 0:15:25\n",
            "epoch [4/50] batch [35/50] time 0.359 (0.394) data 0.001 (0.030) loss 3.3398 (3.3324) acc 28.1250 (26.5179) lr 1.9921e-03 eta 0:15:12\n",
            "epoch [4/50] batch [40/50] time 0.359 (0.390) data 0.000 (0.027) loss 3.0234 (3.3206) acc 21.8750 (26.4844) lr 1.9921e-03 eta 0:15:00\n",
            "epoch [4/50] batch [45/50] time 0.359 (0.387) data 0.001 (0.024) loss 3.3496 (3.3546) acc 34.3750 (26.3194) lr 1.9921e-03 eta 0:14:51\n",
            "epoch [4/50] batch [50/50] time 0.357 (0.384) data 0.000 (0.021) loss 3.3711 (3.3309) acc 31.2500 (26.5625) lr 1.9823e-03 eta 0:14:42\n",
            "epoch [5/50] batch [5/50] time 0.355 (0.601) data 0.001 (0.225) loss 3.4180 (3.2430) acc 31.2500 (30.6250) lr 1.9823e-03 eta 0:22:58\n",
            "epoch [5/50] batch [10/50] time 0.358 (0.481) data 0.001 (0.113) loss 3.2324 (3.2320) acc 21.8750 (30.0000) lr 1.9823e-03 eta 0:18:21\n",
            "epoch [5/50] batch [15/50] time 0.361 (0.440) data 0.001 (0.076) loss 2.8047 (3.1581) acc 43.7500 (31.0417) lr 1.9823e-03 eta 0:16:45\n",
            "epoch [5/50] batch [20/50] time 0.369 (0.421) data 0.008 (0.058) loss 2.9043 (3.1479) acc 25.0000 (29.8438) lr 1.9823e-03 eta 0:16:00\n",
            "epoch [5/50] batch [25/50] time 0.366 (0.410) data 0.001 (0.047) loss 3.3379 (3.1726) acc 31.2500 (29.3750) lr 1.9823e-03 eta 0:15:31\n",
            "epoch [5/50] batch [30/50] time 0.367 (0.402) data 0.007 (0.040) loss 3.1914 (3.2234) acc 21.8750 (28.5417) lr 1.9823e-03 eta 0:15:11\n",
            "epoch [5/50] batch [35/50] time 0.357 (0.396) data 0.001 (0.034) loss 3.0137 (3.2136) acc 28.1250 (28.7500) lr 1.9823e-03 eta 0:14:56\n",
            "epoch [5/50] batch [40/50] time 0.353 (0.391) data 0.000 (0.030) loss 3.5645 (3.2072) acc 21.8750 (28.7500) lr 1.9823e-03 eta 0:14:42\n",
            "epoch [5/50] batch [45/50] time 0.355 (0.387) data 0.000 (0.027) loss 3.4395 (3.2215) acc 28.1250 (28.8194) lr 1.9823e-03 eta 0:14:32\n",
            "epoch [5/50] batch [50/50] time 0.355 (0.384) data 0.000 (0.024) loss 3.7031 (3.2314) acc 18.7500 (28.5000) lr 1.9686e-03 eta 0:14:23\n",
            "epoch [6/50] batch [5/50] time 0.350 (0.569) data 0.001 (0.185) loss 3.3535 (3.2711) acc 25.0000 (24.3750) lr 1.9686e-03 eta 0:21:17\n",
            "epoch [6/50] batch [10/50] time 0.370 (0.464) data 0.008 (0.094) loss 3.2305 (3.1705) acc 28.1250 (28.7500) lr 1.9686e-03 eta 0:17:20\n",
            "epoch [6/50] batch [15/50] time 0.367 (0.430) data 0.008 (0.064) loss 3.8418 (3.1939) acc 15.6250 (28.9583) lr 1.9686e-03 eta 0:16:00\n",
            "epoch [6/50] batch [20/50] time 0.363 (0.413) data 0.001 (0.049) loss 3.1543 (3.1937) acc 37.5000 (29.5312) lr 1.9686e-03 eta 0:15:21\n",
            "epoch [6/50] batch [25/50] time 0.358 (0.402) data 0.001 (0.040) loss 3.6641 (3.2038) acc 25.0000 (29.6250) lr 1.9686e-03 eta 0:14:54\n",
            "epoch [6/50] batch [30/50] time 0.370 (0.396) data 0.020 (0.035) loss 2.9199 (3.1519) acc 34.3750 (30.4167) lr 1.9686e-03 eta 0:14:38\n",
            "epoch [6/50] batch [35/50] time 0.354 (0.390) data 0.001 (0.030) loss 2.8770 (3.2094) acc 34.3750 (29.5536) lr 1.9686e-03 eta 0:14:24\n",
            "epoch [6/50] batch [40/50] time 0.353 (0.386) data 0.001 (0.027) loss 3.1562 (3.1980) acc 40.6250 (29.4531) lr 1.9686e-03 eta 0:14:12\n",
            "epoch [6/50] batch [45/50] time 0.354 (0.382) data 0.000 (0.024) loss 3.0430 (3.1895) acc 34.3750 (29.3750) lr 1.9686e-03 eta 0:14:02\n",
            "epoch [6/50] batch [50/50] time 0.355 (0.379) data 0.000 (0.021) loss 2.9434 (3.2085) acc 37.5000 (29.3125) lr 1.9511e-03 eta 0:13:54\n",
            "epoch [7/50] batch [5/50] time 0.350 (0.544) data 0.001 (0.150) loss 2.7930 (3.2313) acc 40.6250 (30.6250) lr 1.9511e-03 eta 0:19:53\n",
            "epoch [7/50] batch [10/50] time 0.367 (0.451) data 0.008 (0.076) loss 3.1191 (3.2105) acc 34.3750 (28.4375) lr 1.9511e-03 eta 0:16:28\n",
            "epoch [7/50] batch [15/50] time 0.362 (0.421) data 0.010 (0.053) loss 2.5430 (3.1836) acc 43.7500 (28.7500) lr 1.9511e-03 eta 0:15:20\n",
            "epoch [7/50] batch [20/50] time 0.360 (0.406) data 0.004 (0.041) loss 3.6387 (3.2066) acc 21.8750 (29.5312) lr 1.9511e-03 eta 0:14:46\n",
            "epoch [7/50] batch [25/50] time 0.372 (0.398) data 0.008 (0.034) loss 3.6309 (3.2384) acc 15.6250 (28.7500) lr 1.9511e-03 eta 0:14:24\n",
            "epoch [7/50] batch [30/50] time 0.351 (0.391) data 0.000 (0.029) loss 3.0938 (3.2355) acc 18.7500 (28.3333) lr 1.9511e-03 eta 0:14:08\n",
            "epoch [7/50] batch [35/50] time 0.356 (0.386) data 0.001 (0.025) loss 3.7832 (3.2171) acc 15.6250 (28.2143) lr 1.9511e-03 eta 0:13:55\n",
            "epoch [7/50] batch [40/50] time 0.355 (0.382) data 0.000 (0.022) loss 2.8906 (3.1877) acc 34.3750 (29.0625) lr 1.9511e-03 eta 0:13:44\n",
            "epoch [7/50] batch [45/50] time 0.354 (0.379) data 0.000 (0.019) loss 3.1777 (3.2093) acc 34.3750 (28.4722) lr 1.9511e-03 eta 0:13:36\n",
            "epoch [7/50] batch [50/50] time 0.355 (0.376) data 0.000 (0.018) loss 3.8555 (3.2289) acc 25.0000 (28.1875) lr 1.9298e-03 eta 0:13:28\n",
            "epoch [8/50] batch [5/50] time 0.354 (0.602) data 0.001 (0.227) loss 3.1875 (3.2199) acc 21.8750 (26.2500) lr 1.9298e-03 eta 0:21:30\n",
            "epoch [8/50] batch [10/50] time 0.369 (0.484) data 0.013 (0.118) loss 3.2129 (3.1805) acc 21.8750 (27.1875) lr 1.9298e-03 eta 0:17:14\n",
            "epoch [8/50] batch [15/50] time 0.369 (0.444) data 0.016 (0.082) loss 2.7812 (3.1759) acc 46.8750 (28.5417) lr 1.9298e-03 eta 0:15:48\n",
            "epoch [8/50] batch [20/50] time 0.362 (0.424) data 0.001 (0.063) loss 3.1328 (3.1343) acc 37.5000 (29.3750) lr 1.9298e-03 eta 0:15:03\n",
            "epoch [8/50] batch [25/50] time 0.359 (0.412) data 0.006 (0.052) loss 2.7812 (3.1023) acc 40.6250 (30.7500) lr 1.9298e-03 eta 0:14:35\n",
            "epoch [8/50] batch [30/50] time 0.358 (0.404) data 0.002 (0.044) loss 3.1758 (3.0856) acc 34.3750 (31.8750) lr 1.9298e-03 eta 0:14:15\n",
            "epoch [8/50] batch [35/50] time 0.354 (0.397) data 0.001 (0.038) loss 3.6113 (3.1115) acc 28.1250 (31.6964) lr 1.9298e-03 eta 0:13:58\n",
            "epoch [8/50] batch [40/50] time 0.357 (0.391) data 0.000 (0.033) loss 2.4688 (3.1011) acc 37.5000 (31.8750) lr 1.9298e-03 eta 0:13:46\n",
            "epoch [8/50] batch [45/50] time 0.357 (0.387) data 0.000 (0.030) loss 3.3887 (3.1313) acc 31.2500 (31.5278) lr 1.9298e-03 eta 0:13:35\n",
            "epoch [8/50] batch [50/50] time 0.358 (0.384) data 0.000 (0.027) loss 2.5059 (3.1214) acc 40.6250 (31.4375) lr 1.9048e-03 eta 0:13:26\n",
            "epoch [9/50] batch [5/50] time 0.353 (0.604) data 0.001 (0.219) loss 3.0527 (3.3137) acc 34.3750 (28.7500) lr 1.9048e-03 eta 0:21:04\n",
            "epoch [9/50] batch [10/50] time 0.359 (0.485) data 0.001 (0.113) loss 3.7344 (3.3180) acc 12.5000 (27.8125) lr 1.9048e-03 eta 0:16:52\n",
            "epoch [9/50] batch [15/50] time 0.364 (0.445) data 0.009 (0.077) loss 2.5547 (3.2073) acc 40.6250 (30.2083) lr 1.9048e-03 eta 0:15:27\n",
            "epoch [9/50] batch [20/50] time 0.361 (0.425) data 0.001 (0.060) loss 3.8398 (3.2134) acc 25.0000 (30.7812) lr 1.9048e-03 eta 0:14:43\n",
            "epoch [9/50] batch [25/50] time 0.356 (0.412) data 0.000 (0.048) loss 3.0273 (3.1557) acc 31.2500 (30.5000) lr 1.9048e-03 eta 0:14:15\n",
            "epoch [9/50] batch [30/50] time 0.355 (0.403) data 0.000 (0.040) loss 2.9180 (3.1497) acc 43.7500 (30.8333) lr 1.9048e-03 eta 0:13:54\n",
            "epoch [9/50] batch [35/50] time 0.356 (0.396) data 0.001 (0.035) loss 3.0488 (3.1798) acc 34.3750 (30.8929) lr 1.9048e-03 eta 0:13:38\n",
            "epoch [9/50] batch [40/50] time 0.355 (0.391) data 0.000 (0.030) loss 3.5508 (3.1783) acc 12.5000 (30.8594) lr 1.9048e-03 eta 0:13:26\n",
            "epoch [9/50] batch [45/50] time 0.360 (0.388) data 0.000 (0.027) loss 3.0273 (3.1583) acc 31.2500 (30.8333) lr 1.9048e-03 eta 0:13:16\n",
            "epoch [9/50] batch [50/50] time 0.358 (0.384) data 0.001 (0.024) loss 3.3223 (3.1657) acc 28.1250 (30.4375) lr 1.8763e-03 eta 0:13:08\n",
            "epoch [10/50] batch [5/50] time 0.364 (0.718) data 0.008 (0.285) loss 3.2500 (2.7895) acc 21.8750 (37.5000) lr 1.8763e-03 eta 0:24:28\n",
            "epoch [10/50] batch [10/50] time 0.366 (0.541) data 0.007 (0.146) loss 3.2871 (3.0311) acc 28.1250 (31.8750) lr 1.8763e-03 eta 0:18:22\n",
            "epoch [10/50] batch [15/50] time 0.366 (0.483) data 0.008 (0.101) loss 3.2227 (3.1301) acc 21.8750 (30.8333) lr 1.8763e-03 eta 0:16:22\n",
            "epoch [10/50] batch [20/50] time 0.366 (0.454) data 0.008 (0.077) loss 2.8164 (3.1782) acc 43.7500 (30.4688) lr 1.8763e-03 eta 0:15:21\n",
            "epoch [10/50] batch [25/50] time 0.355 (0.435) data 0.000 (0.062) loss 3.2266 (3.1830) acc 34.3750 (29.7500) lr 1.8763e-03 eta 0:14:40\n",
            "epoch [10/50] batch [30/50] time 0.355 (0.422) data 0.001 (0.052) loss 3.4883 (3.2042) acc 12.5000 (28.1250) lr 1.8763e-03 eta 0:14:11\n",
            "epoch [10/50] batch [35/50] time 0.356 (0.412) data 0.001 (0.045) loss 2.9414 (3.1917) acc 31.2500 (28.3036) lr 1.8763e-03 eta 0:13:50\n",
            "epoch [10/50] batch [40/50] time 0.355 (0.405) data 0.000 (0.039) loss 2.6875 (3.1731) acc 34.3750 (28.1250) lr 1.8763e-03 eta 0:13:34\n",
            "epoch [10/50] batch [45/50] time 0.355 (0.400) data 0.000 (0.035) loss 2.8262 (3.1669) acc 31.2500 (27.9861) lr 1.8763e-03 eta 0:13:21\n",
            "epoch [10/50] batch [50/50] time 0.357 (0.395) data 0.000 (0.031) loss 3.5059 (3.1618) acc 31.2500 (28.2500) lr 1.8443e-03 eta 0:13:10\n",
            "epoch [11/50] batch [5/50] time 0.368 (0.778) data 0.007 (0.319) loss 3.3906 (3.1250) acc 25.0000 (28.1250) lr 1.8443e-03 eta 0:25:52\n",
            "epoch [11/50] batch [10/50] time 0.368 (0.571) data 0.009 (0.162) loss 2.6855 (2.9805) acc 40.6250 (30.6250) lr 1.8443e-03 eta 0:18:55\n",
            "epoch [11/50] batch [15/50] time 0.364 (0.502) data 0.007 (0.111) loss 3.5352 (3.0396) acc 15.6250 (31.4583) lr 1.8443e-03 eta 0:16:36\n",
            "epoch [11/50] batch [20/50] time 0.354 (0.466) data 0.001 (0.084) loss 3.3691 (3.0597) acc 25.0000 (30.9375) lr 1.8443e-03 eta 0:15:23\n",
            "epoch [11/50] batch [25/50] time 0.356 (0.445) data 0.001 (0.068) loss 3.5586 (3.0723) acc 25.0000 (30.6250) lr 1.8443e-03 eta 0:14:38\n",
            "epoch [11/50] batch [30/50] time 0.354 (0.430) data 0.001 (0.057) loss 3.3770 (3.0948) acc 28.1250 (30.8333) lr 1.8443e-03 eta 0:14:07\n",
            "epoch [11/50] batch [35/50] time 0.356 (0.420) data 0.001 (0.049) loss 2.8398 (3.0908) acc 43.7500 (31.2500) lr 1.8443e-03 eta 0:13:44\n",
            "epoch [11/50] batch [40/50] time 0.355 (0.411) data 0.000 (0.043) loss 3.3047 (3.0991) acc 25.0000 (30.8594) lr 1.8443e-03 eta 0:13:26\n",
            "epoch [11/50] batch [45/50] time 0.353 (0.405) data 0.000 (0.038) loss 2.5059 (3.0753) acc 46.8750 (31.5278) lr 1.8443e-03 eta 0:13:12\n",
            "epoch [11/50] batch [50/50] time 0.355 (0.400) data 0.001 (0.034) loss 3.5156 (3.0911) acc 25.0000 (31.0625) lr 1.8090e-03 eta 0:13:00\n",
            "epoch [12/50] batch [5/50] time 0.359 (0.875) data 0.007 (0.447) loss 2.8262 (2.7387) acc 37.5000 (36.2500) lr 1.8090e-03 eta 0:28:21\n",
            "epoch [12/50] batch [10/50] time 0.364 (0.618) data 0.007 (0.226) loss 2.4766 (2.9336) acc 53.1250 (34.3750) lr 1.8090e-03 eta 0:19:59\n",
            "epoch [12/50] batch [15/50] time 0.353 (0.531) data 0.001 (0.151) loss 2.5566 (2.9281) acc 43.7500 (34.3750) lr 1.8090e-03 eta 0:17:06\n",
            "epoch [12/50] batch [20/50] time 0.356 (0.487) data 0.001 (0.114) loss 2.8125 (2.9444) acc 31.2500 (33.2812) lr 1.8090e-03 eta 0:15:39\n",
            "epoch [12/50] batch [25/50] time 0.354 (0.460) data 0.000 (0.091) loss 2.7070 (2.9724) acc 31.2500 (32.2500) lr 1.8090e-03 eta 0:14:46\n",
            "epoch [12/50] batch [30/50] time 0.355 (0.443) data 0.001 (0.076) loss 3.4180 (2.9876) acc 21.8750 (31.9792) lr 1.8090e-03 eta 0:14:10\n",
            "epoch [12/50] batch [35/50] time 0.354 (0.430) data 0.001 (0.065) loss 3.1562 (2.9794) acc 31.2500 (32.4107) lr 1.8090e-03 eta 0:13:44\n",
            "epoch [12/50] batch [40/50] time 0.355 (0.421) data 0.001 (0.057) loss 3.5117 (2.9996) acc 25.0000 (31.8750) lr 1.8090e-03 eta 0:13:23\n",
            "epoch [12/50] batch [45/50] time 0.356 (0.414) data 0.001 (0.051) loss 3.1035 (2.9980) acc 28.1250 (32.0833) lr 1.8090e-03 eta 0:13:08\n",
            "epoch [12/50] batch [50/50] time 0.356 (0.408) data 0.001 (0.046) loss 2.8574 (2.9955) acc 43.7500 (32.3125) lr 1.7705e-03 eta 0:12:55\n",
            "epoch [13/50] batch [5/50] time 0.360 (0.602) data 0.004 (0.186) loss 2.7949 (2.9504) acc 31.2500 (30.0000) lr 1.7705e-03 eta 0:19:01\n",
            "epoch [13/50] batch [10/50] time 0.355 (0.479) data 0.001 (0.094) loss 3.0078 (3.1004) acc 40.6250 (30.3125) lr 1.7705e-03 eta 0:15:06\n",
            "epoch [13/50] batch [15/50] time 0.354 (0.438) data 0.001 (0.063) loss 3.2734 (3.0553) acc 25.0000 (30.4167) lr 1.7705e-03 eta 0:13:45\n",
            "epoch [13/50] batch [20/50] time 0.357 (0.418) data 0.001 (0.047) loss 2.7129 (2.9797) acc 40.6250 (32.6562) lr 1.7705e-03 eta 0:13:05\n",
            "epoch [13/50] batch [25/50] time 0.355 (0.406) data 0.000 (0.038) loss 3.5742 (3.0187) acc 25.0000 (31.7500) lr 1.7705e-03 eta 0:12:40\n",
            "epoch [13/50] batch [30/50] time 0.361 (0.398) data 0.007 (0.032) loss 2.7578 (3.0165) acc 34.3750 (31.8750) lr 1.7705e-03 eta 0:12:24\n",
            "epoch [13/50] batch [35/50] time 0.361 (0.393) data 0.005 (0.028) loss 3.2559 (3.0352) acc 34.3750 (31.9643) lr 1.7705e-03 eta 0:12:13\n",
            "epoch [13/50] batch [40/50] time 0.359 (0.389) data 0.001 (0.025) loss 2.5332 (3.0319) acc 46.8750 (31.8750) lr 1.7705e-03 eta 0:12:03\n",
            "epoch [13/50] batch [45/50] time 0.357 (0.385) data 0.001 (0.022) loss 3.1328 (3.0280) acc 25.0000 (31.9444) lr 1.7705e-03 eta 0:11:54\n",
            "epoch [13/50] batch [50/50] time 0.356 (0.382) data 0.000 (0.020) loss 3.0781 (3.0192) acc 31.2500 (32.2500) lr 1.7290e-03 eta 0:11:47\n",
            "epoch [14/50] batch [5/50] time 0.350 (0.565) data 0.001 (0.164) loss 3.2871 (2.7832) acc 31.2500 (37.5000) lr 1.7290e-03 eta 0:17:23\n",
            "epoch [14/50] batch [10/50] time 0.357 (0.461) data 0.001 (0.082) loss 2.8672 (2.8021) acc 25.0000 (35.0000) lr 1.7290e-03 eta 0:14:07\n",
            "epoch [14/50] batch [15/50] time 0.361 (0.426) data 0.003 (0.055) loss 2.9844 (2.9671) acc 28.1250 (32.2917) lr 1.7290e-03 eta 0:13:01\n",
            "epoch [14/50] batch [20/50] time 0.355 (0.408) data 0.001 (0.042) loss 3.2109 (3.0059) acc 40.6250 (32.5000) lr 1.7290e-03 eta 0:12:27\n",
            "epoch [14/50] batch [25/50] time 0.363 (0.399) data 0.008 (0.034) loss 3.0254 (3.0171) acc 31.2500 (31.5000) lr 1.7290e-03 eta 0:12:08\n",
            "epoch [14/50] batch [30/50] time 0.364 (0.393) data 0.007 (0.029) loss 2.0801 (3.0462) acc 53.1250 (31.3542) lr 1.7290e-03 eta 0:11:55\n",
            "epoch [14/50] batch [35/50] time 0.356 (0.389) data 0.001 (0.026) loss 3.4590 (3.0593) acc 15.6250 (30.6250) lr 1.7290e-03 eta 0:11:45\n",
            "epoch [14/50] batch [40/50] time 0.355 (0.384) data 0.001 (0.022) loss 3.4297 (3.0836) acc 18.7500 (29.8438) lr 1.7290e-03 eta 0:11:35\n",
            "epoch [14/50] batch [45/50] time 0.358 (0.381) data 0.001 (0.020) loss 2.8848 (3.0793) acc 37.5000 (30.0000) lr 1.7290e-03 eta 0:11:28\n",
            "epoch [14/50] batch [50/50] time 0.355 (0.379) data 0.000 (0.018) loss 3.2090 (3.1002) acc 40.6250 (30.0000) lr 1.6845e-03 eta 0:11:22\n",
            "epoch [15/50] batch [5/50] time 0.354 (0.524) data 0.001 (0.128) loss 3.1309 (2.9020) acc 34.3750 (38.7500) lr 1.6845e-03 eta 0:15:40\n",
            "epoch [15/50] batch [10/50] time 0.356 (0.439) data 0.001 (0.064) loss 3.1113 (2.9570) acc 15.6250 (33.4375) lr 1.6845e-03 eta 0:13:06\n",
            "epoch [15/50] batch [15/50] time 0.353 (0.411) data 0.001 (0.043) loss 2.8613 (3.0378) acc 28.1250 (31.8750) lr 1.6845e-03 eta 0:12:14\n",
            "epoch [15/50] batch [20/50] time 0.356 (0.398) data 0.001 (0.032) loss 3.1348 (3.0392) acc 28.1250 (31.4062) lr 1.6845e-03 eta 0:11:47\n",
            "epoch [15/50] batch [25/50] time 0.360 (0.390) data 0.007 (0.027) loss 3.1523 (3.0450) acc 25.0000 (31.3750) lr 1.6845e-03 eta 0:11:31\n",
            "epoch [15/50] batch [30/50] time 0.357 (0.385) data 0.001 (0.023) loss 3.0840 (3.0184) acc 31.2500 (31.8750) lr 1.6845e-03 eta 0:11:20\n",
            "epoch [15/50] batch [35/50] time 0.356 (0.381) data 0.001 (0.020) loss 3.3711 (3.0559) acc 25.0000 (31.3393) lr 1.6845e-03 eta 0:11:12\n",
            "epoch [15/50] batch [40/50] time 0.356 (0.378) data 0.001 (0.018) loss 3.0996 (3.0351) acc 28.1250 (32.0312) lr 1.6845e-03 eta 0:11:05\n",
            "epoch [15/50] batch [45/50] time 0.358 (0.376) data 0.001 (0.016) loss 2.6074 (3.0302) acc 37.5000 (32.0833) lr 1.6845e-03 eta 0:10:59\n",
            "epoch [15/50] batch [50/50] time 0.356 (0.374) data 0.000 (0.014) loss 2.4727 (3.0091) acc 43.7500 (32.3125) lr 1.6374e-03 eta 0:10:53\n",
            "epoch [16/50] batch [5/50] time 0.355 (0.553) data 0.001 (0.170) loss 2.8848 (3.0195) acc 34.3750 (34.3750) lr 1.6374e-03 eta 0:16:04\n",
            "epoch [16/50] batch [10/50] time 0.356 (0.454) data 0.001 (0.085) loss 3.5156 (3.1297) acc 25.0000 (30.6250) lr 1.6374e-03 eta 0:13:09\n",
            "epoch [16/50] batch [15/50] time 0.355 (0.421) data 0.001 (0.057) loss 2.8750 (3.0885) acc 31.2500 (31.4583) lr 1.6374e-03 eta 0:12:10\n",
            "epoch [16/50] batch [20/50] time 0.355 (0.405) data 0.001 (0.043) loss 2.6094 (3.0461) acc 31.2500 (31.2500) lr 1.6374e-03 eta 0:11:39\n",
            "epoch [16/50] batch [25/50] time 0.357 (0.395) data 0.000 (0.035) loss 2.8262 (3.0055) acc 34.3750 (31.8750) lr 1.6374e-03 eta 0:11:21\n",
            "epoch [16/50] batch [30/50] time 0.366 (0.390) data 0.008 (0.030) loss 3.0293 (2.9729) acc 34.3750 (31.5625) lr 1.6374e-03 eta 0:11:10\n",
            "epoch [16/50] batch [35/50] time 0.356 (0.386) data 0.001 (0.026) loss 2.4375 (2.9652) acc 34.3750 (31.4286) lr 1.6374e-03 eta 0:11:01\n",
            "epoch [16/50] batch [40/50] time 0.358 (0.382) data 0.001 (0.023) loss 3.0039 (2.9698) acc 31.2500 (30.9375) lr 1.6374e-03 eta 0:10:53\n",
            "epoch [16/50] batch [45/50] time 0.358 (0.379) data 0.001 (0.020) loss 3.6133 (2.9826) acc 21.8750 (30.8333) lr 1.6374e-03 eta 0:10:46\n",
            "epoch [16/50] batch [50/50] time 0.357 (0.377) data 0.001 (0.018) loss 2.9395 (2.9885) acc 37.5000 (31.0000) lr 1.5878e-03 eta 0:10:40\n",
            "epoch [17/50] batch [5/50] time 0.352 (0.588) data 0.001 (0.207) loss 3.3555 (2.9262) acc 31.2500 (35.0000) lr 1.5878e-03 eta 0:16:37\n",
            "epoch [17/50] batch [10/50] time 0.355 (0.472) data 0.000 (0.104) loss 3.2148 (2.9193) acc 31.2500 (33.7500) lr 1.5878e-03 eta 0:13:17\n",
            "epoch [17/50] batch [15/50] time 0.357 (0.433) data 0.001 (0.070) loss 3.2109 (2.9689) acc 31.2500 (33.1250) lr 1.5878e-03 eta 0:12:09\n",
            "epoch [17/50] batch [20/50] time 0.358 (0.414) data 0.001 (0.052) loss 3.6094 (2.9916) acc 18.7500 (32.6562) lr 1.5878e-03 eta 0:11:34\n",
            "epoch [17/50] batch [25/50] time 0.355 (0.402) data 0.001 (0.042) loss 2.9219 (2.9663) acc 37.5000 (32.7500) lr 1.5878e-03 eta 0:11:13\n",
            "epoch [17/50] batch [30/50] time 0.364 (0.395) data 0.008 (0.036) loss 2.7461 (2.9939) acc 37.5000 (32.6042) lr 1.5878e-03 eta 0:10:59\n",
            "epoch [17/50] batch [35/50] time 0.357 (0.390) data 0.001 (0.032) loss 2.9707 (2.9749) acc 28.1250 (33.1250) lr 1.5878e-03 eta 0:10:49\n",
            "epoch [17/50] batch [40/50] time 0.355 (0.386) data 0.001 (0.028) loss 3.1855 (2.9586) acc 31.2500 (33.4375) lr 1.5878e-03 eta 0:10:40\n",
            "epoch [17/50] batch [45/50] time 0.355 (0.383) data 0.001 (0.025) loss 2.7383 (2.9917) acc 28.1250 (32.7778) lr 1.5878e-03 eta 0:10:33\n",
            "epoch [17/50] batch [50/50] time 0.355 (0.380) data 0.001 (0.022) loss 3.1895 (2.9927) acc 15.6250 (32.4375) lr 1.5358e-03 eta 0:10:27\n",
            "epoch [18/50] batch [5/50] time 0.351 (0.565) data 0.001 (0.172) loss 2.6172 (2.7754) acc 43.7500 (40.6250) lr 1.5358e-03 eta 0:15:29\n",
            "epoch [18/50] batch [10/50] time 0.356 (0.460) data 0.001 (0.086) loss 3.3691 (2.7797) acc 25.0000 (37.8125) lr 1.5358e-03 eta 0:12:35\n",
            "epoch [18/50] batch [15/50] time 0.365 (0.428) data 0.007 (0.059) loss 3.2949 (2.7654) acc 21.8750 (36.0417) lr 1.5358e-03 eta 0:11:39\n",
            "epoch [18/50] batch [20/50] time 0.356 (0.410) data 0.001 (0.045) loss 3.2715 (2.8721) acc 21.8750 (33.7500) lr 1.5358e-03 eta 0:11:09\n",
            "epoch [18/50] batch [25/50] time 0.365 (0.400) data 0.007 (0.036) loss 3.0898 (2.8670) acc 37.5000 (34.1250) lr 1.5358e-03 eta 0:10:50\n",
            "epoch [18/50] batch [30/50] time 0.358 (0.393) data 0.001 (0.031) loss 3.3633 (2.9364) acc 21.8750 (33.7500) lr 1.5358e-03 eta 0:10:37\n",
            "epoch [18/50] batch [35/50] time 0.358 (0.389) data 0.001 (0.027) loss 3.0488 (2.9153) acc 31.2500 (34.0179) lr 1.5358e-03 eta 0:10:28\n",
            "epoch [18/50] batch [40/50] time 0.359 (0.385) data 0.001 (0.024) loss 3.0371 (2.9339) acc 34.3750 (33.9062) lr 1.5358e-03 eta 0:10:20\n",
            "epoch [18/50] batch [45/50] time 0.356 (0.382) data 0.001 (0.022) loss 3.0703 (2.9538) acc 21.8750 (33.5417) lr 1.5358e-03 eta 0:10:13\n",
            "epoch [18/50] batch [50/50] time 0.357 (0.380) data 0.000 (0.019) loss 3.3711 (2.9420) acc 15.6250 (33.3125) lr 1.4818e-03 eta 0:10:07\n",
            "epoch [19/50] batch [5/50] time 0.350 (0.585) data 0.001 (0.174) loss 3.4609 (3.0387) acc 25.0000 (27.5000) lr 1.4818e-03 eta 0:15:32\n",
            "epoch [19/50] batch [10/50] time 0.359 (0.471) data 0.000 (0.087) loss 3.1445 (3.0254) acc 21.8750 (27.1875) lr 1.4818e-03 eta 0:12:28\n",
            "epoch [19/50] batch [15/50] time 0.356 (0.433) data 0.001 (0.058) loss 2.8965 (3.0275) acc 28.1250 (26.8750) lr 1.4818e-03 eta 0:11:25\n",
            "epoch [19/50] batch [20/50] time 0.354 (0.414) data 0.001 (0.044) loss 2.9395 (3.0134) acc 31.2500 (27.8125) lr 1.4818e-03 eta 0:10:53\n",
            "epoch [19/50] batch [25/50] time 0.365 (0.403) data 0.007 (0.036) loss 3.1309 (2.9863) acc 34.3750 (30.1250) lr 1.4818e-03 eta 0:10:34\n",
            "epoch [19/50] batch [30/50] time 0.359 (0.396) data 0.001 (0.030) loss 2.5586 (2.9839) acc 34.3750 (31.2500) lr 1.4818e-03 eta 0:10:21\n",
            "epoch [19/50] batch [35/50] time 0.356 (0.391) data 0.001 (0.026) loss 2.8262 (2.9748) acc 28.1250 (31.9643) lr 1.4818e-03 eta 0:10:11\n",
            "epoch [19/50] batch [40/50] time 0.357 (0.387) data 0.001 (0.023) loss 2.5859 (2.9603) acc 37.5000 (32.5000) lr 1.4818e-03 eta 0:10:03\n",
            "epoch [19/50] batch [45/50] time 0.356 (0.383) data 0.000 (0.020) loss 3.0508 (2.9653) acc 34.3750 (32.3611) lr 1.4818e-03 eta 0:09:55\n",
            "epoch [19/50] batch [50/50] time 0.354 (0.380) data 0.001 (0.018) loss 2.9180 (2.9844) acc 31.2500 (32.0000) lr 1.4258e-03 eta 0:09:49\n",
            "epoch [20/50] batch [5/50] time 0.353 (0.583) data 0.001 (0.204) loss 2.0957 (2.5652) acc 50.0000 (37.5000) lr 1.4258e-03 eta 0:15:01\n",
            "epoch [20/50] batch [10/50] time 0.359 (0.470) data 0.001 (0.102) loss 3.0312 (2.7617) acc 28.1250 (35.0000) lr 1.4258e-03 eta 0:12:03\n",
            "epoch [20/50] batch [15/50] time 0.356 (0.432) data 0.000 (0.069) loss 2.4434 (2.7586) acc 50.0000 (35.0000) lr 1.4258e-03 eta 0:11:02\n",
            "epoch [20/50] batch [20/50] time 0.372 (0.414) data 0.007 (0.052) loss 3.1348 (2.8034) acc 25.0000 (33.9062) lr 1.4258e-03 eta 0:10:32\n",
            "epoch [20/50] batch [25/50] time 0.358 (0.403) data 0.003 (0.042) loss 2.8965 (2.8148) acc 28.1250 (33.7500) lr 1.4258e-03 eta 0:10:15\n",
            "epoch [20/50] batch [30/50] time 0.368 (0.397) data 0.009 (0.036) loss 2.5566 (2.7598) acc 46.8750 (35.8333) lr 1.4258e-03 eta 0:10:02\n",
            "epoch [20/50] batch [35/50] time 0.358 (0.392) data 0.001 (0.032) loss 3.4160 (2.8000) acc 25.0000 (35.1786) lr 1.4258e-03 eta 0:09:53\n",
            "epoch [20/50] batch [40/50] time 0.355 (0.387) data 0.001 (0.028) loss 3.3008 (2.8449) acc 28.1250 (34.5312) lr 1.4258e-03 eta 0:09:44\n",
            "epoch [20/50] batch [45/50] time 0.356 (0.384) data 0.000 (0.025) loss 3.1895 (2.8900) acc 28.1250 (33.4028) lr 1.4258e-03 eta 0:09:37\n",
            "epoch [20/50] batch [50/50] time 0.355 (0.381) data 0.000 (0.022) loss 3.2871 (2.8946) acc 21.8750 (33.5000) lr 1.3681e-03 eta 0:09:31\n",
            "epoch [21/50] batch [5/50] time 0.350 (0.559) data 0.001 (0.174) loss 2.5801 (2.8750) acc 37.5000 (33.1250) lr 1.3681e-03 eta 0:13:55\n",
            "epoch [21/50] batch [10/50] time 0.358 (0.458) data 0.001 (0.087) loss 3.1055 (3.0299) acc 25.0000 (30.3125) lr 1.3681e-03 eta 0:11:22\n",
            "epoch [21/50] batch [15/50] time 0.355 (0.424) data 0.000 (0.058) loss 2.4551 (2.9620) acc 37.5000 (31.2500) lr 1.3681e-03 eta 0:10:29\n",
            "epoch [21/50] batch [20/50] time 0.361 (0.409) data 0.001 (0.046) loss 2.9102 (2.9377) acc 34.3750 (32.5000) lr 1.3681e-03 eta 0:10:04\n",
            "epoch [21/50] batch [25/50] time 0.367 (0.400) data 0.008 (0.038) loss 2.9316 (2.9392) acc 28.1250 (33.0000) lr 1.3681e-03 eta 0:09:49\n",
            "epoch [21/50] batch [30/50] time 0.357 (0.394) data 0.001 (0.033) loss 2.7520 (2.9161) acc 43.7500 (33.4375) lr 1.3681e-03 eta 0:09:39\n",
            "epoch [21/50] batch [35/50] time 0.359 (0.390) data 0.006 (0.029) loss 3.2324 (2.9164) acc 21.8750 (33.3036) lr 1.3681e-03 eta 0:09:30\n",
            "epoch [21/50] batch [40/50] time 0.356 (0.386) data 0.001 (0.026) loss 3.2109 (2.9250) acc 25.0000 (33.5156) lr 1.3681e-03 eta 0:09:23\n",
            "epoch [21/50] batch [45/50] time 0.356 (0.382) data 0.000 (0.023) loss 3.2461 (2.9112) acc 31.2500 (33.5417) lr 1.3681e-03 eta 0:09:16\n",
            "epoch [21/50] batch [50/50] time 0.355 (0.380) data 0.000 (0.021) loss 2.7715 (2.8923) acc 37.5000 (33.5000) lr 1.3090e-03 eta 0:09:10\n",
            "epoch [22/50] batch [5/50] time 0.355 (0.569) data 0.001 (0.174) loss 3.1699 (2.7730) acc 37.5000 (38.1250) lr 1.3090e-03 eta 0:13:42\n",
            "epoch [22/50] batch [10/50] time 0.356 (0.462) data 0.001 (0.088) loss 2.2070 (2.8080) acc 40.6250 (35.0000) lr 1.3090e-03 eta 0:11:05\n",
            "epoch [22/50] batch [15/50] time 0.356 (0.427) data 0.000 (0.059) loss 2.8457 (2.8145) acc 34.3750 (34.5833) lr 1.3090e-03 eta 0:10:12\n",
            "epoch [22/50] batch [20/50] time 0.360 (0.409) data 0.001 (0.044) loss 2.5488 (2.8152) acc 37.5000 (35.0000) lr 1.3090e-03 eta 0:09:44\n",
            "epoch [22/50] batch [25/50] time 0.359 (0.400) data 0.001 (0.036) loss 3.0352 (2.8780) acc 37.5000 (34.3750) lr 1.3090e-03 eta 0:09:29\n",
            "epoch [22/50] batch [30/50] time 0.360 (0.394) data 0.001 (0.031) loss 2.8340 (2.9054) acc 37.5000 (33.9583) lr 1.3090e-03 eta 0:09:19\n",
            "epoch [22/50] batch [35/50] time 0.356 (0.389) data 0.001 (0.028) loss 2.7969 (2.8890) acc 28.1250 (34.3750) lr 1.3090e-03 eta 0:09:11\n",
            "epoch [22/50] batch [40/50] time 0.355 (0.385) data 0.000 (0.024) loss 2.9785 (2.8987) acc 43.7500 (34.2969) lr 1.3090e-03 eta 0:09:03\n",
            "epoch [22/50] batch [45/50] time 0.356 (0.382) data 0.000 (0.022) loss 2.8965 (2.8989) acc 40.6250 (34.3750) lr 1.3090e-03 eta 0:08:56\n",
            "epoch [22/50] batch [50/50] time 0.354 (0.379) data 0.000 (0.019) loss 2.1953 (2.8729) acc 50.0000 (34.7500) lr 1.2487e-03 eta 0:08:50\n",
            "epoch [23/50] batch [5/50] time 0.351 (0.577) data 0.001 (0.195) loss 2.9316 (2.7426) acc 31.2500 (38.7500) lr 1.2487e-03 eta 0:13:25\n",
            "epoch [23/50] batch [10/50] time 0.357 (0.467) data 0.001 (0.098) loss 2.8750 (2.8943) acc 31.2500 (37.5000) lr 1.2487e-03 eta 0:10:48\n",
            "epoch [23/50] batch [15/50] time 0.355 (0.430) data 0.001 (0.065) loss 2.9785 (2.9188) acc 37.5000 (34.3750) lr 1.2487e-03 eta 0:09:55\n",
            "epoch [23/50] batch [20/50] time 0.368 (0.413) data 0.015 (0.051) loss 3.8555 (2.9383) acc 21.8750 (34.5312) lr 1.2487e-03 eta 0:09:29\n",
            "epoch [23/50] batch [25/50] time 0.363 (0.404) data 0.007 (0.042) loss 3.2363 (2.9296) acc 21.8750 (34.0000) lr 1.2487e-03 eta 0:09:15\n",
            "epoch [23/50] batch [30/50] time 0.367 (0.397) data 0.014 (0.036) loss 3.2812 (2.9616) acc 25.0000 (33.0208) lr 1.2487e-03 eta 0:09:04\n",
            "epoch [23/50] batch [35/50] time 0.358 (0.393) data 0.001 (0.032) loss 2.9043 (2.9266) acc 37.5000 (33.3036) lr 1.2487e-03 eta 0:08:56\n",
            "epoch [23/50] batch [40/50] time 0.355 (0.388) data 0.000 (0.028) loss 2.3320 (2.8973) acc 50.0000 (34.2969) lr 1.2487e-03 eta 0:08:47\n",
            "epoch [23/50] batch [45/50] time 0.354 (0.384) data 0.000 (0.025) loss 2.5605 (2.8789) acc 34.3750 (34.3056) lr 1.2487e-03 eta 0:08:40\n",
            "epoch [23/50] batch [50/50] time 0.357 (0.382) data 0.000 (0.023) loss 2.9746 (2.8778) acc 21.8750 (33.8750) lr 1.1874e-03 eta 0:08:35\n",
            "epoch [24/50] batch [5/50] time 0.351 (0.568) data 0.001 (0.175) loss 3.1074 (3.0953) acc 34.3750 (30.0000) lr 1.1874e-03 eta 0:12:44\n",
            "epoch [24/50] batch [10/50] time 0.358 (0.462) data 0.001 (0.088) loss 2.2656 (2.7922) acc 40.6250 (36.2500) lr 1.1874e-03 eta 0:10:19\n",
            "epoch [24/50] batch [15/50] time 0.364 (0.428) data 0.009 (0.059) loss 2.6777 (2.7681) acc 37.5000 (36.0417) lr 1.1874e-03 eta 0:09:30\n",
            "epoch [24/50] batch [20/50] time 0.364 (0.411) data 0.007 (0.046) loss 2.7227 (2.7383) acc 37.5000 (36.2500) lr 1.1874e-03 eta 0:09:06\n",
            "epoch [24/50] batch [25/50] time 0.364 (0.401) data 0.008 (0.037) loss 2.6777 (2.7688) acc 40.6250 (35.5000) lr 1.1874e-03 eta 0:08:51\n",
            "epoch [24/50] batch [30/50] time 0.368 (0.395) data 0.009 (0.032) loss 2.9160 (2.8486) acc 31.2500 (33.8542) lr 1.1874e-03 eta 0:08:41\n",
            "epoch [24/50] batch [35/50] time 0.356 (0.390) data 0.001 (0.028) loss 3.1797 (2.8847) acc 25.0000 (33.1250) lr 1.1874e-03 eta 0:08:33\n",
            "epoch [24/50] batch [40/50] time 0.356 (0.386) data 0.000 (0.025) loss 2.6270 (2.8832) acc 31.2500 (33.1250) lr 1.1874e-03 eta 0:08:25\n",
            "epoch [24/50] batch [45/50] time 0.354 (0.383) data 0.000 (0.022) loss 2.9766 (2.8900) acc 31.2500 (33.4722) lr 1.1874e-03 eta 0:08:19\n",
            "epoch [24/50] batch [50/50] time 0.356 (0.380) data 0.000 (0.020) loss 2.8516 (2.8827) acc 28.1250 (34.0000) lr 1.1253e-03 eta 0:08:13\n",
            "epoch [25/50] batch [5/50] time 0.370 (0.570) data 0.008 (0.181) loss 2.4141 (2.9699) acc 43.7500 (33.1250) lr 1.1253e-03 eta 0:12:18\n",
            "epoch [25/50] batch [10/50] time 0.354 (0.464) data 0.001 (0.092) loss 2.8887 (3.0037) acc 31.2500 (31.8750) lr 1.1253e-03 eta 0:09:58\n",
            "epoch [25/50] batch [15/50] time 0.360 (0.429) data 0.001 (0.062) loss 2.7598 (2.8658) acc 37.5000 (33.5417) lr 1.1253e-03 eta 0:09:11\n",
            "epoch [25/50] batch [20/50] time 0.367 (0.413) data 0.008 (0.048) loss 2.9395 (2.8494) acc 40.6250 (34.0625) lr 1.1253e-03 eta 0:08:48\n",
            "epoch [25/50] batch [25/50] time 0.354 (0.403) data 0.001 (0.039) loss 3.2344 (2.8398) acc 21.8750 (34.0000) lr 1.1253e-03 eta 0:08:33\n",
            "epoch [25/50] batch [30/50] time 0.369 (0.396) data 0.009 (0.034) loss 2.7832 (2.8723) acc 31.2500 (33.4375) lr 1.1253e-03 eta 0:08:23\n",
            "epoch [25/50] batch [35/50] time 0.356 (0.392) data 0.001 (0.030) loss 2.8711 (2.8695) acc 34.3750 (33.6607) lr 1.1253e-03 eta 0:08:15\n",
            "epoch [25/50] batch [40/50] time 0.355 (0.387) data 0.000 (0.026) loss 2.5859 (2.8940) acc 31.2500 (33.2812) lr 1.1253e-03 eta 0:08:07\n",
            "epoch [25/50] batch [45/50] time 0.357 (0.384) data 0.000 (0.023) loss 2.7266 (2.9062) acc 37.5000 (33.4028) lr 1.1253e-03 eta 0:08:01\n",
            "epoch [25/50] batch [50/50] time 0.355 (0.381) data 0.000 (0.021) loss 2.4004 (2.8812) acc 46.8750 (33.9375) lr 1.0628e-03 eta 0:07:56\n",
            "epoch [26/50] batch [5/50] time 0.353 (0.575) data 0.001 (0.176) loss 2.7656 (2.5828) acc 40.6250 (40.0000) lr 1.0628e-03 eta 0:11:56\n",
            "epoch [26/50] batch [10/50] time 0.356 (0.465) data 0.000 (0.088) loss 3.1738 (2.6569) acc 37.5000 (39.6875) lr 1.0628e-03 eta 0:09:37\n",
            "epoch [26/50] batch [15/50] time 0.357 (0.430) data 0.001 (0.060) loss 3.5059 (2.7305) acc 37.5000 (38.5417) lr 1.0628e-03 eta 0:08:51\n",
            "epoch [26/50] batch [20/50] time 0.370 (0.413) data 0.012 (0.046) loss 3.1133 (2.7824) acc 31.2500 (37.1875) lr 1.0628e-03 eta 0:08:28\n",
            "epoch [26/50] batch [25/50] time 0.367 (0.404) data 0.008 (0.038) loss 3.3945 (2.8094) acc 25.0000 (36.2500) lr 1.0628e-03 eta 0:08:14\n",
            "epoch [26/50] batch [30/50] time 0.369 (0.397) data 0.009 (0.033) loss 2.4883 (2.8080) acc 40.6250 (36.0417) lr 1.0628e-03 eta 0:08:04\n",
            "epoch [26/50] batch [35/50] time 0.358 (0.391) data 0.001 (0.028) loss 3.6465 (2.8376) acc 21.8750 (35.1786) lr 1.0628e-03 eta 0:07:55\n",
            "epoch [26/50] batch [40/50] time 0.357 (0.387) data 0.001 (0.025) loss 2.6152 (2.8284) acc 31.2500 (35.3906) lr 1.0628e-03 eta 0:07:48\n",
            "epoch [26/50] batch [45/50] time 0.356 (0.384) data 0.000 (0.022) loss 3.0293 (2.8214) acc 34.3750 (35.8333) lr 1.0628e-03 eta 0:07:42\n",
            "epoch [26/50] batch [50/50] time 0.357 (0.381) data 0.000 (0.020) loss 3.0938 (2.8565) acc 25.0000 (35.3125) lr 1.0000e-03 eta 0:07:36\n",
            "epoch [27/50] batch [5/50] time 0.352 (0.569) data 0.000 (0.173) loss 2.7773 (2.8980) acc 37.5000 (30.0000) lr 1.0000e-03 eta 0:11:19\n",
            "epoch [27/50] batch [10/50] time 0.357 (0.464) data 0.001 (0.088) loss 3.0254 (2.9365) acc 25.0000 (30.0000) lr 1.0000e-03 eta 0:09:12\n",
            "epoch [27/50] batch [15/50] time 0.361 (0.429) data 0.007 (0.060) loss 2.7207 (2.8686) acc 31.2500 (31.2500) lr 1.0000e-03 eta 0:08:28\n",
            "epoch [27/50] batch [20/50] time 0.356 (0.413) data 0.001 (0.047) loss 3.0332 (2.8644) acc 37.5000 (32.9688) lr 1.0000e-03 eta 0:08:07\n",
            "epoch [27/50] batch [25/50] time 0.361 (0.403) data 0.004 (0.038) loss 2.9922 (2.8686) acc 28.1250 (32.7500) lr 1.0000e-03 eta 0:07:53\n",
            "epoch [27/50] batch [30/50] time 0.355 (0.397) data 0.001 (0.033) loss 3.9766 (2.9199) acc 18.7500 (32.3958) lr 1.0000e-03 eta 0:07:43\n",
            "epoch [27/50] batch [35/50] time 0.356 (0.391) data 0.001 (0.028) loss 3.1211 (2.9374) acc 31.2500 (32.3214) lr 1.0000e-03 eta 0:07:35\n",
            "epoch [27/50] batch [40/50] time 0.355 (0.386) data 0.000 (0.025) loss 2.9883 (2.9403) acc 25.0000 (31.9531) lr 1.0000e-03 eta 0:07:28\n",
            "epoch [27/50] batch [45/50] time 0.354 (0.383) data 0.000 (0.022) loss 2.2500 (2.9161) acc 46.8750 (32.6389) lr 1.0000e-03 eta 0:07:22\n",
            "epoch [27/50] batch [50/50] time 0.354 (0.380) data 0.000 (0.020) loss 2.6348 (2.8758) acc 28.1250 (33.3750) lr 9.3721e-04 eta 0:07:17\n",
            "epoch [28/50] batch [5/50] time 0.357 (0.581) data 0.001 (0.199) loss 2.6309 (2.6289) acc 21.8750 (36.8750) lr 9.3721e-04 eta 0:11:05\n",
            "epoch [28/50] batch [10/50] time 0.362 (0.470) data 0.007 (0.101) loss 2.5879 (2.7186) acc 37.5000 (36.2500) lr 9.3721e-04 eta 0:08:55\n",
            "epoch [28/50] batch [15/50] time 0.359 (0.434) data 0.005 (0.069) loss 2.8730 (2.7004) acc 43.7500 (37.5000) lr 9.3721e-04 eta 0:08:12\n",
            "epoch [28/50] batch [20/50] time 0.358 (0.416) data 0.003 (0.053) loss 2.2559 (2.6624) acc 40.6250 (37.5000) lr 9.3721e-04 eta 0:07:49\n",
            "epoch [28/50] batch [25/50] time 0.362 (0.405) data 0.008 (0.044) loss 2.5742 (2.7180) acc 31.2500 (36.3750) lr 9.3721e-04 eta 0:07:35\n",
            "epoch [28/50] batch [30/50] time 0.376 (0.399) data 0.016 (0.038) loss 2.4219 (2.7480) acc 43.7500 (36.0417) lr 9.3721e-04 eta 0:07:26\n",
            "epoch [28/50] batch [35/50] time 0.355 (0.393) data 0.001 (0.033) loss 2.9316 (2.7893) acc 31.2500 (35.7143) lr 9.3721e-04 eta 0:07:17\n",
            "epoch [28/50] batch [40/50] time 0.356 (0.388) data 0.000 (0.029) loss 3.2578 (2.7953) acc 21.8750 (35.5469) lr 9.3721e-04 eta 0:07:10\n",
            "epoch [28/50] batch [45/50] time 0.357 (0.384) data 0.000 (0.025) loss 3.1504 (2.7991) acc 21.8750 (35.2083) lr 9.3721e-04 eta 0:07:04\n",
            "epoch [28/50] batch [50/50] time 0.355 (0.381) data 0.000 (0.023) loss 2.4316 (2.8012) acc 50.0000 (35.5000) lr 8.7467e-04 eta 0:06:59\n",
            "epoch [29/50] batch [5/50] time 0.364 (0.523) data 0.001 (0.123) loss 2.9785 (2.9961) acc 31.2500 (29.3750) lr 8.7467e-04 eta 0:09:32\n",
            "epoch [29/50] batch [10/50] time 0.360 (0.442) data 0.006 (0.064) loss 3.1855 (3.0170) acc 34.3750 (30.9375) lr 8.7467e-04 eta 0:08:02\n",
            "epoch [29/50] batch [15/50] time 0.358 (0.415) data 0.001 (0.043) loss 2.5234 (3.0066) acc 40.6250 (30.6250) lr 8.7467e-04 eta 0:07:30\n",
            "epoch [29/50] batch [20/50] time 0.369 (0.403) data 0.012 (0.035) loss 2.5312 (2.9486) acc 37.5000 (31.4062) lr 8.7467e-04 eta 0:07:15\n",
            "epoch [29/50] batch [25/50] time 0.363 (0.394) data 0.007 (0.029) loss 2.7578 (2.8766) acc 37.5000 (33.5000) lr 8.7467e-04 eta 0:07:04\n",
            "epoch [29/50] batch [30/50] time 0.360 (0.389) data 0.001 (0.025) loss 2.7930 (2.8448) acc 34.3750 (34.0625) lr 8.7467e-04 eta 0:06:56\n",
            "epoch [29/50] batch [35/50] time 0.355 (0.384) data 0.001 (0.021) loss 2.8340 (2.8326) acc 37.5000 (34.5536) lr 8.7467e-04 eta 0:06:49\n",
            "epoch [29/50] batch [40/50] time 0.355 (0.381) data 0.000 (0.019) loss 2.4316 (2.8173) acc 53.1250 (34.6875) lr 8.7467e-04 eta 0:06:43\n",
            "epoch [29/50] batch [45/50] time 0.355 (0.378) data 0.000 (0.017) loss 2.6797 (2.7961) acc 37.5000 (34.7917) lr 8.7467e-04 eta 0:06:38\n",
            "epoch [29/50] batch [50/50] time 0.356 (0.376) data 0.000 (0.015) loss 2.4121 (2.8002) acc 43.7500 (35.0000) lr 8.1262e-04 eta 0:06:34\n",
            "epoch [30/50] batch [5/50] time 0.364 (0.583) data 0.008 (0.181) loss 2.6602 (2.6504) acc 43.7500 (41.8750) lr 8.1262e-04 eta 0:10:09\n",
            "epoch [30/50] batch [10/50] time 0.375 (0.474) data 0.010 (0.095) loss 3.2363 (2.7846) acc 34.3750 (39.0625) lr 8.1262e-04 eta 0:08:13\n",
            "epoch [30/50] batch [15/50] time 0.366 (0.437) data 0.008 (0.064) loss 2.5879 (2.7108) acc 34.3750 (39.5833) lr 8.1262e-04 eta 0:07:32\n",
            "epoch [30/50] batch [20/50] time 0.356 (0.419) data 0.001 (0.050) loss 3.7031 (2.7771) acc 31.2500 (38.7500) lr 8.1262e-04 eta 0:07:11\n",
            "epoch [30/50] batch [25/50] time 0.364 (0.407) data 0.007 (0.041) loss 3.2070 (2.8360) acc 21.8750 (36.8750) lr 8.1262e-04 eta 0:06:57\n",
            "epoch [30/50] batch [30/50] time 0.357 (0.399) data 0.001 (0.034) loss 3.5078 (2.8184) acc 28.1250 (37.0833) lr 8.1262e-04 eta 0:06:46\n",
            "epoch [30/50] batch [35/50] time 0.356 (0.393) data 0.001 (0.029) loss 2.6699 (2.8122) acc 34.3750 (36.4286) lr 8.1262e-04 eta 0:06:38\n",
            "epoch [30/50] batch [40/50] time 0.355 (0.388) data 0.000 (0.026) loss 2.9844 (2.7963) acc 37.5000 (36.0938) lr 8.1262e-04 eta 0:06:32\n",
            "epoch [30/50] batch [45/50] time 0.356 (0.385) data 0.000 (0.023) loss 2.1914 (2.7902) acc 46.8750 (35.8333) lr 8.1262e-04 eta 0:06:26\n",
            "epoch [30/50] batch [50/50] time 0.357 (0.382) data 0.000 (0.021) loss 2.3887 (2.7996) acc 46.8750 (35.9375) lr 7.5131e-04 eta 0:06:21\n",
            "epoch [31/50] batch [5/50] time 0.365 (0.535) data 0.011 (0.144) loss 2.1113 (2.6641) acc 56.2500 (37.5000) lr 7.5131e-04 eta 0:08:51\n",
            "epoch [31/50] batch [10/50] time 0.357 (0.448) data 0.001 (0.075) loss 3.0762 (2.6863) acc 31.2500 (36.8750) lr 7.5131e-04 eta 0:07:23\n",
            "epoch [31/50] batch [15/50] time 0.363 (0.421) data 0.008 (0.053) loss 2.1562 (2.6605) acc 53.1250 (36.8750) lr 7.5131e-04 eta 0:06:54\n",
            "epoch [31/50] batch [20/50] time 0.357 (0.405) data 0.001 (0.040) loss 3.1348 (2.7084) acc 34.3750 (37.1875) lr 7.5131e-04 eta 0:06:37\n",
            "epoch [31/50] batch [25/50] time 0.367 (0.397) data 0.008 (0.033) loss 2.7344 (2.7034) acc 43.7500 (37.6250) lr 7.5131e-04 eta 0:06:26\n",
            "epoch [31/50] batch [30/50] time 0.354 (0.390) data 0.000 (0.028) loss 3.0195 (2.7318) acc 28.1250 (37.0833) lr 7.5131e-04 eta 0:06:18\n",
            "epoch [31/50] batch [35/50] time 0.357 (0.385) data 0.001 (0.024) loss 3.0859 (2.7713) acc 28.1250 (36.4286) lr 7.5131e-04 eta 0:06:11\n",
            "epoch [31/50] batch [40/50] time 0.357 (0.382) data 0.000 (0.021) loss 2.5625 (2.7737) acc 43.7500 (35.9375) lr 7.5131e-04 eta 0:06:06\n",
            "epoch [31/50] batch [45/50] time 0.356 (0.379) data 0.001 (0.019) loss 3.0801 (2.7892) acc 28.1250 (35.4861) lr 7.5131e-04 eta 0:06:01\n",
            "epoch [31/50] batch [50/50] time 0.357 (0.376) data 0.001 (0.017) loss 2.4609 (2.7862) acc 43.7500 (35.6250) lr 6.9098e-04 eta 0:05:57\n",
            "epoch [32/50] batch [5/50] time 0.366 (0.611) data 0.001 (0.198) loss 2.6992 (2.6414) acc 28.1250 (36.2500) lr 6.9098e-04 eta 0:09:37\n",
            "epoch [32/50] batch [10/50] time 0.364 (0.487) data 0.008 (0.101) loss 2.6387 (2.7514) acc 34.3750 (32.8125) lr 6.9098e-04 eta 0:07:37\n",
            "epoch [32/50] batch [15/50] time 0.362 (0.447) data 0.001 (0.070) loss 3.1973 (2.6905) acc 25.0000 (35.6250) lr 6.9098e-04 eta 0:06:57\n",
            "epoch [32/50] batch [20/50] time 0.369 (0.426) data 0.008 (0.054) loss 2.6504 (2.7544) acc 46.8750 (35.3125) lr 6.9098e-04 eta 0:06:36\n",
            "epoch [32/50] batch [25/50] time 0.362 (0.414) data 0.001 (0.045) loss 3.2676 (2.7634) acc 31.2500 (35.5000) lr 6.9098e-04 eta 0:06:22\n",
            "epoch [32/50] batch [30/50] time 0.355 (0.405) data 0.001 (0.038) loss 2.6328 (2.7290) acc 31.2500 (35.5208) lr 6.9098e-04 eta 0:06:12\n",
            "epoch [32/50] batch [35/50] time 0.355 (0.398) data 0.001 (0.032) loss 2.4297 (2.7472) acc 46.8750 (35.1786) lr 6.9098e-04 eta 0:06:03\n",
            "epoch [32/50] batch [40/50] time 0.358 (0.392) data 0.000 (0.028) loss 2.8848 (2.7791) acc 25.0000 (34.6875) lr 6.9098e-04 eta 0:05:57\n",
            "epoch [32/50] batch [45/50] time 0.356 (0.388) data 0.000 (0.025) loss 2.7441 (2.7725) acc 28.1250 (34.6528) lr 6.9098e-04 eta 0:05:51\n",
            "epoch [32/50] batch [50/50] time 0.355 (0.385) data 0.000 (0.023) loss 2.2520 (2.7627) acc 43.7500 (35.1250) lr 6.3188e-04 eta 0:05:46\n",
            "epoch [33/50] batch [5/50] time 0.366 (0.614) data 0.001 (0.148) loss 2.4453 (2.5633) acc 34.3750 (40.6250) lr 6.3188e-04 eta 0:09:09\n",
            "epoch [33/50] batch [10/50] time 0.368 (0.487) data 0.008 (0.075) loss 2.7539 (2.7008) acc 31.2500 (37.1875) lr 6.3188e-04 eta 0:07:13\n",
            "epoch [33/50] batch [15/50] time 0.368 (0.447) data 0.009 (0.053) loss 2.8887 (2.7348) acc 31.2500 (36.4583) lr 6.3188e-04 eta 0:06:35\n",
            "epoch [33/50] batch [20/50] time 0.366 (0.426) data 0.009 (0.042) loss 2.6250 (2.6822) acc 43.7500 (37.6562) lr 6.3188e-04 eta 0:06:15\n",
            "epoch [33/50] batch [25/50] time 0.365 (0.414) data 0.008 (0.035) loss 3.6914 (2.7522) acc 18.7500 (36.8750) lr 6.3188e-04 eta 0:06:02\n",
            "epoch [33/50] batch [30/50] time 0.364 (0.405) data 0.008 (0.030) loss 3.3242 (2.7790) acc 21.8750 (36.0417) lr 6.3188e-04 eta 0:05:52\n",
            "epoch [33/50] batch [35/50] time 0.356 (0.398) data 0.001 (0.026) loss 2.6543 (2.7434) acc 31.2500 (36.7857) lr 6.3188e-04 eta 0:05:44\n",
            "epoch [33/50] batch [40/50] time 0.354 (0.393) data 0.000 (0.023) loss 3.3164 (2.7711) acc 34.3750 (36.7969) lr 6.3188e-04 eta 0:05:37\n",
            "epoch [33/50] batch [45/50] time 0.356 (0.389) data 0.000 (0.020) loss 2.7773 (2.7625) acc 28.1250 (36.5278) lr 6.3188e-04 eta 0:05:32\n",
            "epoch [33/50] batch [50/50] time 0.355 (0.385) data 0.000 (0.018) loss 3.1992 (2.7907) acc 28.1250 (36.1250) lr 5.7422e-04 eta 0:05:27\n",
            "epoch [34/50] batch [5/50] time 0.375 (0.655) data 0.006 (0.162) loss 2.9102 (2.6816) acc 28.1250 (39.3750) lr 5.7422e-04 eta 0:09:13\n",
            "epoch [34/50] batch [10/50] time 0.356 (0.508) data 0.001 (0.082) loss 2.3809 (2.6850) acc 46.8750 (39.0625) lr 5.7422e-04 eta 0:07:06\n",
            "epoch [34/50] batch [15/50] time 0.360 (0.459) data 0.001 (0.056) loss 2.5117 (2.6984) acc 37.5000 (38.3333) lr 5.7422e-04 eta 0:06:23\n",
            "epoch [34/50] batch [20/50] time 0.368 (0.436) data 0.012 (0.044) loss 3.1484 (2.7577) acc 25.0000 (36.4062) lr 5.7422e-04 eta 0:06:01\n",
            "epoch [34/50] batch [25/50] time 0.355 (0.420) data 0.001 (0.035) loss 3.1094 (2.7949) acc 25.0000 (35.6250) lr 5.7422e-04 eta 0:05:46\n",
            "epoch [34/50] batch [30/50] time 0.357 (0.409) data 0.001 (0.030) loss 2.2637 (2.7497) acc 37.5000 (36.5625) lr 5.7422e-04 eta 0:05:35\n",
            "epoch [34/50] batch [35/50] time 0.354 (0.402) data 0.001 (0.025) loss 3.1133 (2.7896) acc 28.1250 (35.8929) lr 5.7422e-04 eta 0:05:27\n",
            "epoch [34/50] batch [40/50] time 0.357 (0.396) data 0.000 (0.022) loss 2.7148 (2.7717) acc 40.6250 (36.4844) lr 5.7422e-04 eta 0:05:20\n",
            "epoch [34/50] batch [45/50] time 0.356 (0.391) data 0.000 (0.020) loss 2.8613 (2.7599) acc 37.5000 (36.1806) lr 5.7422e-04 eta 0:05:15\n",
            "epoch [34/50] batch [50/50] time 0.356 (0.388) data 0.000 (0.018) loss 1.5879 (2.7440) acc 65.6250 (36.8125) lr 5.1825e-04 eta 0:05:10\n",
            "epoch [35/50] batch [5/50] time 0.369 (0.782) data 0.007 (0.304) loss 2.2207 (2.4438) acc 50.0000 (41.8750) lr 5.1825e-04 eta 0:10:21\n",
            "epoch [35/50] batch [10/50] time 0.364 (0.574) data 0.009 (0.157) loss 2.7090 (2.5578) acc 40.6250 (40.0000) lr 5.1825e-04 eta 0:07:33\n",
            "epoch [35/50] batch [15/50] time 0.364 (0.504) data 0.009 (0.107) loss 2.5410 (2.6286) acc 40.6250 (38.9583) lr 5.1825e-04 eta 0:06:36\n",
            "epoch [35/50] batch [20/50] time 0.367 (0.469) data 0.010 (0.082) loss 2.6641 (2.6574) acc 40.6250 (39.3750) lr 5.1825e-04 eta 0:06:06\n",
            "epoch [35/50] batch [25/50] time 0.356 (0.448) data 0.000 (0.066) loss 2.2148 (2.7010) acc 46.8750 (38.3750) lr 5.1825e-04 eta 0:05:47\n",
            "epoch [35/50] batch [30/50] time 0.358 (0.433) data 0.002 (0.056) loss 2.7148 (2.7722) acc 34.3750 (36.1458) lr 5.1825e-04 eta 0:05:33\n",
            "epoch [35/50] batch [35/50] time 0.357 (0.422) data 0.001 (0.048) loss 3.0684 (2.7875) acc 18.7500 (35.7143) lr 5.1825e-04 eta 0:05:22\n",
            "epoch [35/50] batch [40/50] time 0.354 (0.413) data 0.000 (0.042) loss 2.3066 (2.7415) acc 40.6250 (36.0938) lr 5.1825e-04 eta 0:05:14\n",
            "epoch [35/50] batch [45/50] time 0.355 (0.407) data 0.000 (0.037) loss 2.7949 (2.7392) acc 43.7500 (36.4583) lr 5.1825e-04 eta 0:05:07\n",
            "epoch [35/50] batch [50/50] time 0.357 (0.402) data 0.000 (0.033) loss 2.4902 (2.7395) acc 28.1250 (36.5625) lr 4.6417e-04 eta 0:05:01\n",
            "epoch [36/50] batch [5/50] time 0.362 (0.829) data 0.008 (0.370) loss 2.4375 (2.3605) acc 37.5000 (45.0000) lr 4.6417e-04 eta 0:10:17\n",
            "epoch [36/50] batch [10/50] time 0.366 (0.597) data 0.001 (0.188) loss 2.6836 (2.5492) acc 28.1250 (37.8125) lr 4.6417e-04 eta 0:07:21\n",
            "epoch [36/50] batch [15/50] time 0.354 (0.519) data 0.001 (0.127) loss 2.8535 (2.6147) acc 37.5000 (37.9167) lr 4.6417e-04 eta 0:06:21\n",
            "epoch [36/50] batch [20/50] time 0.357 (0.478) data 0.001 (0.095) loss 2.5605 (2.6388) acc 46.8750 (37.8125) lr 4.6417e-04 eta 0:05:48\n",
            "epoch [36/50] batch [25/50] time 0.356 (0.453) data 0.001 (0.076) loss 3.2422 (2.7488) acc 37.5000 (36.6250) lr 4.6417e-04 eta 0:05:28\n",
            "epoch [36/50] batch [30/50] time 0.358 (0.437) data 0.001 (0.064) loss 2.7246 (2.7087) acc 34.3750 (36.7708) lr 4.6417e-04 eta 0:05:14\n",
            "epoch [36/50] batch [35/50] time 0.354 (0.425) data 0.001 (0.055) loss 2.7148 (2.7033) acc 40.6250 (36.7857) lr 4.6417e-04 eta 0:05:04\n",
            "epoch [36/50] batch [40/50] time 0.356 (0.417) data 0.001 (0.048) loss 2.5879 (2.7251) acc 50.0000 (37.1094) lr 4.6417e-04 eta 0:04:55\n",
            "epoch [36/50] batch [45/50] time 0.357 (0.410) data 0.001 (0.043) loss 3.3125 (2.7458) acc 21.8750 (36.3889) lr 4.6417e-04 eta 0:04:48\n",
            "epoch [36/50] batch [50/50] time 0.358 (0.405) data 0.001 (0.039) loss 2.3672 (2.7391) acc 40.6250 (36.4375) lr 4.1221e-04 eta 0:04:43\n",
            "epoch [37/50] batch [5/50] time 0.362 (0.832) data 0.001 (0.372) loss 2.8613 (2.6102) acc 31.2500 (44.3750) lr 4.1221e-04 eta 0:09:37\n",
            "epoch [37/50] batch [10/50] time 0.359 (0.598) data 0.001 (0.190) loss 2.3438 (2.6504) acc 43.7500 (42.5000) lr 4.1221e-04 eta 0:06:52\n",
            "epoch [37/50] batch [15/50] time 0.355 (0.518) data 0.001 (0.128) loss 2.7266 (2.6829) acc 21.8750 (39.3750) lr 4.1221e-04 eta 0:05:55\n",
            "epoch [37/50] batch [20/50] time 0.356 (0.478) data 0.000 (0.096) loss 2.9746 (2.7573) acc 18.7500 (37.0312) lr 4.1221e-04 eta 0:05:24\n",
            "epoch [37/50] batch [25/50] time 0.356 (0.453) data 0.001 (0.077) loss 2.7402 (2.7236) acc 28.1250 (36.5000) lr 4.1221e-04 eta 0:05:06\n",
            "epoch [37/50] batch [30/50] time 0.357 (0.437) data 0.000 (0.064) loss 2.7344 (2.7530) acc 28.1250 (36.0417) lr 4.1221e-04 eta 0:04:52\n",
            "epoch [37/50] batch [35/50] time 0.354 (0.426) data 0.001 (0.055) loss 2.7676 (2.7254) acc 34.3750 (36.1607) lr 4.1221e-04 eta 0:04:42\n",
            "epoch [37/50] batch [40/50] time 0.358 (0.417) data 0.000 (0.048) loss 2.4062 (2.7231) acc 37.5000 (36.0938) lr 4.1221e-04 eta 0:04:35\n",
            "epoch [37/50] batch [45/50] time 0.356 (0.410) data 0.001 (0.043) loss 2.8281 (2.7294) acc 40.6250 (36.4583) lr 4.1221e-04 eta 0:04:28\n",
            "epoch [37/50] batch [50/50] time 0.357 (0.405) data 0.001 (0.039) loss 1.9375 (2.7117) acc 53.1250 (36.7500) lr 3.6258e-04 eta 0:04:23\n",
            "epoch [38/50] batch [5/50] time 0.350 (0.598) data 0.001 (0.215) loss 3.5254 (2.7461) acc 21.8750 (31.8750) lr 3.6258e-04 eta 0:06:25\n",
            "epoch [38/50] batch [10/50] time 0.355 (0.477) data 0.001 (0.108) loss 2.4961 (2.8455) acc 31.2500 (32.1875) lr 3.6258e-04 eta 0:05:05\n",
            "epoch [38/50] batch [15/50] time 0.357 (0.437) data 0.001 (0.072) loss 3.0000 (2.8414) acc 34.3750 (32.5000) lr 3.6258e-04 eta 0:04:37\n",
            "epoch [38/50] batch [20/50] time 0.356 (0.417) data 0.000 (0.054) loss 2.3047 (2.7056) acc 59.3750 (35.7812) lr 3.6258e-04 eta 0:04:22\n",
            "epoch [38/50] batch [25/50] time 0.372 (0.405) data 0.011 (0.044) loss 2.6426 (2.7020) acc 34.3750 (35.6250) lr 3.6258e-04 eta 0:04:13\n",
            "epoch [38/50] batch [30/50] time 0.364 (0.399) data 0.007 (0.037) loss 1.9551 (2.6833) acc 56.2500 (36.4583) lr 3.6258e-04 eta 0:04:07\n",
            "epoch [38/50] batch [35/50] time 0.357 (0.394) data 0.001 (0.033) loss 2.6152 (2.7096) acc 34.3750 (35.8036) lr 3.6258e-04 eta 0:04:02\n",
            "epoch [38/50] batch [40/50] time 0.355 (0.389) data 0.001 (0.029) loss 2.1797 (2.7058) acc 40.6250 (36.0156) lr 3.6258e-04 eta 0:03:57\n",
            "epoch [38/50] batch [45/50] time 0.356 (0.385) data 0.001 (0.026) loss 2.5000 (2.7015) acc 43.7500 (36.2500) lr 3.6258e-04 eta 0:03:53\n",
            "epoch [38/50] batch [50/50] time 0.357 (0.382) data 0.001 (0.023) loss 2.4277 (2.7038) acc 46.8750 (36.4375) lr 3.1545e-04 eta 0:03:49\n",
            "epoch [39/50] batch [5/50] time 0.353 (0.581) data 0.001 (0.186) loss 2.8535 (2.5514) acc 31.2500 (42.5000) lr 3.1545e-04 eta 0:05:45\n",
            "epoch [39/50] batch [10/50] time 0.356 (0.469) data 0.001 (0.093) loss 2.4688 (2.6325) acc 37.5000 (38.1250) lr 3.1545e-04 eta 0:04:36\n",
            "epoch [39/50] batch [15/50] time 0.356 (0.431) data 0.001 (0.062) loss 2.7715 (2.6429) acc 37.5000 (38.3333) lr 3.1545e-04 eta 0:04:12\n",
            "epoch [39/50] batch [20/50] time 0.354 (0.412) data 0.001 (0.047) loss 2.3672 (2.6464) acc 40.6250 (37.1875) lr 3.1545e-04 eta 0:03:59\n",
            "epoch [39/50] batch [25/50] time 0.355 (0.401) data 0.001 (0.038) loss 2.9551 (2.6393) acc 31.2500 (37.7500) lr 3.1545e-04 eta 0:03:50\n",
            "epoch [39/50] batch [30/50] time 0.372 (0.395) data 0.009 (0.032) loss 3.4883 (2.7185) acc 31.2500 (37.7083) lr 3.1545e-04 eta 0:03:44\n",
            "epoch [39/50] batch [35/50] time 0.357 (0.390) data 0.001 (0.028) loss 2.8555 (2.7149) acc 28.1250 (37.5893) lr 3.1545e-04 eta 0:03:40\n",
            "epoch [39/50] batch [40/50] time 0.355 (0.386) data 0.001 (0.025) loss 2.7871 (2.7143) acc 25.0000 (37.3438) lr 3.1545e-04 eta 0:03:35\n",
            "epoch [39/50] batch [45/50] time 0.356 (0.382) data 0.001 (0.022) loss 2.6523 (2.7086) acc 34.3750 (37.5000) lr 3.1545e-04 eta 0:03:32\n",
            "epoch [39/50] batch [50/50] time 0.356 (0.380) data 0.001 (0.020) loss 2.5176 (2.7147) acc 34.3750 (37.2500) lr 2.7103e-04 eta 0:03:28\n",
            "epoch [40/50] batch [5/50] time 0.352 (0.603) data 0.001 (0.231) loss 2.9180 (2.8855) acc 25.0000 (34.3750) lr 2.7103e-04 eta 0:05:28\n",
            "epoch [40/50] batch [10/50] time 0.372 (0.482) data 0.007 (0.118) loss 3.2656 (2.8875) acc 34.3750 (35.6250) lr 2.7103e-04 eta 0:04:20\n",
            "epoch [40/50] batch [15/50] time 0.357 (0.441) data 0.001 (0.079) loss 3.1777 (2.7668) acc 31.2500 (36.6667) lr 2.7103e-04 eta 0:03:55\n",
            "epoch [40/50] batch [20/50] time 0.355 (0.419) data 0.001 (0.060) loss 2.7461 (2.7226) acc 34.3750 (37.9688) lr 2.7103e-04 eta 0:03:42\n",
            "epoch [40/50] batch [25/50] time 0.358 (0.407) data 0.001 (0.048) loss 2.4062 (2.6793) acc 53.1250 (39.2500) lr 2.7103e-04 eta 0:03:33\n",
            "epoch [40/50] batch [30/50] time 0.357 (0.399) data 0.001 (0.040) loss 2.6777 (2.6764) acc 31.2500 (38.9583) lr 2.7103e-04 eta 0:03:27\n",
            "epoch [40/50] batch [35/50] time 0.356 (0.393) data 0.002 (0.035) loss 2.2227 (2.6589) acc 46.8750 (38.7500) lr 2.7103e-04 eta 0:03:22\n",
            "epoch [40/50] batch [40/50] time 0.355 (0.389) data 0.001 (0.031) loss 3.2969 (2.6683) acc 18.7500 (38.2812) lr 2.7103e-04 eta 0:03:18\n",
            "epoch [40/50] batch [45/50] time 0.356 (0.385) data 0.000 (0.027) loss 2.8281 (2.6949) acc 31.2500 (37.9167) lr 2.7103e-04 eta 0:03:14\n",
            "epoch [40/50] batch [50/50] time 0.356 (0.382) data 0.000 (0.025) loss 2.8223 (2.6857) acc 40.6250 (38.3125) lr 2.2949e-04 eta 0:03:11\n",
            "epoch [41/50] batch [5/50] time 0.351 (0.618) data 0.000 (0.243) loss 2.5195 (2.7773) acc 37.5000 (37.5000) lr 2.2949e-04 eta 0:05:05\n",
            "epoch [41/50] batch [10/50] time 0.356 (0.487) data 0.001 (0.122) loss 3.4277 (2.6397) acc 31.2500 (41.5625) lr 2.2949e-04 eta 0:03:58\n",
            "epoch [41/50] batch [15/50] time 0.355 (0.443) data 0.001 (0.082) loss 2.8281 (2.7285) acc 34.3750 (39.5833) lr 2.2949e-04 eta 0:03:34\n",
            "epoch [41/50] batch [20/50] time 0.360 (0.422) data 0.001 (0.062) loss 3.3086 (2.7300) acc 25.0000 (38.7500) lr 2.2949e-04 eta 0:03:22\n",
            "epoch [41/50] batch [25/50] time 0.360 (0.410) data 0.000 (0.050) loss 2.5938 (2.7245) acc 37.5000 (38.5000) lr 2.2949e-04 eta 0:03:14\n",
            "epoch [41/50] batch [30/50] time 0.363 (0.403) data 0.008 (0.043) loss 2.6992 (2.7276) acc 34.3750 (38.1250) lr 2.2949e-04 eta 0:03:09\n",
            "epoch [41/50] batch [35/50] time 0.355 (0.397) data 0.001 (0.038) loss 2.5566 (2.7543) acc 37.5000 (37.9464) lr 2.2949e-04 eta 0:03:04\n",
            "epoch [41/50] batch [40/50] time 0.357 (0.392) data 0.001 (0.033) loss 2.8398 (2.7506) acc 37.5000 (37.6562) lr 2.2949e-04 eta 0:03:00\n",
            "epoch [41/50] batch [45/50] time 0.360 (0.388) data 0.001 (0.029) loss 3.3242 (2.7503) acc 28.1250 (37.0833) lr 2.2949e-04 eta 0:02:56\n",
            "epoch [41/50] batch [50/50] time 0.356 (0.384) data 0.000 (0.026) loss 2.6523 (2.7573) acc 31.2500 (36.6250) lr 1.9098e-04 eta 0:02:53\n",
            "epoch [42/50] batch [5/50] time 0.354 (0.551) data 0.001 (0.154) loss 2.1172 (2.4578) acc 59.3750 (46.2500) lr 1.9098e-04 eta 0:04:04\n",
            "epoch [42/50] batch [10/50] time 0.357 (0.453) data 0.001 (0.077) loss 2.6621 (2.4746) acc 31.2500 (43.4375) lr 1.9098e-04 eta 0:03:19\n",
            "epoch [42/50] batch [15/50] time 0.355 (0.421) data 0.001 (0.052) loss 2.5879 (2.5385) acc 40.6250 (41.6667) lr 1.9098e-04 eta 0:03:02\n",
            "epoch [42/50] batch [20/50] time 0.368 (0.406) data 0.011 (0.040) loss 2.4531 (2.6737) acc 31.2500 (39.0625) lr 1.9098e-04 eta 0:02:54\n",
            "epoch [42/50] batch [25/50] time 0.362 (0.397) data 0.007 (0.033) loss 3.0781 (2.6637) acc 28.1250 (38.8750) lr 1.9098e-04 eta 0:02:48\n",
            "epoch [42/50] batch [30/50] time 0.364 (0.392) data 0.007 (0.029) loss 2.3496 (2.6379) acc 34.3750 (39.2708) lr 1.9098e-04 eta 0:02:44\n",
            "epoch [42/50] batch [35/50] time 0.361 (0.387) data 0.001 (0.025) loss 2.2461 (2.6178) acc 46.8750 (39.2857) lr 1.9098e-04 eta 0:02:40\n",
            "epoch [42/50] batch [40/50] time 0.356 (0.383) data 0.000 (0.022) loss 3.2988 (2.6435) acc 28.1250 (38.9844) lr 1.9098e-04 eta 0:02:37\n",
            "epoch [42/50] batch [45/50] time 0.356 (0.380) data 0.000 (0.020) loss 2.8574 (2.6417) acc 28.1250 (38.6111) lr 1.9098e-04 eta 0:02:34\n",
            "epoch [42/50] batch [50/50] time 0.357 (0.378) data 0.000 (0.018) loss 2.6348 (2.6342) acc 34.3750 (38.5625) lr 1.5567e-04 eta 0:02:31\n",
            "epoch [43/50] batch [5/50] time 0.356 (0.567) data 0.001 (0.169) loss 2.7500 (2.6174) acc 40.6250 (39.3750) lr 1.5567e-04 eta 0:03:43\n",
            "epoch [43/50] batch [10/50] time 0.357 (0.461) data 0.001 (0.085) loss 2.7305 (2.7257) acc 37.5000 (38.4375) lr 1.5567e-04 eta 0:02:59\n",
            "epoch [43/50] batch [15/50] time 0.364 (0.429) data 0.010 (0.059) loss 3.0059 (2.7150) acc 25.0000 (38.1250) lr 1.5567e-04 eta 0:02:45\n",
            "epoch [43/50] batch [20/50] time 0.368 (0.413) data 0.008 (0.045) loss 3.1191 (2.6474) acc 31.2500 (39.0625) lr 1.5567e-04 eta 0:02:36\n",
            "epoch [43/50] batch [25/50] time 0.386 (0.403) data 0.008 (0.037) loss 2.2090 (2.6616) acc 50.0000 (38.1250) lr 1.5567e-04 eta 0:02:31\n",
            "epoch [43/50] batch [30/50] time 0.364 (0.397) data 0.003 (0.032) loss 2.5312 (2.6542) acc 34.3750 (38.6458) lr 1.5567e-04 eta 0:02:26\n",
            "epoch [43/50] batch [35/50] time 0.357 (0.392) data 0.001 (0.028) loss 2.8730 (2.6635) acc 37.5000 (38.5714) lr 1.5567e-04 eta 0:02:23\n",
            "epoch [43/50] batch [40/50] time 0.356 (0.388) data 0.000 (0.025) loss 2.5957 (2.6767) acc 43.7500 (38.7500) lr 1.5567e-04 eta 0:02:19\n",
            "epoch [43/50] batch [45/50] time 0.355 (0.384) data 0.000 (0.022) loss 2.3027 (2.6918) acc 50.0000 (38.0556) lr 1.5567e-04 eta 0:02:16\n",
            "epoch [43/50] batch [50/50] time 0.356 (0.381) data 0.000 (0.020) loss 1.9404 (2.6768) acc 43.7500 (38.2500) lr 1.2369e-04 eta 0:02:13\n",
            "epoch [44/50] batch [5/50] time 0.350 (0.616) data 0.001 (0.240) loss 2.6816 (2.8004) acc 34.3750 (36.2500) lr 1.2369e-04 eta 0:03:32\n",
            "epoch [44/50] batch [10/50] time 0.358 (0.487) data 0.001 (0.120) loss 2.5918 (2.7459) acc 37.5000 (37.8125) lr 1.2369e-04 eta 0:02:45\n",
            "epoch [44/50] batch [15/50] time 0.359 (0.445) data 0.001 (0.082) loss 2.7910 (2.6517) acc 43.7500 (40.6250) lr 1.2369e-04 eta 0:02:29\n",
            "epoch [44/50] batch [20/50] time 0.355 (0.423) data 0.001 (0.062) loss 2.9609 (2.7164) acc 28.1250 (38.1250) lr 1.2369e-04 eta 0:02:19\n",
            "epoch [44/50] batch [25/50] time 0.360 (0.411) data 0.004 (0.050) loss 2.2207 (2.6595) acc 53.1250 (39.2500) lr 1.2369e-04 eta 0:02:13\n",
            "epoch [44/50] batch [30/50] time 0.357 (0.403) data 0.001 (0.042) loss 2.7969 (2.6368) acc 31.2500 (39.0625) lr 1.2369e-04 eta 0:02:08\n",
            "epoch [44/50] batch [35/50] time 0.353 (0.396) data 0.001 (0.036) loss 2.4668 (2.6608) acc 40.6250 (38.2143) lr 1.2369e-04 eta 0:02:04\n",
            "epoch [44/50] batch [40/50] time 0.355 (0.391) data 0.000 (0.032) loss 3.4434 (2.6937) acc 28.1250 (37.7344) lr 1.2369e-04 eta 0:02:01\n",
            "epoch [44/50] batch [45/50] time 0.353 (0.387) data 0.000 (0.028) loss 2.6035 (2.6946) acc 40.6250 (38.0556) lr 1.2369e-04 eta 0:01:58\n",
            "epoch [44/50] batch [50/50] time 0.357 (0.384) data 0.000 (0.026) loss 2.6055 (2.7115) acc 40.6250 (37.6250) lr 9.5173e-05 eta 0:01:55\n",
            "epoch [45/50] batch [5/50] time 0.351 (0.537) data 0.001 (0.156) loss 2.9180 (2.6152) acc 46.8750 (40.6250) lr 9.5173e-05 eta 0:02:38\n",
            "epoch [45/50] batch [10/50] time 0.359 (0.446) data 0.001 (0.078) loss 2.9121 (2.7143) acc 34.3750 (38.7500) lr 9.5173e-05 eta 0:02:09\n",
            "epoch [45/50] batch [15/50] time 0.357 (0.419) data 0.000 (0.054) loss 2.0645 (2.7091) acc 56.2500 (38.3333) lr 9.5173e-05 eta 0:01:59\n",
            "epoch [45/50] batch [20/50] time 0.364 (0.405) data 0.005 (0.043) loss 2.7480 (2.7431) acc 37.5000 (37.9688) lr 9.5173e-05 eta 0:01:53\n",
            "epoch [45/50] batch [25/50] time 0.358 (0.398) data 0.006 (0.036) loss 2.6289 (2.7077) acc 46.8750 (38.5000) lr 9.5173e-05 eta 0:01:49\n",
            "epoch [45/50] batch [30/50] time 0.360 (0.392) data 0.006 (0.030) loss 2.7441 (2.6999) acc 34.3750 (38.2292) lr 9.5173e-05 eta 0:01:45\n",
            "epoch [45/50] batch [35/50] time 0.352 (0.387) data 0.001 (0.026) loss 3.0918 (2.6928) acc 28.1250 (38.6607) lr 9.5173e-05 eta 0:01:42\n",
            "epoch [45/50] batch [40/50] time 0.355 (0.383) data 0.000 (0.023) loss 2.7578 (2.6575) acc 40.6250 (39.5312) lr 9.5173e-05 eta 0:01:39\n",
            "epoch [45/50] batch [45/50] time 0.356 (0.380) data 0.000 (0.021) loss 2.8242 (2.6719) acc 43.7500 (38.9583) lr 9.5173e-05 eta 0:01:36\n",
            "epoch [45/50] batch [50/50] time 0.355 (0.377) data 0.000 (0.019) loss 2.5117 (2.6685) acc 43.7500 (39.1875) lr 7.0224e-05 eta 0:01:34\n",
            "epoch [46/50] batch [5/50] time 0.351 (0.627) data 0.001 (0.255) loss 3.1855 (2.7777) acc 21.8750 (34.3750) lr 7.0224e-05 eta 0:02:33\n",
            "epoch [46/50] batch [10/50] time 0.357 (0.492) data 0.001 (0.128) loss 3.0879 (2.6682) acc 34.3750 (37.1875) lr 7.0224e-05 eta 0:01:58\n",
            "epoch [46/50] batch [15/50] time 0.355 (0.450) data 0.003 (0.087) loss 3.0430 (2.6872) acc 46.8750 (38.9583) lr 7.0224e-05 eta 0:01:45\n",
            "epoch [46/50] batch [20/50] time 0.361 (0.428) data 0.001 (0.066) loss 2.3281 (2.6425) acc 46.8750 (39.5312) lr 7.0224e-05 eta 0:01:38\n",
            "epoch [46/50] batch [25/50] time 0.372 (0.415) data 0.010 (0.054) loss 2.1309 (2.5927) acc 46.8750 (40.0000) lr 7.0224e-05 eta 0:01:33\n",
            "epoch [46/50] batch [30/50] time 0.361 (0.407) data 0.006 (0.046) loss 2.7031 (2.6238) acc 31.2500 (39.2708) lr 7.0224e-05 eta 0:01:29\n",
            "epoch [46/50] batch [35/50] time 0.355 (0.400) data 0.001 (0.039) loss 2.7637 (2.6114) acc 31.2500 (39.6429) lr 7.0224e-05 eta 0:01:25\n",
            "epoch [46/50] batch [40/50] time 0.357 (0.394) data 0.000 (0.034) loss 2.6836 (2.6073) acc 40.6250 (39.9219) lr 7.0224e-05 eta 0:01:22\n",
            "epoch [46/50] batch [45/50] time 0.357 (0.390) data 0.000 (0.031) loss 3.0391 (2.6491) acc 37.5000 (39.3750) lr 7.0224e-05 eta 0:01:19\n",
            "epoch [46/50] batch [50/50] time 0.355 (0.386) data 0.000 (0.028) loss 2.4668 (2.6407) acc 34.3750 (39.0625) lr 4.8943e-05 eta 0:01:17\n",
            "epoch [47/50] batch [5/50] time 0.350 (0.621) data 0.001 (0.224) loss 2.7109 (2.7316) acc 40.6250 (36.8750) lr 4.8943e-05 eta 0:02:01\n",
            "epoch [47/50] batch [10/50] time 0.362 (0.491) data 0.001 (0.114) loss 3.6152 (2.6090) acc 21.8750 (38.4375) lr 4.8943e-05 eta 0:01:33\n",
            "epoch [47/50] batch [15/50] time 0.355 (0.448) data 0.001 (0.078) loss 2.9707 (2.6915) acc 31.2500 (37.0833) lr 4.8943e-05 eta 0:01:22\n",
            "epoch [47/50] batch [20/50] time 0.367 (0.427) data 0.008 (0.060) loss 2.7480 (2.7062) acc 37.5000 (37.1875) lr 4.8943e-05 eta 0:01:16\n",
            "epoch [47/50] batch [25/50] time 0.363 (0.414) data 0.003 (0.049) loss 2.6250 (2.6673) acc 40.6250 (38.7500) lr 4.8943e-05 eta 0:01:12\n",
            "epoch [47/50] batch [30/50] time 0.356 (0.406) data 0.001 (0.042) loss 2.5957 (2.6435) acc 37.5000 (39.5833) lr 4.8943e-05 eta 0:01:09\n",
            "epoch [47/50] batch [35/50] time 0.355 (0.399) data 0.001 (0.036) loss 2.5254 (2.6487) acc 50.0000 (39.4643) lr 4.8943e-05 eta 0:01:05\n",
            "epoch [47/50] batch [40/50] time 0.359 (0.394) data 0.000 (0.032) loss 1.9668 (2.6387) acc 59.3750 (39.3750) lr 4.8943e-05 eta 0:01:03\n",
            "epoch [47/50] batch [45/50] time 0.356 (0.390) data 0.000 (0.028) loss 2.2754 (2.6192) acc 53.1250 (40.2083) lr 4.8943e-05 eta 0:01:00\n",
            "epoch [47/50] batch [50/50] time 0.356 (0.386) data 0.000 (0.025) loss 2.7891 (2.6393) acc 25.0000 (39.5000) lr 3.1417e-05 eta 0:00:57\n",
            "epoch [48/50] batch [5/50] time 0.361 (0.584) data 0.007 (0.188) loss 3.2285 (2.5977) acc 31.2500 (38.1250) lr 3.1417e-05 eta 0:01:24\n",
            "epoch [48/50] batch [10/50] time 0.362 (0.473) data 0.007 (0.096) loss 3.1074 (2.7176) acc 25.0000 (36.2500) lr 3.1417e-05 eta 0:01:06\n",
            "epoch [48/50] batch [15/50] time 0.370 (0.437) data 0.016 (0.066) loss 2.7969 (2.8228) acc 34.3750 (33.5417) lr 3.1417e-05 eta 0:00:58\n",
            "epoch [48/50] batch [20/50] time 0.368 (0.418) data 0.008 (0.050) loss 2.5312 (2.7985) acc 37.5000 (33.9062) lr 3.1417e-05 eta 0:00:54\n",
            "epoch [48/50] batch [25/50] time 0.360 (0.408) data 0.001 (0.041) loss 2.3652 (2.7417) acc 37.5000 (34.5000) lr 3.1417e-05 eta 0:00:51\n",
            "epoch [48/50] batch [30/50] time 0.360 (0.401) data 0.001 (0.035) loss 2.5391 (2.7145) acc 40.6250 (35.5208) lr 3.1417e-05 eta 0:00:48\n",
            "epoch [48/50] batch [35/50] time 0.356 (0.395) data 0.001 (0.031) loss 3.4707 (2.7169) acc 28.1250 (36.1607) lr 3.1417e-05 eta 0:00:45\n",
            "epoch [48/50] batch [40/50] time 0.356 (0.390) data 0.000 (0.027) loss 2.7207 (2.6945) acc 37.5000 (36.5625) lr 3.1417e-05 eta 0:00:42\n",
            "epoch [48/50] batch [45/50] time 0.356 (0.386) data 0.000 (0.024) loss 2.8477 (2.6940) acc 37.5000 (36.8750) lr 3.1417e-05 eta 0:00:40\n",
            "epoch [48/50] batch [50/50] time 0.355 (0.383) data 0.000 (0.021) loss 2.5605 (2.7046) acc 40.6250 (36.5625) lr 1.7713e-05 eta 0:00:38\n",
            "epoch [49/50] batch [5/50] time 0.357 (0.626) data 0.009 (0.249) loss 2.7207 (2.6598) acc 40.6250 (41.2500) lr 1.7713e-05 eta 0:00:59\n",
            "epoch [49/50] batch [10/50] time 0.358 (0.495) data 0.001 (0.128) loss 2.8555 (2.7141) acc 40.6250 (40.9375) lr 1.7713e-05 eta 0:00:44\n",
            "epoch [49/50] batch [15/50] time 0.368 (0.451) data 0.010 (0.087) loss 2.4004 (2.6977) acc 46.8750 (40.6250) lr 1.7713e-05 eta 0:00:38\n",
            "epoch [49/50] batch [20/50] time 0.364 (0.429) data 0.007 (0.067) loss 2.9941 (2.7217) acc 34.3750 (40.6250) lr 1.7713e-05 eta 0:00:34\n",
            "epoch [49/50] batch [25/50] time 0.354 (0.415) data 0.001 (0.054) loss 2.6289 (2.6890) acc 34.3750 (41.1250) lr 1.7713e-05 eta 0:00:31\n",
            "epoch [49/50] batch [30/50] time 0.355 (0.405) data 0.001 (0.045) loss 2.4336 (2.6891) acc 40.6250 (40.7292) lr 1.7713e-05 eta 0:00:28\n",
            "epoch [49/50] batch [35/50] time 0.356 (0.398) data 0.001 (0.039) loss 2.8477 (2.6981) acc 31.2500 (40.0000) lr 1.7713e-05 eta 0:00:25\n",
            "epoch [49/50] batch [40/50] time 0.355 (0.393) data 0.000 (0.034) loss 2.7500 (2.6938) acc 37.5000 (40.0781) lr 1.7713e-05 eta 0:00:23\n",
            "epoch [49/50] batch [45/50] time 0.356 (0.388) data 0.000 (0.030) loss 2.7402 (2.6617) acc 28.1250 (40.0694) lr 1.7713e-05 eta 0:00:21\n",
            "epoch [49/50] batch [50/50] time 0.356 (0.385) data 0.000 (0.027) loss 2.3516 (2.6580) acc 40.6250 (39.7500) lr 7.8853e-06 eta 0:00:19\n",
            "epoch [50/50] batch [5/50] time 0.374 (0.760) data 0.009 (0.286) loss 2.3301 (2.5566) acc 37.5000 (38.1250) lr 7.8853e-06 eta 0:00:34\n",
            "epoch [50/50] batch [10/50] time 0.358 (0.560) data 0.001 (0.145) loss 2.4160 (2.6658) acc 53.1250 (39.0625) lr 7.8853e-06 eta 0:00:22\n",
            "epoch [50/50] batch [15/50] time 0.371 (0.496) data 0.011 (0.100) loss 3.3066 (2.6745) acc 18.7500 (37.5000) lr 7.8853e-06 eta 0:00:17\n",
            "epoch [50/50] batch [20/50] time 0.372 (0.465) data 0.013 (0.078) loss 2.4629 (2.6708) acc 37.5000 (37.5000) lr 7.8853e-06 eta 0:00:13\n",
            "epoch [50/50] batch [25/50] time 0.356 (0.445) data 0.001 (0.063) loss 2.5684 (2.6136) acc 46.8750 (39.7500) lr 7.8853e-06 eta 0:00:11\n",
            "epoch [50/50] batch [30/50] time 0.356 (0.430) data 0.000 (0.053) loss 2.4883 (2.6588) acc 46.8750 (39.0625) lr 7.8853e-06 eta 0:00:08\n",
            "epoch [50/50] batch [35/50] time 0.355 (0.419) data 0.001 (0.045) loss 2.6797 (2.6065) acc 43.7500 (39.9107) lr 7.8853e-06 eta 0:00:06\n",
            "epoch [50/50] batch [40/50] time 0.356 (0.411) data 0.000 (0.040) loss 2.9297 (2.6127) acc 28.1250 (39.6094) lr 7.8853e-06 eta 0:00:04\n",
            "epoch [50/50] batch [45/50] time 0.354 (0.405) data 0.000 (0.035) loss 2.6055 (2.6164) acc 40.6250 (39.5833) lr 7.8853e-06 eta 0:00:02\n",
            "epoch [50/50] batch [50/50] time 0.357 (0.400) data 0.000 (0.032) loss 2.9414 (2.6346) acc 31.2500 (38.6250) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:40<00:00,  2.47it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 51\n",
            "* accuracy: 0.5%\n",
            "* error: 99.5%\n",
            "* macro_f1: 1.0%\n",
            "Elapsed: 0:16:54\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed2\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Creating a 8-shot dataset\n",
            "Saving preprocessed few-shot data to /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_8-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  1,600\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed2/tensorboard)\n",
            "epoch [1/50] batch [5/50] time 0.360 (1.362) data 0.001 (0.417) loss 4.5195 (4.5523) acc 21.8750 (15.0000) lr 1.0000e-05 eta 0:56:38\n",
            "epoch [1/50] batch [10/50] time 0.352 (0.859) data 0.001 (0.209) loss 4.1992 (4.5355) acc 12.5000 (14.3750) lr 1.0000e-05 eta 0:35:39\n",
            "epoch [1/50] batch [15/50] time 0.353 (0.693) data 0.001 (0.140) loss 4.8398 (4.5786) acc 6.2500 (12.0833) lr 1.0000e-05 eta 0:28:42\n",
            "epoch [1/50] batch [20/50] time 0.351 (0.609) data 0.001 (0.105) loss 4.3906 (4.5393) acc 15.6250 (12.5000) lr 1.0000e-05 eta 0:25:10\n",
            "epoch [1/50] batch [25/50] time 0.355 (0.558) data 0.001 (0.084) loss 4.2539 (4.5580) acc 12.5000 (11.6250) lr 1.0000e-05 eta 0:23:00\n",
            "epoch [1/50] batch [30/50] time 0.355 (0.524) data 0.000 (0.070) loss 4.0781 (4.5272) acc 25.0000 (12.2917) lr 1.0000e-05 eta 0:21:33\n",
            "epoch [1/50] batch [35/50] time 0.356 (0.500) data 0.001 (0.060) loss 3.9824 (4.4706) acc 25.0000 (13.5714) lr 1.0000e-05 eta 0:20:31\n",
            "epoch [1/50] batch [40/50] time 0.352 (0.481) data 0.000 (0.053) loss 4.1094 (4.3992) acc 15.6250 (14.6875) lr 1.0000e-05 eta 0:19:44\n",
            "epoch [1/50] batch [45/50] time 0.358 (0.467) data 0.000 (0.047) loss 3.8125 (4.3452) acc 21.8750 (15.3472) lr 1.0000e-05 eta 0:19:07\n",
            "epoch [1/50] batch [50/50] time 0.357 (0.456) data 0.001 (0.042) loss 4.1758 (4.3026) acc 15.6250 (15.7500) lr 2.0000e-03 eta 0:18:38\n",
            "epoch [2/50] batch [5/50] time 0.374 (0.885) data 0.007 (0.412) loss 3.7676 (3.5102) acc 25.0000 (31.8750) lr 2.0000e-03 eta 0:36:04\n",
            "epoch [2/50] batch [10/50] time 0.363 (0.625) data 0.008 (0.209) loss 3.8535 (3.5133) acc 18.7500 (29.3750) lr 2.0000e-03 eta 0:25:25\n",
            "epoch [2/50] batch [15/50] time 0.372 (0.539) data 0.009 (0.141) loss 3.7598 (3.5482) acc 21.8750 (27.7083) lr 2.0000e-03 eta 0:21:53\n",
            "epoch [2/50] batch [20/50] time 0.358 (0.495) data 0.001 (0.107) loss 3.8535 (3.5792) acc 6.2500 (26.4062) lr 2.0000e-03 eta 0:20:03\n",
            "epoch [2/50] batch [25/50] time 0.363 (0.468) data 0.001 (0.086) loss 3.8848 (3.5852) acc 15.6250 (25.1250) lr 2.0000e-03 eta 0:18:55\n",
            "epoch [2/50] batch [30/50] time 0.361 (0.450) data 0.001 (0.072) loss 3.5703 (3.6068) acc 21.8750 (24.3750) lr 2.0000e-03 eta 0:18:10\n",
            "epoch [2/50] batch [35/50] time 0.361 (0.438) data 0.001 (0.062) loss 3.9941 (3.6013) acc 12.5000 (24.3750) lr 2.0000e-03 eta 0:17:37\n",
            "epoch [2/50] batch [40/50] time 0.363 (0.428) data 0.000 (0.054) loss 3.2090 (3.5843) acc 37.5000 (24.6094) lr 2.0000e-03 eta 0:17:11\n",
            "epoch [2/50] batch [45/50] time 0.364 (0.421) data 0.001 (0.048) loss 3.3359 (3.5594) acc 18.7500 (24.3056) lr 2.0000e-03 eta 0:16:52\n",
            "epoch [2/50] batch [50/50] time 0.364 (0.415) data 0.001 (0.043) loss 2.8418 (3.5495) acc 34.3750 (24.4375) lr 1.9980e-03 eta 0:16:36\n",
            "epoch [3/50] batch [5/50] time 0.354 (0.601) data 0.001 (0.222) loss 3.7578 (3.4062) acc 21.8750 (24.3750) lr 1.9980e-03 eta 0:23:58\n",
            "epoch [3/50] batch [10/50] time 0.362 (0.482) data 0.001 (0.111) loss 2.9258 (3.3385) acc 28.1250 (25.3125) lr 1.9980e-03 eta 0:19:10\n",
            "epoch [3/50] batch [15/50] time 0.364 (0.441) data 0.001 (0.074) loss 3.4336 (3.4108) acc 15.6250 (25.4167) lr 1.9980e-03 eta 0:17:31\n",
            "epoch [3/50] batch [20/50] time 0.358 (0.420) data 0.001 (0.056) loss 3.1133 (3.3892) acc 40.6250 (26.8750) lr 1.9980e-03 eta 0:16:40\n",
            "epoch [3/50] batch [25/50] time 0.360 (0.408) data 0.001 (0.045) loss 3.3320 (3.4070) acc 28.1250 (26.3750) lr 1.9980e-03 eta 0:16:09\n",
            "epoch [3/50] batch [30/50] time 0.365 (0.400) data 0.007 (0.038) loss 3.3809 (3.3986) acc 18.7500 (25.7292) lr 1.9980e-03 eta 0:15:49\n",
            "epoch [3/50] batch [35/50] time 0.362 (0.395) data 0.001 (0.033) loss 3.6699 (3.3846) acc 15.6250 (25.8036) lr 1.9980e-03 eta 0:15:35\n",
            "epoch [3/50] batch [40/50] time 0.357 (0.391) data 0.001 (0.029) loss 3.6191 (3.4417) acc 18.7500 (24.9219) lr 1.9980e-03 eta 0:15:22\n",
            "epoch [3/50] batch [45/50] time 0.357 (0.387) data 0.001 (0.026) loss 3.4258 (3.4398) acc 21.8750 (24.8611) lr 1.9980e-03 eta 0:15:11\n",
            "epoch [3/50] batch [50/50] time 0.357 (0.384) data 0.001 (0.024) loss 3.4473 (3.4281) acc 18.7500 (24.6875) lr 1.9921e-03 eta 0:15:02\n",
            "epoch [4/50] batch [5/50] time 0.361 (0.561) data 0.001 (0.178) loss 3.6738 (3.5602) acc 18.7500 (22.5000) lr 1.9921e-03 eta 0:21:54\n",
            "epoch [4/50] batch [10/50] time 0.354 (0.457) data 0.001 (0.089) loss 2.9980 (3.4590) acc 31.2500 (23.7500) lr 1.9921e-03 eta 0:17:50\n",
            "epoch [4/50] batch [15/50] time 0.355 (0.423) data 0.001 (0.060) loss 3.8027 (3.4014) acc 28.1250 (25.6250) lr 1.9921e-03 eta 0:16:27\n",
            "epoch [4/50] batch [20/50] time 0.355 (0.406) data 0.001 (0.045) loss 3.1035 (3.4119) acc 34.3750 (26.0938) lr 1.9921e-03 eta 0:15:46\n",
            "epoch [4/50] batch [25/50] time 0.353 (0.396) data 0.001 (0.036) loss 3.8340 (3.4045) acc 21.8750 (26.2500) lr 1.9921e-03 eta 0:15:20\n",
            "epoch [4/50] batch [30/50] time 0.361 (0.390) data 0.007 (0.031) loss 3.2578 (3.4169) acc 25.0000 (25.5208) lr 1.9921e-03 eta 0:15:04\n",
            "epoch [4/50] batch [35/50] time 0.354 (0.385) data 0.001 (0.027) loss 3.3945 (3.4327) acc 21.8750 (25.2679) lr 1.9921e-03 eta 0:14:51\n",
            "epoch [4/50] batch [40/50] time 0.354 (0.381) data 0.001 (0.024) loss 3.4980 (3.4397) acc 31.2500 (25.0781) lr 1.9921e-03 eta 0:14:40\n",
            "epoch [4/50] batch [45/50] time 0.354 (0.378) data 0.001 (0.021) loss 3.1836 (3.4200) acc 31.2500 (25.2083) lr 1.9921e-03 eta 0:14:31\n",
            "epoch [4/50] batch [50/50] time 0.353 (0.376) data 0.000 (0.019) loss 3.4219 (3.4282) acc 43.7500 (25.3125) lr 1.9823e-03 eta 0:14:24\n",
            "epoch [5/50] batch [5/50] time 0.363 (0.542) data 0.001 (0.152) loss 3.7988 (3.1637) acc 18.7500 (28.7500) lr 1.9823e-03 eta 0:20:42\n",
            "epoch [5/50] batch [10/50] time 0.351 (0.447) data 0.001 (0.076) loss 3.6680 (3.3236) acc 21.8750 (25.3125) lr 1.9823e-03 eta 0:17:03\n",
            "epoch [5/50] batch [15/50] time 0.352 (0.416) data 0.001 (0.051) loss 3.5234 (3.4047) acc 28.1250 (24.5833) lr 1.9823e-03 eta 0:15:50\n",
            "epoch [5/50] batch [20/50] time 0.354 (0.400) data 0.001 (0.038) loss 3.1367 (3.3989) acc 37.5000 (26.0938) lr 1.9823e-03 eta 0:15:12\n",
            "epoch [5/50] batch [25/50] time 0.353 (0.391) data 0.001 (0.031) loss 3.0371 (3.3562) acc 31.2500 (26.6250) lr 1.9823e-03 eta 0:14:49\n",
            "epoch [5/50] batch [30/50] time 0.357 (0.385) data 0.004 (0.026) loss 3.5742 (3.3642) acc 18.7500 (26.4583) lr 1.9823e-03 eta 0:14:34\n",
            "epoch [5/50] batch [35/50] time 0.355 (0.382) data 0.001 (0.023) loss 3.3770 (3.3204) acc 28.1250 (27.4107) lr 1.9823e-03 eta 0:14:24\n",
            "epoch [5/50] batch [40/50] time 0.353 (0.378) data 0.001 (0.020) loss 3.0117 (3.3139) acc 28.1250 (26.7188) lr 1.9823e-03 eta 0:14:15\n",
            "epoch [5/50] batch [45/50] time 0.354 (0.376) data 0.001 (0.018) loss 2.8418 (3.2944) acc 31.2500 (27.0833) lr 1.9823e-03 eta 0:14:07\n",
            "epoch [5/50] batch [50/50] time 0.354 (0.374) data 0.001 (0.016) loss 3.1406 (3.2941) acc 28.1250 (26.8750) lr 1.9686e-03 eta 0:14:00\n",
            "epoch [6/50] batch [5/50] time 0.355 (0.529) data 0.001 (0.153) loss 3.8984 (3.1547) acc 12.5000 (29.3750) lr 1.9686e-03 eta 0:19:47\n",
            "epoch [6/50] batch [10/50] time 0.356 (0.442) data 0.000 (0.077) loss 2.7754 (3.1555) acc 43.7500 (31.5625) lr 1.9686e-03 eta 0:16:29\n",
            "epoch [6/50] batch [15/50] time 0.354 (0.413) data 0.001 (0.052) loss 3.4180 (3.2165) acc 28.1250 (28.7500) lr 1.9686e-03 eta 0:15:22\n",
            "epoch [6/50] batch [20/50] time 0.355 (0.399) data 0.000 (0.039) loss 2.9922 (3.2555) acc 34.3750 (28.7500) lr 1.9686e-03 eta 0:14:48\n",
            "epoch [6/50] batch [25/50] time 0.356 (0.390) data 0.000 (0.031) loss 3.2773 (3.2384) acc 28.1250 (29.2500) lr 1.9686e-03 eta 0:14:27\n",
            "epoch [6/50] batch [30/50] time 0.365 (0.386) data 0.007 (0.027) loss 3.8125 (3.2417) acc 12.5000 (28.8542) lr 1.9686e-03 eta 0:14:16\n",
            "epoch [6/50] batch [35/50] time 0.359 (0.382) data 0.006 (0.024) loss 2.9043 (3.2257) acc 34.3750 (28.3929) lr 1.9686e-03 eta 0:14:06\n",
            "epoch [6/50] batch [40/50] time 0.357 (0.379) data 0.001 (0.021) loss 3.9141 (3.2693) acc 21.8750 (27.9688) lr 1.9686e-03 eta 0:13:57\n",
            "epoch [6/50] batch [45/50] time 0.355 (0.376) data 0.001 (0.019) loss 3.5137 (3.2537) acc 25.0000 (28.4722) lr 1.9686e-03 eta 0:13:50\n",
            "epoch [6/50] batch [50/50] time 0.360 (0.375) data 0.000 (0.017) loss 3.3398 (3.2594) acc 28.1250 (28.5000) lr 1.9511e-03 eta 0:13:44\n",
            "epoch [7/50] batch [5/50] time 0.354 (0.554) data 0.001 (0.173) loss 3.2129 (3.1023) acc 40.6250 (31.2500) lr 1.9511e-03 eta 0:20:16\n",
            "epoch [7/50] batch [10/50] time 0.355 (0.455) data 0.001 (0.087) loss 3.0664 (3.1016) acc 37.5000 (31.5625) lr 1.9511e-03 eta 0:16:37\n",
            "epoch [7/50] batch [15/50] time 0.358 (0.423) data 0.001 (0.059) loss 3.1680 (3.1909) acc 31.2500 (30.6250) lr 1.9511e-03 eta 0:15:23\n",
            "epoch [7/50] batch [20/50] time 0.362 (0.407) data 0.006 (0.045) loss 3.6719 (3.2194) acc 34.3750 (30.4688) lr 1.9511e-03 eta 0:14:47\n",
            "epoch [7/50] batch [25/50] time 0.366 (0.399) data 0.000 (0.037) loss 3.1387 (3.2005) acc 31.2500 (30.2500) lr 1.9511e-03 eta 0:14:27\n",
            "epoch [7/50] batch [30/50] time 0.358 (0.393) data 0.001 (0.031) loss 3.1035 (3.2598) acc 40.6250 (30.0000) lr 1.9511e-03 eta 0:14:12\n",
            "epoch [7/50] batch [35/50] time 0.357 (0.388) data 0.001 (0.028) loss 3.3926 (3.2796) acc 31.2500 (29.1964) lr 1.9511e-03 eta 0:14:00\n",
            "epoch [7/50] batch [40/50] time 0.361 (0.385) data 0.001 (0.024) loss 3.0117 (3.3133) acc 34.3750 (28.4375) lr 1.9511e-03 eta 0:13:50\n",
            "epoch [7/50] batch [45/50] time 0.358 (0.381) data 0.001 (0.022) loss 2.9219 (3.3246) acc 37.5000 (28.1250) lr 1.9511e-03 eta 0:13:42\n",
            "epoch [7/50] batch [50/50] time 0.356 (0.379) data 0.000 (0.019) loss 3.7871 (3.3410) acc 12.5000 (27.3750) lr 1.9298e-03 eta 0:13:34\n",
            "epoch [8/50] batch [5/50] time 0.352 (0.571) data 0.001 (0.195) loss 3.7891 (3.6035) acc 21.8750 (19.3750) lr 1.9298e-03 eta 0:20:25\n",
            "epoch [8/50] batch [10/50] time 0.360 (0.464) data 0.001 (0.098) loss 3.4004 (3.3949) acc 25.0000 (23.7500) lr 1.9298e-03 eta 0:16:32\n",
            "epoch [8/50] batch [15/50] time 0.354 (0.428) data 0.001 (0.065) loss 2.9531 (3.2602) acc 28.1250 (25.8333) lr 1.9298e-03 eta 0:15:13\n",
            "epoch [8/50] batch [20/50] time 0.357 (0.410) data 0.001 (0.049) loss 3.6543 (3.2731) acc 12.5000 (25.4688) lr 1.9298e-03 eta 0:14:33\n",
            "epoch [8/50] batch [25/50] time 0.371 (0.400) data 0.012 (0.040) loss 2.9570 (3.2891) acc 34.3750 (25.0000) lr 1.9298e-03 eta 0:14:09\n",
            "epoch [8/50] batch [30/50] time 0.375 (0.395) data 0.008 (0.035) loss 3.2344 (3.3392) acc 25.0000 (24.1667) lr 1.9298e-03 eta 0:13:56\n",
            "epoch [8/50] batch [35/50] time 0.356 (0.390) data 0.001 (0.030) loss 3.2070 (3.3286) acc 28.1250 (24.9107) lr 1.9298e-03 eta 0:13:43\n",
            "epoch [8/50] batch [40/50] time 0.357 (0.386) data 0.001 (0.026) loss 3.1543 (3.3097) acc 21.8750 (25.7812) lr 1.9298e-03 eta 0:13:33\n",
            "epoch [8/50] batch [45/50] time 0.358 (0.382) data 0.001 (0.023) loss 3.0762 (3.3017) acc 25.0000 (25.7639) lr 1.9298e-03 eta 0:13:24\n",
            "epoch [8/50] batch [50/50] time 0.356 (0.380) data 0.000 (0.021) loss 2.6992 (3.2637) acc 50.0000 (26.7500) lr 1.9048e-03 eta 0:13:17\n",
            "epoch [9/50] batch [5/50] time 0.379 (0.629) data 0.009 (0.196) loss 2.8730 (3.0387) acc 31.2500 (32.5000) lr 1.9048e-03 eta 0:21:58\n",
            "epoch [9/50] batch [10/50] time 0.355 (0.492) data 0.003 (0.098) loss 3.4082 (3.2543) acc 31.2500 (28.1250) lr 1.9048e-03 eta 0:17:07\n",
            "epoch [9/50] batch [15/50] time 0.356 (0.446) data 0.001 (0.066) loss 3.0918 (3.2198) acc 28.1250 (29.3750) lr 1.9048e-03 eta 0:15:30\n",
            "epoch [9/50] batch [20/50] time 0.367 (0.424) data 0.007 (0.050) loss 3.4238 (3.2663) acc 21.8750 (27.6562) lr 1.9048e-03 eta 0:14:41\n",
            "epoch [9/50] batch [25/50] time 0.365 (0.411) data 0.008 (0.041) loss 2.7363 (3.2769) acc 40.6250 (27.7500) lr 1.9048e-03 eta 0:14:13\n",
            "epoch [9/50] batch [30/50] time 0.363 (0.403) data 0.008 (0.034) loss 3.2188 (3.2958) acc 28.1250 (27.3958) lr 1.9048e-03 eta 0:13:53\n",
            "epoch [9/50] batch [35/50] time 0.358 (0.396) data 0.001 (0.030) loss 2.7305 (3.2902) acc 46.8750 (28.0357) lr 1.9048e-03 eta 0:13:38\n",
            "epoch [9/50] batch [40/50] time 0.356 (0.391) data 0.001 (0.026) loss 2.4707 (3.2605) acc 43.7500 (28.5938) lr 1.9048e-03 eta 0:13:25\n",
            "epoch [9/50] batch [45/50] time 0.354 (0.387) data 0.000 (0.023) loss 3.2656 (3.2816) acc 21.8750 (28.1250) lr 1.9048e-03 eta 0:13:15\n",
            "epoch [9/50] batch [50/50] time 0.355 (0.384) data 0.001 (0.021) loss 3.2461 (3.2878) acc 34.3750 (28.2500) lr 1.8763e-03 eta 0:13:07\n",
            "epoch [10/50] batch [5/50] time 0.350 (0.603) data 0.001 (0.228) loss 3.9180 (3.4531) acc 21.8750 (22.5000) lr 1.8763e-03 eta 0:20:33\n",
            "epoch [10/50] batch [10/50] time 0.354 (0.479) data 0.001 (0.114) loss 3.4160 (3.2773) acc 15.6250 (25.6250) lr 1.8763e-03 eta 0:16:16\n",
            "epoch [10/50] batch [15/50] time 0.355 (0.438) data 0.001 (0.077) loss 3.0352 (3.1565) acc 34.3750 (28.1250) lr 1.8763e-03 eta 0:14:50\n",
            "epoch [10/50] batch [20/50] time 0.369 (0.418) data 0.012 (0.059) loss 3.3730 (3.1750) acc 28.1250 (28.1250) lr 1.8763e-03 eta 0:14:09\n",
            "epoch [10/50] batch [25/50] time 0.360 (0.408) data 0.001 (0.048) loss 3.4492 (3.1841) acc 21.8750 (28.1250) lr 1.8763e-03 eta 0:13:45\n",
            "epoch [10/50] batch [30/50] time 0.360 (0.400) data 0.001 (0.041) loss 2.6758 (3.1598) acc 43.7500 (29.4792) lr 1.8763e-03 eta 0:13:28\n",
            "epoch [10/50] batch [35/50] time 0.356 (0.395) data 0.001 (0.036) loss 2.8691 (3.1754) acc 31.2500 (29.1071) lr 1.8763e-03 eta 0:13:15\n",
            "epoch [10/50] batch [40/50] time 0.354 (0.390) data 0.001 (0.031) loss 3.2969 (3.1886) acc 28.1250 (28.5156) lr 1.8763e-03 eta 0:13:03\n",
            "epoch [10/50] batch [45/50] time 0.355 (0.386) data 0.000 (0.028) loss 2.9316 (3.1998) acc 34.3750 (28.6111) lr 1.8763e-03 eta 0:12:53\n",
            "epoch [10/50] batch [50/50] time 0.356 (0.383) data 0.000 (0.025) loss 2.9551 (3.1871) acc 37.5000 (29.1250) lr 1.8443e-03 eta 0:12:45\n",
            "epoch [11/50] batch [5/50] time 0.351 (0.585) data 0.001 (0.209) loss 3.5879 (3.3137) acc 25.0000 (28.7500) lr 1.8443e-03 eta 0:19:26\n",
            "epoch [11/50] batch [10/50] time 0.357 (0.471) data 0.001 (0.105) loss 2.7383 (3.1803) acc 28.1250 (28.4375) lr 1.8443e-03 eta 0:15:36\n",
            "epoch [11/50] batch [15/50] time 0.360 (0.432) data 0.001 (0.070) loss 4.0391 (3.2470) acc 18.7500 (28.5417) lr 1.8443e-03 eta 0:14:18\n",
            "epoch [11/50] batch [20/50] time 0.356 (0.414) data 0.001 (0.054) loss 3.3672 (3.2519) acc 18.7500 (28.5938) lr 1.8443e-03 eta 0:13:40\n",
            "epoch [11/50] batch [25/50] time 0.369 (0.404) data 0.008 (0.044) loss 3.8086 (3.2217) acc 21.8750 (29.6250) lr 1.8443e-03 eta 0:13:17\n",
            "epoch [11/50] batch [30/50] time 0.365 (0.397) data 0.008 (0.037) loss 3.3594 (3.2234) acc 25.0000 (29.7917) lr 1.8443e-03 eta 0:13:02\n",
            "epoch [11/50] batch [35/50] time 0.359 (0.393) data 0.001 (0.033) loss 3.8555 (3.2381) acc 15.6250 (29.9107) lr 1.8443e-03 eta 0:12:51\n",
            "epoch [11/50] batch [40/50] time 0.356 (0.388) data 0.000 (0.029) loss 2.9980 (3.2125) acc 31.2500 (30.1562) lr 1.8443e-03 eta 0:12:40\n",
            "epoch [11/50] batch [45/50] time 0.355 (0.384) data 0.000 (0.025) loss 3.0000 (3.2042) acc 28.1250 (29.6528) lr 1.8443e-03 eta 0:12:31\n",
            "epoch [11/50] batch [50/50] time 0.356 (0.381) data 0.000 (0.023) loss 2.7734 (3.2332) acc 31.2500 (29.0000) lr 1.8090e-03 eta 0:12:23\n",
            "epoch [12/50] batch [5/50] time 0.354 (0.567) data 0.001 (0.166) loss 3.3770 (2.8449) acc 25.0000 (34.3750) lr 1.8090e-03 eta 0:18:21\n",
            "epoch [12/50] batch [10/50] time 0.357 (0.461) data 0.001 (0.084) loss 2.9648 (3.0084) acc 37.5000 (32.8125) lr 1.8090e-03 eta 0:14:54\n",
            "epoch [12/50] batch [15/50] time 0.363 (0.426) data 0.008 (0.056) loss 3.6660 (3.1432) acc 25.0000 (30.6250) lr 1.8090e-03 eta 0:13:45\n",
            "epoch [12/50] batch [20/50] time 0.362 (0.410) data 0.006 (0.043) loss 3.2344 (3.1589) acc 28.1250 (30.0000) lr 1.8090e-03 eta 0:13:11\n",
            "epoch [12/50] batch [25/50] time 0.356 (0.400) data 0.001 (0.036) loss 3.0195 (3.1351) acc 18.7500 (29.2500) lr 1.8090e-03 eta 0:12:50\n",
            "epoch [12/50] batch [30/50] time 0.369 (0.395) data 0.006 (0.032) loss 3.2207 (3.1727) acc 25.0000 (28.5417) lr 1.8090e-03 eta 0:12:38\n",
            "epoch [12/50] batch [35/50] time 0.355 (0.390) data 0.001 (0.028) loss 3.2812 (3.2328) acc 31.2500 (27.9464) lr 1.8090e-03 eta 0:12:27\n",
            "epoch [12/50] batch [40/50] time 0.353 (0.386) data 0.000 (0.024) loss 3.2754 (3.2494) acc 18.7500 (26.8750) lr 1.8090e-03 eta 0:12:17\n",
            "epoch [12/50] batch [45/50] time 0.355 (0.382) data 0.000 (0.022) loss 3.2754 (3.2307) acc 21.8750 (27.0833) lr 1.8090e-03 eta 0:12:08\n",
            "epoch [12/50] batch [50/50] time 0.358 (0.380) data 0.000 (0.020) loss 3.0762 (3.2209) acc 28.1250 (27.1875) lr 1.7705e-03 eta 0:12:01\n",
            "epoch [13/50] batch [5/50] time 0.354 (0.530) data 0.000 (0.145) loss 2.8809 (3.2098) acc 43.7500 (33.7500) lr 1.7705e-03 eta 0:16:44\n",
            "epoch [13/50] batch [10/50] time 0.356 (0.442) data 0.001 (0.073) loss 3.6211 (3.3324) acc 15.6250 (27.8125) lr 1.7705e-03 eta 0:13:55\n",
            "epoch [13/50] batch [15/50] time 0.368 (0.414) data 0.013 (0.050) loss 2.6504 (3.2362) acc 37.5000 (28.1250) lr 1.7705e-03 eta 0:13:00\n",
            "epoch [13/50] batch [20/50] time 0.369 (0.401) data 0.001 (0.037) loss 2.4844 (3.1489) acc 40.6250 (30.0000) lr 1.7705e-03 eta 0:12:33\n",
            "epoch [13/50] batch [25/50] time 0.356 (0.393) data 0.001 (0.031) loss 3.5215 (3.1415) acc 25.0000 (30.2500) lr 1.7705e-03 eta 0:12:17\n",
            "epoch [13/50] batch [30/50] time 0.356 (0.388) data 0.001 (0.026) loss 3.2480 (3.1331) acc 37.5000 (31.1458) lr 1.7705e-03 eta 0:12:04\n",
            "epoch [13/50] batch [35/50] time 0.356 (0.384) data 0.001 (0.023) loss 2.8301 (3.1222) acc 37.5000 (31.5179) lr 1.7705e-03 eta 0:11:55\n",
            "epoch [13/50] batch [40/50] time 0.355 (0.380) data 0.000 (0.020) loss 3.4648 (3.1394) acc 18.7500 (31.0938) lr 1.7705e-03 eta 0:11:47\n",
            "epoch [13/50] batch [45/50] time 0.353 (0.377) data 0.000 (0.018) loss 3.3965 (3.1300) acc 18.7500 (30.6944) lr 1.7705e-03 eta 0:11:39\n",
            "epoch [13/50] batch [50/50] time 0.353 (0.375) data 0.000 (0.016) loss 3.2051 (3.1458) acc 28.1250 (30.6250) lr 1.7290e-03 eta 0:11:33\n",
            "epoch [14/50] batch [5/50] time 0.352 (0.543) data 0.001 (0.157) loss 2.4531 (2.7492) acc 34.3750 (36.2500) lr 1.7290e-03 eta 0:16:42\n",
            "epoch [14/50] batch [10/50] time 0.359 (0.450) data 0.007 (0.080) loss 2.9570 (2.9459) acc 40.6250 (33.4375) lr 1.7290e-03 eta 0:13:47\n",
            "epoch [14/50] batch [15/50] time 0.366 (0.422) data 0.008 (0.056) loss 3.4395 (3.0094) acc 25.0000 (32.5000) lr 1.7290e-03 eta 0:12:54\n",
            "epoch [14/50] batch [20/50] time 0.357 (0.407) data 0.001 (0.043) loss 2.4648 (2.9631) acc 37.5000 (33.9062) lr 1.7290e-03 eta 0:12:24\n",
            "epoch [14/50] batch [25/50] time 0.364 (0.398) data 0.013 (0.036) loss 2.8574 (2.9701) acc 31.2500 (33.8750) lr 1.7290e-03 eta 0:12:06\n",
            "epoch [14/50] batch [30/50] time 0.362 (0.392) data 0.001 (0.031) loss 3.1660 (2.9864) acc 34.3750 (33.4375) lr 1.7290e-03 eta 0:11:54\n",
            "epoch [14/50] batch [35/50] time 0.354 (0.388) data 0.001 (0.027) loss 3.5586 (3.0136) acc 25.0000 (33.2143) lr 1.7290e-03 eta 0:11:44\n",
            "epoch [14/50] batch [40/50] time 0.356 (0.384) data 0.000 (0.024) loss 3.4043 (3.0013) acc 18.7500 (32.8906) lr 1.7290e-03 eta 0:11:34\n",
            "epoch [14/50] batch [45/50] time 0.354 (0.381) data 0.000 (0.021) loss 2.9668 (2.9865) acc 31.2500 (33.0556) lr 1.7290e-03 eta 0:11:27\n",
            "epoch [14/50] batch [50/50] time 0.355 (0.378) data 0.000 (0.019) loss 2.8730 (3.0043) acc 40.6250 (32.8125) lr 1.6845e-03 eta 0:11:20\n",
            "epoch [15/50] batch [5/50] time 0.350 (0.566) data 0.000 (0.182) loss 2.4551 (2.9203) acc 34.3750 (30.6250) lr 1.6845e-03 eta 0:16:56\n",
            "epoch [15/50] batch [10/50] time 0.365 (0.462) data 0.011 (0.093) loss 2.8594 (2.8432) acc 34.3750 (32.1875) lr 1.6845e-03 eta 0:13:46\n",
            "epoch [15/50] batch [15/50] time 0.357 (0.427) data 0.001 (0.063) loss 2.5586 (2.8258) acc 40.6250 (33.1250) lr 1.6845e-03 eta 0:12:42\n",
            "epoch [15/50] batch [20/50] time 0.361 (0.410) data 0.008 (0.049) loss 3.5059 (2.8871) acc 25.0000 (33.7500) lr 1.6845e-03 eta 0:12:10\n",
            "epoch [15/50] batch [25/50] time 0.356 (0.400) data 0.001 (0.040) loss 3.6035 (2.9211) acc 18.7500 (33.1250) lr 1.6845e-03 eta 0:11:50\n",
            "epoch [15/50] batch [30/50] time 0.366 (0.394) data 0.010 (0.034) loss 3.9199 (2.9833) acc 18.7500 (32.6042) lr 1.6845e-03 eta 0:11:37\n",
            "epoch [15/50] batch [35/50] time 0.355 (0.388) data 0.001 (0.029) loss 2.6309 (3.0215) acc 34.3750 (32.0536) lr 1.6845e-03 eta 0:11:25\n",
            "epoch [15/50] batch [40/50] time 0.354 (0.384) data 0.000 (0.026) loss 3.0645 (3.0350) acc 28.1250 (31.7969) lr 1.6845e-03 eta 0:11:15\n",
            "epoch [15/50] batch [45/50] time 0.354 (0.381) data 0.000 (0.023) loss 3.0312 (3.0490) acc 25.0000 (31.3889) lr 1.6845e-03 eta 0:11:08\n",
            "epoch [15/50] batch [50/50] time 0.355 (0.378) data 0.001 (0.021) loss 3.5820 (3.0645) acc 15.6250 (30.9375) lr 1.6374e-03 eta 0:11:01\n",
            "epoch [16/50] batch [5/50] time 0.349 (0.593) data 0.001 (0.225) loss 3.6621 (2.9281) acc 28.1250 (30.6250) lr 1.6374e-03 eta 0:17:14\n",
            "epoch [16/50] batch [10/50] time 0.356 (0.475) data 0.001 (0.113) loss 2.8711 (2.8855) acc 31.2500 (33.1250) lr 1.6374e-03 eta 0:13:47\n",
            "epoch [16/50] batch [15/50] time 0.364 (0.438) data 0.010 (0.079) loss 2.9434 (2.9849) acc 21.8750 (31.4583) lr 1.6374e-03 eta 0:12:39\n",
            "epoch [16/50] batch [20/50] time 0.363 (0.418) data 0.001 (0.060) loss 2.4434 (2.9413) acc 43.7500 (31.8750) lr 1.6374e-03 eta 0:12:03\n",
            "epoch [16/50] batch [25/50] time 0.360 (0.407) data 0.006 (0.049) loss 3.5879 (2.9932) acc 34.3750 (31.8750) lr 1.6374e-03 eta 0:11:42\n",
            "epoch [16/50] batch [30/50] time 0.353 (0.399) data 0.001 (0.041) loss 2.8262 (2.9992) acc 28.1250 (31.4583) lr 1.6374e-03 eta 0:11:26\n",
            "epoch [16/50] batch [35/50] time 0.355 (0.393) data 0.001 (0.035) loss 2.9512 (2.9998) acc 31.2500 (31.6071) lr 1.6374e-03 eta 0:11:13\n",
            "epoch [16/50] batch [40/50] time 0.355 (0.388) data 0.000 (0.031) loss 2.9023 (3.0149) acc 40.6250 (31.7188) lr 1.6374e-03 eta 0:11:03\n",
            "epoch [16/50] batch [45/50] time 0.356 (0.384) data 0.000 (0.027) loss 2.9961 (3.0248) acc 40.6250 (31.3889) lr 1.6374e-03 eta 0:10:55\n",
            "epoch [16/50] batch [50/50] time 0.353 (0.381) data 0.000 (0.025) loss 3.3379 (3.0598) acc 25.0000 (30.6875) lr 1.5878e-03 eta 0:10:48\n",
            "epoch [17/50] batch [5/50] time 0.358 (0.567) data 0.007 (0.170) loss 3.0254 (2.9703) acc 34.3750 (33.7500) lr 1.5878e-03 eta 0:16:00\n",
            "epoch [17/50] batch [10/50] time 0.361 (0.463) data 0.001 (0.087) loss 3.1172 (3.0602) acc 25.0000 (31.8750) lr 1.5878e-03 eta 0:13:03\n",
            "epoch [17/50] batch [15/50] time 0.366 (0.430) data 0.011 (0.060) loss 3.0430 (3.1089) acc 28.1250 (31.2500) lr 1.5878e-03 eta 0:12:04\n",
            "epoch [17/50] batch [20/50] time 0.371 (0.413) data 0.008 (0.046) loss 3.2969 (3.1266) acc 25.0000 (31.0938) lr 1.5878e-03 eta 0:11:33\n",
            "epoch [17/50] batch [25/50] time 0.368 (0.403) data 0.011 (0.038) loss 3.1992 (3.1858) acc 28.1250 (30.1250) lr 1.5878e-03 eta 0:11:15\n",
            "epoch [17/50] batch [30/50] time 0.372 (0.397) data 0.012 (0.034) loss 3.1973 (3.1883) acc 31.2500 (29.4792) lr 1.5878e-03 eta 0:11:03\n",
            "epoch [17/50] batch [35/50] time 0.356 (0.392) data 0.001 (0.030) loss 3.0234 (3.1926) acc 37.5000 (29.7321) lr 1.5878e-03 eta 0:10:53\n",
            "epoch [17/50] batch [40/50] time 0.356 (0.388) data 0.000 (0.026) loss 2.4980 (3.1808) acc 43.7500 (29.8438) lr 1.5878e-03 eta 0:10:43\n",
            "epoch [17/50] batch [45/50] time 0.355 (0.384) data 0.000 (0.023) loss 3.3242 (3.1678) acc 25.0000 (29.8611) lr 1.5878e-03 eta 0:10:35\n",
            "epoch [17/50] batch [50/50] time 0.356 (0.381) data 0.000 (0.021) loss 2.9258 (3.1518) acc 31.2500 (29.9375) lr 1.5358e-03 eta 0:10:29\n",
            "epoch [18/50] batch [5/50] time 0.364 (0.578) data 0.009 (0.200) loss 2.8125 (3.0488) acc 31.2500 (34.3750) lr 1.5358e-03 eta 0:15:50\n",
            "epoch [18/50] batch [10/50] time 0.364 (0.470) data 0.009 (0.103) loss 3.4707 (2.9619) acc 28.1250 (36.8750) lr 1.5358e-03 eta 0:12:51\n",
            "epoch [18/50] batch [15/50] time 0.363 (0.435) data 0.011 (0.072) loss 2.8027 (2.9766) acc 34.3750 (35.4167) lr 1.5358e-03 eta 0:11:50\n",
            "epoch [18/50] batch [20/50] time 0.364 (0.416) data 0.001 (0.055) loss 3.0840 (2.9840) acc 31.2500 (34.8438) lr 1.5358e-03 eta 0:11:18\n",
            "epoch [18/50] batch [25/50] time 0.357 (0.405) data 0.001 (0.044) loss 3.0078 (3.0174) acc 28.1250 (34.0000) lr 1.5358e-03 eta 0:10:57\n",
            "epoch [18/50] batch [30/50] time 0.357 (0.396) data 0.000 (0.037) loss 2.5605 (3.0110) acc 34.3750 (34.0625) lr 1.5358e-03 eta 0:10:42\n",
            "epoch [18/50] batch [35/50] time 0.356 (0.391) data 0.001 (0.032) loss 3.1719 (3.0353) acc 25.0000 (33.6607) lr 1.5358e-03 eta 0:10:30\n",
            "epoch [18/50] batch [40/50] time 0.354 (0.386) data 0.000 (0.028) loss 3.1445 (3.0275) acc 37.5000 (33.6719) lr 1.5358e-03 eta 0:10:21\n",
            "epoch [18/50] batch [45/50] time 0.356 (0.383) data 0.000 (0.025) loss 2.1406 (3.0194) acc 46.8750 (33.6806) lr 1.5358e-03 eta 0:10:14\n",
            "epoch [18/50] batch [50/50] time 0.354 (0.380) data 0.000 (0.022) loss 3.2246 (3.0295) acc 34.3750 (33.4375) lr 1.4818e-03 eta 0:10:07\n",
            "epoch [19/50] batch [5/50] time 0.386 (0.650) data 0.012 (0.171) loss 2.6289 (2.7008) acc 43.7500 (38.7500) lr 1.4818e-03 eta 0:17:16\n",
            "epoch [19/50] batch [10/50] time 0.353 (0.505) data 0.001 (0.088) loss 2.8086 (2.8146) acc 37.5000 (35.6250) lr 1.4818e-03 eta 0:13:23\n",
            "epoch [19/50] batch [15/50] time 0.363 (0.458) data 0.008 (0.061) loss 2.7812 (2.8353) acc 31.2500 (36.8750) lr 1.4818e-03 eta 0:12:06\n",
            "epoch [19/50] batch [20/50] time 0.363 (0.435) data 0.008 (0.048) loss 3.1465 (2.9000) acc 28.1250 (35.1562) lr 1.4818e-03 eta 0:11:27\n",
            "epoch [19/50] batch [25/50] time 0.357 (0.420) data 0.001 (0.039) loss 3.3047 (2.9205) acc 28.1250 (34.7500) lr 1.4818e-03 eta 0:11:02\n",
            "epoch [19/50] batch [30/50] time 0.354 (0.410) data 0.001 (0.032) loss 2.2461 (2.9299) acc 46.8750 (34.2708) lr 1.4818e-03 eta 0:10:43\n",
            "epoch [19/50] batch [35/50] time 0.356 (0.402) data 0.001 (0.028) loss 3.2129 (2.9584) acc 18.7500 (33.3929) lr 1.4818e-03 eta 0:10:29\n",
            "epoch [19/50] batch [40/50] time 0.354 (0.396) data 0.000 (0.025) loss 2.4844 (2.9787) acc 37.5000 (32.6562) lr 1.4818e-03 eta 0:10:17\n",
            "epoch [19/50] batch [45/50] time 0.355 (0.391) data 0.000 (0.022) loss 2.9473 (2.9833) acc 34.3750 (32.5000) lr 1.4818e-03 eta 0:10:08\n",
            "epoch [19/50] batch [50/50] time 0.354 (0.388) data 0.000 (0.020) loss 3.0098 (3.0010) acc 25.0000 (32.2500) lr 1.4258e-03 eta 0:10:00\n",
            "epoch [20/50] batch [5/50] time 0.416 (0.667) data 0.013 (0.188) loss 3.4160 (2.7859) acc 25.0000 (37.5000) lr 1.4258e-03 eta 0:17:11\n",
            "epoch [20/50] batch [10/50] time 0.358 (0.513) data 0.001 (0.095) loss 2.7070 (2.8824) acc 37.5000 (34.3750) lr 1.4258e-03 eta 0:13:09\n",
            "epoch [20/50] batch [15/50] time 0.370 (0.464) data 0.007 (0.066) loss 2.9648 (2.9934) acc 25.0000 (32.9167) lr 1.4258e-03 eta 0:11:52\n",
            "epoch [20/50] batch [20/50] time 0.364 (0.439) data 0.008 (0.051) loss 2.7500 (2.9421) acc 40.6250 (33.1250) lr 1.4258e-03 eta 0:11:10\n",
            "epoch [20/50] batch [25/50] time 0.353 (0.423) data 0.000 (0.042) loss 3.2344 (2.9665) acc 31.2500 (33.0000) lr 1.4258e-03 eta 0:10:44\n",
            "epoch [20/50] batch [30/50] time 0.355 (0.411) data 0.001 (0.035) loss 3.3652 (2.9545) acc 28.1250 (33.1250) lr 1.4258e-03 eta 0:10:25\n",
            "epoch [20/50] batch [35/50] time 0.356 (0.403) data 0.001 (0.030) loss 2.5742 (2.9499) acc 43.7500 (33.2143) lr 1.4258e-03 eta 0:10:10\n",
            "epoch [20/50] batch [40/50] time 0.354 (0.397) data 0.000 (0.026) loss 2.8633 (2.9669) acc 46.8750 (32.8906) lr 1.4258e-03 eta 0:09:59\n",
            "epoch [20/50] batch [45/50] time 0.355 (0.392) data 0.000 (0.023) loss 3.6270 (2.9685) acc 25.0000 (33.3333) lr 1.4258e-03 eta 0:09:50\n",
            "epoch [20/50] batch [50/50] time 0.355 (0.389) data 0.000 (0.021) loss 3.9531 (3.0056) acc 25.0000 (32.8125) lr 1.3681e-03 eta 0:09:42\n",
            "epoch [21/50] batch [5/50] time 0.358 (0.732) data 0.001 (0.291) loss 3.1934 (2.9297) acc 34.3750 (37.5000) lr 1.3681e-03 eta 0:18:13\n",
            "epoch [21/50] batch [10/50] time 0.354 (0.546) data 0.001 (0.148) loss 2.7461 (2.9510) acc 34.3750 (33.7500) lr 1.3681e-03 eta 0:13:33\n",
            "epoch [21/50] batch [15/50] time 0.356 (0.484) data 0.001 (0.100) loss 3.0547 (2.9484) acc 37.5000 (32.2917) lr 1.3681e-03 eta 0:11:58\n",
            "epoch [21/50] batch [20/50] time 0.355 (0.452) data 0.001 (0.076) loss 3.0723 (2.9605) acc 34.3750 (33.1250) lr 1.3681e-03 eta 0:11:09\n",
            "epoch [21/50] batch [25/50] time 0.354 (0.433) data 0.001 (0.061) loss 2.9160 (2.9841) acc 25.0000 (32.5000) lr 1.3681e-03 eta 0:10:38\n",
            "epoch [21/50] batch [30/50] time 0.355 (0.420) data 0.000 (0.051) loss 3.1113 (2.9318) acc 25.0000 (33.5417) lr 1.3681e-03 eta 0:10:17\n",
            "epoch [21/50] batch [35/50] time 0.356 (0.411) data 0.001 (0.043) loss 3.0898 (2.9373) acc 28.1250 (33.2143) lr 1.3681e-03 eta 0:10:01\n",
            "epoch [21/50] batch [40/50] time 0.354 (0.404) data 0.000 (0.038) loss 3.0371 (2.9959) acc 37.5000 (32.1875) lr 1.3681e-03 eta 0:09:49\n",
            "epoch [21/50] batch [45/50] time 0.354 (0.398) data 0.000 (0.034) loss 3.5332 (3.0133) acc 12.5000 (31.7361) lr 1.3681e-03 eta 0:09:39\n",
            "epoch [21/50] batch [50/50] time 0.357 (0.394) data 0.001 (0.031) loss 2.9941 (2.9954) acc 40.6250 (32.0625) lr 1.3090e-03 eta 0:09:31\n",
            "epoch [22/50] batch [5/50] time 0.367 (0.781) data 0.008 (0.323) loss 2.9844 (3.0566) acc 25.0000 (31.2500) lr 1.3090e-03 eta 0:18:49\n",
            "epoch [22/50] batch [10/50] time 0.363 (0.572) data 0.009 (0.166) loss 3.3145 (3.0678) acc 21.8750 (31.5625) lr 1.3090e-03 eta 0:13:44\n",
            "epoch [22/50] batch [15/50] time 0.356 (0.503) data 0.001 (0.112) loss 2.7656 (2.9486) acc 31.2500 (33.7500) lr 1.3090e-03 eta 0:12:01\n",
            "epoch [22/50] batch [20/50] time 0.355 (0.466) data 0.001 (0.085) loss 3.0664 (3.0295) acc 37.5000 (34.0625) lr 1.3090e-03 eta 0:11:06\n",
            "epoch [22/50] batch [25/50] time 0.366 (0.444) data 0.007 (0.068) loss 2.8984 (2.9799) acc 34.3750 (34.1250) lr 1.3090e-03 eta 0:10:32\n",
            "epoch [22/50] batch [30/50] time 0.356 (0.430) data 0.001 (0.057) loss 3.0820 (2.9898) acc 43.7500 (33.5417) lr 1.3090e-03 eta 0:10:10\n",
            "epoch [22/50] batch [35/50] time 0.354 (0.419) data 0.001 (0.049) loss 2.5293 (2.9661) acc 50.0000 (34.6429) lr 1.3090e-03 eta 0:09:53\n",
            "epoch [22/50] batch [40/50] time 0.356 (0.411) data 0.000 (0.043) loss 3.2422 (2.9832) acc 21.8750 (34.3750) lr 1.3090e-03 eta 0:09:39\n",
            "epoch [22/50] batch [45/50] time 0.355 (0.405) data 0.000 (0.038) loss 3.0176 (2.9744) acc 34.3750 (34.6528) lr 1.3090e-03 eta 0:09:29\n",
            "epoch [22/50] batch [50/50] time 0.357 (0.400) data 0.001 (0.034) loss 2.6992 (2.9753) acc 43.7500 (34.6250) lr 1.2487e-03 eta 0:09:20\n",
            "epoch [23/50] batch [5/50] time 0.352 (0.650) data 0.001 (0.287) loss 2.6836 (2.8324) acc 34.3750 (33.7500) lr 1.2487e-03 eta 0:15:06\n",
            "epoch [23/50] batch [10/50] time 0.357 (0.503) data 0.001 (0.144) loss 2.7441 (2.8125) acc 34.3750 (34.6875) lr 1.2487e-03 eta 0:11:38\n",
            "epoch [23/50] batch [15/50] time 0.355 (0.454) data 0.001 (0.096) loss 3.3848 (2.9092) acc 25.0000 (34.3750) lr 1.2487e-03 eta 0:10:28\n",
            "epoch [23/50] batch [20/50] time 0.355 (0.429) data 0.001 (0.072) loss 3.3887 (2.9554) acc 18.7500 (33.2812) lr 1.2487e-03 eta 0:09:52\n",
            "epoch [23/50] batch [25/50] time 0.356 (0.415) data 0.001 (0.058) loss 3.4004 (3.0388) acc 18.7500 (31.1250) lr 1.2487e-03 eta 0:09:30\n",
            "epoch [23/50] batch [30/50] time 0.361 (0.405) data 0.001 (0.049) loss 2.8574 (3.0125) acc 37.5000 (31.7708) lr 1.2487e-03 eta 0:09:15\n",
            "epoch [23/50] batch [35/50] time 0.356 (0.399) data 0.001 (0.042) loss 2.6367 (3.0294) acc 28.1250 (30.6250) lr 1.2487e-03 eta 0:09:04\n",
            "epoch [23/50] batch [40/50] time 0.357 (0.394) data 0.001 (0.037) loss 3.2031 (3.0277) acc 21.8750 (30.9375) lr 1.2487e-03 eta 0:08:55\n",
            "epoch [23/50] batch [45/50] time 0.357 (0.389) data 0.001 (0.033) loss 3.0898 (3.0224) acc 34.3750 (30.8333) lr 1.2487e-03 eta 0:08:47\n",
            "epoch [23/50] batch [50/50] time 0.356 (0.386) data 0.001 (0.030) loss 2.3672 (2.9969) acc 53.1250 (31.6250) lr 1.1874e-03 eta 0:08:41\n",
            "epoch [24/50] batch [5/50] time 0.355 (0.562) data 0.000 (0.170) loss 3.2949 (2.8277) acc 21.8750 (36.8750) lr 1.1874e-03 eta 0:12:35\n",
            "epoch [24/50] batch [10/50] time 0.363 (0.462) data 0.008 (0.087) loss 3.4648 (2.9301) acc 25.0000 (33.7500) lr 1.1874e-03 eta 0:10:19\n",
            "epoch [24/50] batch [15/50] time 0.355 (0.427) data 0.000 (0.058) loss 2.4316 (2.9008) acc 46.8750 (34.3750) lr 1.1874e-03 eta 0:09:29\n",
            "epoch [24/50] batch [20/50] time 0.355 (0.409) data 0.000 (0.044) loss 3.3047 (2.9259) acc 31.2500 (33.9062) lr 1.1874e-03 eta 0:09:03\n",
            "epoch [24/50] batch [25/50] time 0.356 (0.398) data 0.001 (0.035) loss 3.1387 (2.9248) acc 21.8750 (33.6250) lr 1.1874e-03 eta 0:08:47\n",
            "epoch [24/50] batch [30/50] time 0.359 (0.392) data 0.001 (0.030) loss 3.3359 (2.9570) acc 21.8750 (33.3333) lr 1.1874e-03 eta 0:08:37\n",
            "epoch [24/50] batch [35/50] time 0.355 (0.387) data 0.001 (0.026) loss 3.1113 (2.9579) acc 37.5000 (34.1071) lr 1.1874e-03 eta 0:08:29\n",
            "epoch [24/50] batch [40/50] time 0.356 (0.383) data 0.001 (0.023) loss 2.9355 (2.9741) acc 37.5000 (33.6719) lr 1.1874e-03 eta 0:08:22\n",
            "epoch [24/50] batch [45/50] time 0.355 (0.380) data 0.001 (0.020) loss 2.8887 (2.9712) acc 34.3750 (33.7500) lr 1.1874e-03 eta 0:08:16\n",
            "epoch [24/50] batch [50/50] time 0.355 (0.378) data 0.000 (0.018) loss 2.7793 (2.9896) acc 46.8750 (33.6875) lr 1.1253e-03 eta 0:08:11\n",
            "epoch [25/50] batch [5/50] time 0.351 (0.541) data 0.001 (0.170) loss 3.2031 (2.9352) acc 15.6250 (34.3750) lr 1.1253e-03 eta 0:11:40\n",
            "epoch [25/50] batch [10/50] time 0.360 (0.448) data 0.001 (0.085) loss 3.7383 (2.9607) acc 15.6250 (32.1875) lr 1.1253e-03 eta 0:09:38\n",
            "epoch [25/50] batch [15/50] time 0.353 (0.417) data 0.001 (0.057) loss 2.8379 (2.9068) acc 28.1250 (33.7500) lr 1.1253e-03 eta 0:08:55\n",
            "epoch [25/50] batch [20/50] time 0.355 (0.401) data 0.001 (0.043) loss 2.5566 (2.9068) acc 40.6250 (33.5938) lr 1.1253e-03 eta 0:08:33\n",
            "epoch [25/50] batch [25/50] time 0.356 (0.393) data 0.001 (0.035) loss 3.3633 (2.8809) acc 25.0000 (34.2500) lr 1.1253e-03 eta 0:08:20\n",
            "epoch [25/50] batch [30/50] time 0.358 (0.387) data 0.001 (0.029) loss 2.6348 (2.9526) acc 40.6250 (33.7500) lr 1.1253e-03 eta 0:08:11\n",
            "epoch [25/50] batch [35/50] time 0.358 (0.383) data 0.001 (0.025) loss 3.3047 (2.9683) acc 18.7500 (33.1250) lr 1.1253e-03 eta 0:08:04\n",
            "epoch [25/50] batch [40/50] time 0.355 (0.380) data 0.001 (0.022) loss 2.9883 (2.9635) acc 34.3750 (33.2812) lr 1.1253e-03 eta 0:07:58\n",
            "epoch [25/50] batch [45/50] time 0.355 (0.377) data 0.000 (0.020) loss 3.7227 (2.9924) acc 25.0000 (32.7083) lr 1.1253e-03 eta 0:07:53\n",
            "epoch [25/50] batch [50/50] time 0.355 (0.375) data 0.000 (0.018) loss 3.8887 (3.0148) acc 25.0000 (32.4375) lr 1.0628e-03 eta 0:07:48\n",
            "epoch [26/50] batch [5/50] time 0.352 (0.577) data 0.001 (0.192) loss 3.4199 (2.7320) acc 28.1250 (34.3750) lr 1.0628e-03 eta 0:11:57\n",
            "epoch [26/50] batch [10/50] time 0.355 (0.465) data 0.001 (0.096) loss 2.7930 (2.8240) acc 46.8750 (34.3750) lr 1.0628e-03 eta 0:09:37\n",
            "epoch [26/50] batch [15/50] time 0.357 (0.429) data 0.001 (0.064) loss 3.1719 (2.8732) acc 28.1250 (32.9167) lr 1.0628e-03 eta 0:08:49\n",
            "epoch [26/50] batch [20/50] time 0.361 (0.411) data 0.007 (0.049) loss 2.5273 (2.8434) acc 43.7500 (34.0625) lr 1.0628e-03 eta 0:08:25\n",
            "epoch [26/50] batch [25/50] time 0.363 (0.401) data 0.010 (0.040) loss 2.5977 (2.8653) acc 46.8750 (34.5000) lr 1.0628e-03 eta 0:08:11\n",
            "epoch [26/50] batch [30/50] time 0.368 (0.395) data 0.008 (0.035) loss 3.0137 (2.8678) acc 25.0000 (33.7500) lr 1.0628e-03 eta 0:08:02\n",
            "epoch [26/50] batch [35/50] time 0.355 (0.391) data 0.001 (0.031) loss 2.7500 (2.8778) acc 31.2500 (33.8393) lr 1.0628e-03 eta 0:07:54\n",
            "epoch [26/50] batch [40/50] time 0.353 (0.386) data 0.000 (0.027) loss 2.9277 (2.8858) acc 31.2500 (33.7500) lr 1.0628e-03 eta 0:07:47\n",
            "epoch [26/50] batch [45/50] time 0.355 (0.383) data 0.000 (0.024) loss 2.9941 (2.9003) acc 37.5000 (33.8194) lr 1.0628e-03 eta 0:07:41\n",
            "epoch [26/50] batch [50/50] time 0.357 (0.380) data 0.000 (0.022) loss 3.2930 (2.9188) acc 31.2500 (33.5000) lr 1.0000e-03 eta 0:07:36\n",
            "epoch [27/50] batch [5/50] time 0.353 (0.599) data 0.001 (0.207) loss 3.3516 (2.9219) acc 28.1250 (36.2500) lr 1.0000e-03 eta 0:11:55\n",
            "epoch [27/50] batch [10/50] time 0.355 (0.477) data 0.001 (0.104) loss 2.5605 (2.8799) acc 40.6250 (36.8750) lr 1.0000e-03 eta 0:09:27\n",
            "epoch [27/50] batch [15/50] time 0.359 (0.437) data 0.001 (0.070) loss 3.4844 (2.8449) acc 28.1250 (38.1250) lr 1.0000e-03 eta 0:08:37\n",
            "epoch [27/50] batch [20/50] time 0.359 (0.417) data 0.001 (0.053) loss 2.7676 (2.8229) acc 40.6250 (38.9062) lr 1.0000e-03 eta 0:08:12\n",
            "epoch [27/50] batch [25/50] time 0.364 (0.406) data 0.009 (0.043) loss 2.9883 (2.8266) acc 34.3750 (38.1250) lr 1.0000e-03 eta 0:07:57\n",
            "epoch [27/50] batch [30/50] time 0.359 (0.399) data 0.003 (0.037) loss 2.8262 (2.8015) acc 34.3750 (38.0208) lr 1.0000e-03 eta 0:07:46\n",
            "epoch [27/50] batch [35/50] time 0.354 (0.394) data 0.001 (0.033) loss 2.9121 (2.7980) acc 43.7500 (37.6786) lr 1.0000e-03 eta 0:07:39\n",
            "epoch [27/50] batch [40/50] time 0.356 (0.389) data 0.001 (0.029) loss 3.1387 (2.8410) acc 31.2500 (36.9531) lr 1.0000e-03 eta 0:07:31\n",
            "epoch [27/50] batch [45/50] time 0.356 (0.386) data 0.000 (0.026) loss 2.7207 (2.8536) acc 37.5000 (36.5972) lr 1.0000e-03 eta 0:07:25\n",
            "epoch [27/50] batch [50/50] time 0.352 (0.382) data 0.000 (0.023) loss 2.8730 (2.8469) acc 37.5000 (37.0625) lr 9.3721e-04 eta 0:07:19\n",
            "epoch [28/50] batch [5/50] time 0.352 (0.614) data 0.000 (0.244) loss 2.8223 (2.7914) acc 34.3750 (36.2500) lr 9.3721e-04 eta 0:11:43\n",
            "epoch [28/50] batch [10/50] time 0.355 (0.485) data 0.001 (0.123) loss 3.2520 (2.8762) acc 25.0000 (34.6875) lr 9.3721e-04 eta 0:09:13\n",
            "epoch [28/50] batch [15/50] time 0.355 (0.442) data 0.001 (0.082) loss 3.2090 (2.9057) acc 31.2500 (33.3333) lr 9.3721e-04 eta 0:08:21\n",
            "epoch [28/50] batch [20/50] time 0.369 (0.422) data 0.007 (0.062) loss 2.2852 (2.8514) acc 34.3750 (33.7500) lr 9.3721e-04 eta 0:07:56\n",
            "epoch [28/50] batch [25/50] time 0.364 (0.410) data 0.007 (0.051) loss 3.2285 (2.8710) acc 31.2500 (33.6250) lr 9.3721e-04 eta 0:07:41\n",
            "epoch [28/50] batch [30/50] time 0.368 (0.402) data 0.001 (0.043) loss 2.3848 (2.8435) acc 43.7500 (35.1042) lr 9.3721e-04 eta 0:07:30\n",
            "epoch [28/50] batch [35/50] time 0.358 (0.396) data 0.001 (0.037) loss 2.5371 (2.8123) acc 43.7500 (35.9821) lr 9.3721e-04 eta 0:07:21\n",
            "epoch [28/50] batch [40/50] time 0.356 (0.391) data 0.000 (0.032) loss 2.9668 (2.8327) acc 34.3750 (35.5469) lr 9.3721e-04 eta 0:07:13\n",
            "epoch [28/50] batch [45/50] time 0.355 (0.387) data 0.000 (0.029) loss 3.2285 (2.8509) acc 25.0000 (35.1389) lr 9.3721e-04 eta 0:07:07\n",
            "epoch [28/50] batch [50/50] time 0.355 (0.384) data 0.000 (0.026) loss 3.1895 (2.8678) acc 25.0000 (34.8750) lr 8.7467e-04 eta 0:07:02\n",
            "epoch [29/50] batch [5/50] time 0.353 (0.597) data 0.001 (0.201) loss 3.0254 (2.6621) acc 31.2500 (41.2500) lr 8.7467e-04 eta 0:10:53\n",
            "epoch [29/50] batch [10/50] time 0.356 (0.477) data 0.001 (0.101) loss 3.2207 (2.7574) acc 21.8750 (36.5625) lr 8.7467e-04 eta 0:08:39\n",
            "epoch [29/50] batch [15/50] time 0.367 (0.439) data 0.010 (0.069) loss 2.7754 (2.7734) acc 34.3750 (35.0000) lr 8.7467e-04 eta 0:07:56\n",
            "epoch [29/50] batch [20/50] time 0.364 (0.420) data 0.009 (0.054) loss 2.1562 (2.7975) acc 50.0000 (35.4688) lr 8.7467e-04 eta 0:07:33\n",
            "epoch [29/50] batch [25/50] time 0.356 (0.408) data 0.001 (0.044) loss 3.2891 (2.8271) acc 25.0000 (35.5000) lr 8.7467e-04 eta 0:07:18\n",
            "epoch [29/50] batch [30/50] time 0.368 (0.401) data 0.010 (0.038) loss 2.5020 (2.8071) acc 43.7500 (35.8333) lr 8.7467e-04 eta 0:07:08\n",
            "epoch [29/50] batch [35/50] time 0.354 (0.395) data 0.001 (0.033) loss 3.0977 (2.8284) acc 34.3750 (35.8036) lr 8.7467e-04 eta 0:07:00\n",
            "epoch [29/50] batch [40/50] time 0.354 (0.390) data 0.000 (0.029) loss 2.9102 (2.8337) acc 37.5000 (35.4688) lr 8.7467e-04 eta 0:06:53\n",
            "epoch [29/50] batch [45/50] time 0.356 (0.386) data 0.000 (0.026) loss 2.9434 (2.8217) acc 31.2500 (35.4167) lr 8.7467e-04 eta 0:06:47\n",
            "epoch [29/50] batch [50/50] time 0.357 (0.383) data 0.000 (0.023) loss 2.4922 (2.8333) acc 46.8750 (35.3750) lr 8.1262e-04 eta 0:06:42\n",
            "epoch [30/50] batch [5/50] time 0.353 (0.616) data 0.000 (0.244) loss 3.0293 (2.8141) acc 34.3750 (36.2500) lr 8.1262e-04 eta 0:10:43\n",
            "epoch [30/50] batch [10/50] time 0.354 (0.486) data 0.001 (0.122) loss 2.7402 (2.7563) acc 40.6250 (38.7500) lr 8.1262e-04 eta 0:08:25\n",
            "epoch [30/50] batch [15/50] time 0.364 (0.445) data 0.005 (0.084) loss 2.1484 (2.7323) acc 50.0000 (38.7500) lr 8.1262e-04 eta 0:07:40\n",
            "epoch [30/50] batch [20/50] time 0.360 (0.424) data 0.006 (0.064) loss 2.9805 (2.7913) acc 40.6250 (37.5000) lr 8.1262e-04 eta 0:07:16\n",
            "epoch [30/50] batch [25/50] time 0.367 (0.412) data 0.013 (0.053) loss 2.8398 (2.8066) acc 31.2500 (36.6250) lr 8.1262e-04 eta 0:07:02\n",
            "epoch [30/50] batch [30/50] time 0.358 (0.404) data 0.001 (0.045) loss 3.2285 (2.8164) acc 31.2500 (36.8750) lr 8.1262e-04 eta 0:06:52\n",
            "epoch [30/50] batch [35/50] time 0.354 (0.397) data 0.001 (0.039) loss 3.4062 (2.8357) acc 25.0000 (35.9821) lr 8.1262e-04 eta 0:06:43\n",
            "epoch [30/50] batch [40/50] time 0.357 (0.392) data 0.000 (0.034) loss 2.4824 (2.8212) acc 50.0000 (36.1719) lr 8.1262e-04 eta 0:06:35\n",
            "epoch [30/50] batch [45/50] time 0.356 (0.388) data 0.000 (0.030) loss 2.8008 (2.8252) acc 37.5000 (36.2500) lr 8.1262e-04 eta 0:06:29\n",
            "epoch [30/50] batch [50/50] time 0.355 (0.385) data 0.001 (0.027) loss 3.3164 (2.8609) acc 25.0000 (35.3125) lr 7.5131e-04 eta 0:06:24\n",
            "epoch [31/50] batch [5/50] time 0.362 (0.573) data 0.011 (0.190) loss 2.5176 (2.4994) acc 40.6250 (38.7500) lr 7.5131e-04 eta 0:09:30\n",
            "epoch [31/50] batch [10/50] time 0.359 (0.468) data 0.001 (0.097) loss 3.4688 (2.7374) acc 21.8750 (35.0000) lr 7.5131e-04 eta 0:07:43\n",
            "epoch [31/50] batch [15/50] time 0.362 (0.433) data 0.000 (0.066) loss 3.1816 (2.8965) acc 31.2500 (32.9167) lr 7.5131e-04 eta 0:07:06\n",
            "epoch [31/50] batch [20/50] time 0.355 (0.414) data 0.001 (0.050) loss 2.9336 (2.8913) acc 40.6250 (34.8438) lr 7.5131e-04 eta 0:06:46\n",
            "epoch [31/50] batch [25/50] time 0.356 (0.403) data 0.001 (0.041) loss 2.6484 (2.8994) acc 34.3750 (34.5000) lr 7.5131e-04 eta 0:06:33\n",
            "epoch [31/50] batch [30/50] time 0.354 (0.395) data 0.001 (0.034) loss 2.8555 (2.8989) acc 25.0000 (34.8958) lr 7.5131e-04 eta 0:06:23\n",
            "epoch [31/50] batch [35/50] time 0.354 (0.390) data 0.001 (0.029) loss 3.1035 (2.9389) acc 34.3750 (34.0179) lr 7.5131e-04 eta 0:06:16\n",
            "epoch [31/50] batch [40/50] time 0.356 (0.385) data 0.000 (0.026) loss 2.9609 (2.9427) acc 37.5000 (33.9844) lr 7.5131e-04 eta 0:06:10\n",
            "epoch [31/50] batch [45/50] time 0.354 (0.382) data 0.000 (0.023) loss 2.6074 (2.9223) acc 40.6250 (33.9583) lr 7.5131e-04 eta 0:06:04\n",
            "epoch [31/50] batch [50/50] time 0.357 (0.380) data 0.000 (0.021) loss 2.8242 (2.8934) acc 40.6250 (34.8125) lr 6.9098e-04 eta 0:06:00\n",
            "epoch [32/50] batch [5/50] time 0.366 (0.816) data 0.010 (0.384) loss 2.8242 (2.7680) acc 34.3750 (33.7500) lr 6.9098e-04 eta 0:12:50\n",
            "epoch [32/50] batch [10/50] time 0.361 (0.587) data 0.001 (0.193) loss 2.5879 (2.8486) acc 43.7500 (33.7500) lr 6.9098e-04 eta 0:09:12\n",
            "epoch [32/50] batch [15/50] time 0.366 (0.513) data 0.009 (0.131) loss 2.5098 (2.8621) acc 40.6250 (33.9583) lr 6.9098e-04 eta 0:07:59\n",
            "epoch [32/50] batch [20/50] time 0.358 (0.474) data 0.001 (0.099) loss 2.9316 (2.8429) acc 34.3750 (35.3125) lr 6.9098e-04 eta 0:07:21\n",
            "epoch [32/50] batch [25/50] time 0.361 (0.451) data 0.001 (0.079) loss 3.4258 (2.8763) acc 31.2500 (34.0000) lr 6.9098e-04 eta 0:06:56\n",
            "epoch [32/50] batch [30/50] time 0.375 (0.436) data 0.007 (0.066) loss 2.4180 (2.8793) acc 50.0000 (35.5208) lr 6.9098e-04 eta 0:06:41\n",
            "epoch [32/50] batch [35/50] time 0.355 (0.425) data 0.001 (0.057) loss 2.7949 (2.8756) acc 50.0000 (36.2500) lr 6.9098e-04 eta 0:06:28\n",
            "epoch [32/50] batch [40/50] time 0.356 (0.416) data 0.000 (0.050) loss 3.0664 (2.8635) acc 31.2500 (36.1719) lr 6.9098e-04 eta 0:06:18\n",
            "epoch [32/50] batch [45/50] time 0.357 (0.409) data 0.000 (0.045) loss 2.7324 (2.8819) acc 31.2500 (35.6250) lr 6.9098e-04 eta 0:06:10\n",
            "epoch [32/50] batch [50/50] time 0.358 (0.404) data 0.001 (0.040) loss 2.9395 (2.8768) acc 37.5000 (35.5625) lr 6.3188e-04 eta 0:06:03\n",
            "epoch [33/50] batch [5/50] time 0.373 (0.926) data 0.015 (0.535) loss 2.9980 (2.5930) acc 28.1250 (40.6250) lr 6.3188e-04 eta 0:13:49\n",
            "epoch [33/50] batch [10/50] time 0.358 (0.642) data 0.001 (0.269) loss 3.2676 (2.7365) acc 18.7500 (35.9375) lr 6.3188e-04 eta 0:09:31\n",
            "epoch [33/50] batch [15/50] time 0.353 (0.547) data 0.000 (0.179) loss 3.2930 (2.8078) acc 28.1250 (35.0000) lr 6.3188e-04 eta 0:08:03\n",
            "epoch [33/50] batch [20/50] time 0.357 (0.499) data 0.001 (0.135) loss 2.7070 (2.7802) acc 34.3750 (35.4688) lr 6.3188e-04 eta 0:07:19\n",
            "epoch [33/50] batch [25/50] time 0.355 (0.470) data 0.001 (0.108) loss 2.9824 (2.8087) acc 37.5000 (36.0000) lr 6.3188e-04 eta 0:06:51\n",
            "epoch [33/50] batch [30/50] time 0.356 (0.451) data 0.001 (0.090) loss 2.0449 (2.7544) acc 53.1250 (36.5625) lr 6.3188e-04 eta 0:06:32\n",
            "epoch [33/50] batch [35/50] time 0.356 (0.438) data 0.001 (0.077) loss 2.9297 (2.7921) acc 28.1250 (36.0714) lr 6.3188e-04 eta 0:06:18\n",
            "epoch [33/50] batch [40/50] time 0.355 (0.428) data 0.001 (0.068) loss 3.0742 (2.8004) acc 31.2500 (36.3281) lr 6.3188e-04 eta 0:06:07\n",
            "epoch [33/50] batch [45/50] time 0.358 (0.420) data 0.001 (0.060) loss 3.4023 (2.8268) acc 28.1250 (35.6250) lr 6.3188e-04 eta 0:05:58\n",
            "epoch [33/50] batch [50/50] time 0.356 (0.413) data 0.001 (0.054) loss 2.7090 (2.8309) acc 31.2500 (35.3750) lr 5.7422e-04 eta 0:05:51\n",
            "epoch [34/50] batch [5/50] time 0.360 (0.581) data 0.009 (0.182) loss 3.6074 (3.1023) acc 18.7500 (30.6250) lr 5.7422e-04 eta 0:08:10\n",
            "epoch [34/50] batch [10/50] time 0.359 (0.472) data 0.001 (0.093) loss 2.5566 (2.8168) acc 40.6250 (36.2500) lr 5.7422e-04 eta 0:06:36\n",
            "epoch [34/50] batch [15/50] time 0.356 (0.433) data 0.001 (0.062) loss 2.5098 (2.8337) acc 40.6250 (35.8333) lr 5.7422e-04 eta 0:06:01\n",
            "epoch [34/50] batch [20/50] time 0.355 (0.414) data 0.001 (0.047) loss 2.1543 (2.7468) acc 59.3750 (37.5000) lr 5.7422e-04 eta 0:05:43\n",
            "epoch [34/50] batch [25/50] time 0.355 (0.402) data 0.001 (0.037) loss 2.2715 (2.7463) acc 53.1250 (38.3750) lr 5.7422e-04 eta 0:05:31\n",
            "epoch [34/50] batch [30/50] time 0.366 (0.396) data 0.008 (0.032) loss 2.8457 (2.7888) acc 28.1250 (37.0833) lr 5.7422e-04 eta 0:05:24\n",
            "epoch [34/50] batch [35/50] time 0.358 (0.391) data 0.001 (0.028) loss 2.7949 (2.7790) acc 34.3750 (37.1429) lr 5.7422e-04 eta 0:05:18\n",
            "epoch [34/50] batch [40/50] time 0.355 (0.387) data 0.001 (0.024) loss 2.7910 (2.7698) acc 34.3750 (37.7344) lr 5.7422e-04 eta 0:05:13\n",
            "epoch [34/50] batch [45/50] time 0.354 (0.383) data 0.001 (0.022) loss 2.6602 (2.7744) acc 34.3750 (37.7778) lr 5.7422e-04 eta 0:05:08\n",
            "epoch [34/50] batch [50/50] time 0.357 (0.381) data 0.001 (0.020) loss 2.7656 (2.7842) acc 37.5000 (37.4375) lr 5.1825e-04 eta 0:05:04\n",
            "epoch [35/50] batch [5/50] time 0.352 (0.555) data 0.001 (0.170) loss 2.4746 (2.7590) acc 34.3750 (35.0000) lr 5.1825e-04 eta 0:07:21\n",
            "epoch [35/50] batch [10/50] time 0.361 (0.456) data 0.001 (0.085) loss 3.2012 (2.8270) acc 34.3750 (34.6875) lr 5.1825e-04 eta 0:06:00\n",
            "epoch [35/50] batch [15/50] time 0.356 (0.422) data 0.001 (0.057) loss 3.1875 (2.8392) acc 25.0000 (35.4167) lr 5.1825e-04 eta 0:05:31\n",
            "epoch [35/50] batch [20/50] time 0.358 (0.406) data 0.001 (0.043) loss 2.9453 (2.8229) acc 34.3750 (35.7812) lr 5.1825e-04 eta 0:05:16\n",
            "epoch [35/50] batch [25/50] time 0.360 (0.396) data 0.001 (0.035) loss 2.3809 (2.8218) acc 43.7500 (35.6250) lr 5.1825e-04 eta 0:05:06\n",
            "epoch [35/50] batch [30/50] time 0.361 (0.390) data 0.005 (0.029) loss 2.9766 (2.8761) acc 31.2500 (34.8958) lr 5.1825e-04 eta 0:05:00\n",
            "epoch [35/50] batch [35/50] time 0.358 (0.385) data 0.001 (0.025) loss 3.8203 (2.8818) acc 18.7500 (34.4643) lr 5.1825e-04 eta 0:04:54\n",
            "epoch [35/50] batch [40/50] time 0.356 (0.382) data 0.001 (0.022) loss 3.0449 (2.8641) acc 18.7500 (34.3750) lr 5.1825e-04 eta 0:04:50\n",
            "epoch [35/50] batch [45/50] time 0.355 (0.379) data 0.001 (0.020) loss 3.2285 (2.8520) acc 15.6250 (34.6528) lr 5.1825e-04 eta 0:04:46\n",
            "epoch [35/50] batch [50/50] time 0.359 (0.377) data 0.001 (0.018) loss 3.2988 (2.8661) acc 21.8750 (34.3125) lr 4.6417e-04 eta 0:04:42\n",
            "epoch [36/50] batch [5/50] time 0.351 (0.564) data 0.001 (0.167) loss 3.0117 (2.8977) acc 28.1250 (34.3750) lr 4.6417e-04 eta 0:07:00\n",
            "epoch [36/50] batch [10/50] time 0.355 (0.460) data 0.001 (0.084) loss 3.0059 (2.7961) acc 37.5000 (35.6250) lr 4.6417e-04 eta 0:05:40\n",
            "epoch [36/50] batch [15/50] time 0.357 (0.425) data 0.001 (0.056) loss 2.8828 (2.7477) acc 34.3750 (36.2500) lr 4.6417e-04 eta 0:05:12\n",
            "epoch [36/50] batch [20/50] time 0.355 (0.408) data 0.001 (0.043) loss 2.4434 (2.7160) acc 40.6250 (37.6562) lr 4.6417e-04 eta 0:04:57\n",
            "epoch [36/50] batch [25/50] time 0.356 (0.397) data 0.001 (0.034) loss 2.8984 (2.7255) acc 34.3750 (37.8750) lr 4.6417e-04 eta 0:04:47\n",
            "epoch [36/50] batch [30/50] time 0.361 (0.391) data 0.001 (0.029) loss 2.8047 (2.7765) acc 34.3750 (36.7708) lr 4.6417e-04 eta 0:04:41\n",
            "epoch [36/50] batch [35/50] time 0.358 (0.387) data 0.001 (0.025) loss 3.2734 (2.7996) acc 25.0000 (36.5179) lr 4.6417e-04 eta 0:04:36\n",
            "epoch [36/50] batch [40/50] time 0.355 (0.383) data 0.001 (0.022) loss 2.6113 (2.7992) acc 43.7500 (36.7188) lr 4.6417e-04 eta 0:04:31\n",
            "epoch [36/50] batch [45/50] time 0.355 (0.380) data 0.001 (0.020) loss 2.5508 (2.8209) acc 46.8750 (36.5972) lr 4.6417e-04 eta 0:04:27\n",
            "epoch [36/50] batch [50/50] time 0.355 (0.377) data 0.000 (0.018) loss 3.2715 (2.8177) acc 31.2500 (36.3750) lr 4.1221e-04 eta 0:04:24\n",
            "epoch [37/50] batch [5/50] time 0.352 (0.589) data 0.001 (0.205) loss 2.8066 (2.8926) acc 40.6250 (35.6250) lr 4.1221e-04 eta 0:06:49\n",
            "epoch [37/50] batch [10/50] time 0.356 (0.472) data 0.001 (0.103) loss 2.8418 (2.9311) acc 28.1250 (33.7500) lr 4.1221e-04 eta 0:05:25\n",
            "epoch [37/50] batch [15/50] time 0.355 (0.433) data 0.001 (0.069) loss 3.3789 (2.9026) acc 28.1250 (35.2083) lr 4.1221e-04 eta 0:04:56\n",
            "epoch [37/50] batch [20/50] time 0.353 (0.413) data 0.001 (0.052) loss 3.2109 (2.9078) acc 25.0000 (34.2188) lr 4.1221e-04 eta 0:04:41\n",
            "epoch [37/50] batch [25/50] time 0.361 (0.403) data 0.007 (0.042) loss 2.4492 (2.8712) acc 34.3750 (33.6250) lr 4.1221e-04 eta 0:04:31\n",
            "epoch [37/50] batch [30/50] time 0.374 (0.396) data 0.016 (0.036) loss 3.1719 (2.8491) acc 34.3750 (34.6875) lr 4.1221e-04 eta 0:04:25\n",
            "epoch [37/50] batch [35/50] time 0.360 (0.391) data 0.005 (0.031) loss 2.5645 (2.8402) acc 40.6250 (34.6429) lr 4.1221e-04 eta 0:04:19\n",
            "epoch [37/50] batch [40/50] time 0.355 (0.387) data 0.001 (0.027) loss 2.5898 (2.8370) acc 43.7500 (35.0000) lr 4.1221e-04 eta 0:04:15\n",
            "epoch [37/50] batch [45/50] time 0.355 (0.383) data 0.001 (0.025) loss 2.9727 (2.8123) acc 37.5000 (35.7639) lr 4.1221e-04 eta 0:04:10\n",
            "epoch [37/50] batch [50/50] time 0.358 (0.381) data 0.001 (0.022) loss 3.2109 (2.8117) acc 28.1250 (35.6875) lr 3.6258e-04 eta 0:04:07\n",
            "epoch [38/50] batch [5/50] time 0.352 (0.596) data 0.001 (0.226) loss 2.9766 (3.1797) acc 25.0000 (33.1250) lr 3.6258e-04 eta 0:06:24\n",
            "epoch [38/50] batch [10/50] time 0.356 (0.476) data 0.001 (0.114) loss 2.8848 (2.9059) acc 25.0000 (37.8125) lr 3.6258e-04 eta 0:05:04\n",
            "epoch [38/50] batch [15/50] time 0.354 (0.436) data 0.001 (0.076) loss 2.5508 (2.7604) acc 31.2500 (38.1250) lr 3.6258e-04 eta 0:04:36\n",
            "epoch [38/50] batch [20/50] time 0.356 (0.415) data 0.001 (0.057) loss 3.1758 (2.7843) acc 25.0000 (37.6562) lr 3.6258e-04 eta 0:04:21\n",
            "epoch [38/50] batch [25/50] time 0.361 (0.404) data 0.002 (0.046) loss 2.4805 (2.7816) acc 40.6250 (37.6250) lr 3.6258e-04 eta 0:04:12\n",
            "epoch [38/50] batch [30/50] time 0.359 (0.397) data 0.002 (0.040) loss 2.2832 (2.7831) acc 40.6250 (37.1875) lr 3.6258e-04 eta 0:04:06\n",
            "epoch [38/50] batch [35/50] time 0.358 (0.392) data 0.001 (0.034) loss 2.2598 (2.8044) acc 40.6250 (36.8750) lr 3.6258e-04 eta 0:04:00\n",
            "epoch [38/50] batch [40/50] time 0.355 (0.387) data 0.001 (0.030) loss 2.1875 (2.7753) acc 50.0000 (37.5781) lr 3.6258e-04 eta 0:03:56\n",
            "epoch [38/50] batch [45/50] time 0.357 (0.384) data 0.001 (0.027) loss 2.5762 (2.7909) acc 40.6250 (37.2222) lr 3.6258e-04 eta 0:03:52\n",
            "epoch [38/50] batch [50/50] time 0.356 (0.381) data 0.000 (0.024) loss 2.4297 (2.8061) acc 34.3750 (36.6250) lr 3.1545e-04 eta 0:03:48\n",
            "epoch [39/50] batch [5/50] time 0.353 (0.636) data 0.001 (0.258) loss 2.4238 (2.8398) acc 46.8750 (37.5000) lr 3.1545e-04 eta 0:06:18\n",
            "epoch [39/50] batch [10/50] time 0.356 (0.496) data 0.001 (0.129) loss 2.7090 (2.7025) acc 37.5000 (38.4375) lr 3.1545e-04 eta 0:04:52\n",
            "epoch [39/50] batch [15/50] time 0.359 (0.449) data 0.001 (0.086) loss 3.3809 (2.8142) acc 31.2500 (35.8333) lr 3.1545e-04 eta 0:04:22\n",
            "epoch [39/50] batch [20/50] time 0.354 (0.426) data 0.001 (0.065) loss 2.5059 (2.8172) acc 31.2500 (35.6250) lr 3.1545e-04 eta 0:04:07\n",
            "epoch [39/50] batch [25/50] time 0.363 (0.413) data 0.009 (0.053) loss 3.0078 (2.8248) acc 31.2500 (35.5000) lr 3.1545e-04 eta 0:03:57\n",
            "epoch [39/50] batch [30/50] time 0.360 (0.405) data 0.001 (0.045) loss 2.4375 (2.7697) acc 53.1250 (36.9792) lr 3.1545e-04 eta 0:03:50\n",
            "epoch [39/50] batch [35/50] time 0.356 (0.399) data 0.001 (0.040) loss 3.3574 (2.7681) acc 28.1250 (37.1429) lr 3.1545e-04 eta 0:03:45\n",
            "epoch [39/50] batch [40/50] time 0.356 (0.394) data 0.001 (0.035) loss 2.7070 (2.7638) acc 40.6250 (36.9531) lr 3.1545e-04 eta 0:03:40\n",
            "epoch [39/50] batch [45/50] time 0.356 (0.390) data 0.000 (0.031) loss 3.1406 (2.7736) acc 28.1250 (36.5278) lr 3.1545e-04 eta 0:03:36\n",
            "epoch [39/50] batch [50/50] time 0.355 (0.386) data 0.000 (0.028) loss 3.1426 (2.7573) acc 31.2500 (36.6250) lr 2.7103e-04 eta 0:03:32\n",
            "epoch [40/50] batch [5/50] time 0.354 (0.539) data 0.001 (0.147) loss 2.8379 (2.7277) acc 34.3750 (38.7500) lr 2.7103e-04 eta 0:04:53\n",
            "epoch [40/50] batch [10/50] time 0.358 (0.447) data 0.001 (0.074) loss 3.4473 (2.7721) acc 18.7500 (37.5000) lr 2.7103e-04 eta 0:04:01\n",
            "epoch [40/50] batch [15/50] time 0.355 (0.417) data 0.001 (0.050) loss 1.8525 (2.6794) acc 46.8750 (36.4583) lr 2.7103e-04 eta 0:03:43\n",
            "epoch [40/50] batch [20/50] time 0.365 (0.403) data 0.010 (0.038) loss 2.5137 (2.6233) acc 43.7500 (38.9062) lr 2.7103e-04 eta 0:03:33\n",
            "epoch [40/50] batch [25/50] time 0.364 (0.394) data 0.006 (0.031) loss 2.4297 (2.6754) acc 50.0000 (39.1250) lr 2.7103e-04 eta 0:03:26\n",
            "epoch [40/50] batch [30/50] time 0.369 (0.389) data 0.008 (0.026) loss 2.8438 (2.6701) acc 40.6250 (39.6875) lr 2.7103e-04 eta 0:03:22\n",
            "epoch [40/50] batch [35/50] time 0.354 (0.386) data 0.001 (0.023) loss 2.8281 (2.6903) acc 40.6250 (39.1964) lr 2.7103e-04 eta 0:03:18\n",
            "epoch [40/50] batch [40/50] time 0.357 (0.382) data 0.001 (0.021) loss 3.4160 (2.6975) acc 25.0000 (38.8281) lr 2.7103e-04 eta 0:03:14\n",
            "epoch [40/50] batch [45/50] time 0.357 (0.379) data 0.000 (0.018) loss 3.1719 (2.7060) acc 37.5000 (38.7500) lr 2.7103e-04 eta 0:03:11\n",
            "epoch [40/50] batch [50/50] time 0.356 (0.377) data 0.000 (0.016) loss 2.7188 (2.7274) acc 50.0000 (38.2500) lr 2.2949e-04 eta 0:03:08\n",
            "epoch [41/50] batch [5/50] time 0.351 (0.558) data 0.001 (0.187) loss 2.7793 (2.7879) acc 40.6250 (36.8750) lr 2.2949e-04 eta 0:04:36\n",
            "epoch [41/50] batch [10/50] time 0.354 (0.457) data 0.000 (0.094) loss 2.8379 (2.8937) acc 34.3750 (32.1875) lr 2.2949e-04 eta 0:03:43\n",
            "epoch [41/50] batch [15/50] time 0.360 (0.423) data 0.002 (0.063) loss 3.0762 (2.8819) acc 25.0000 (32.5000) lr 2.2949e-04 eta 0:03:25\n",
            "epoch [41/50] batch [20/50] time 0.365 (0.407) data 0.009 (0.048) loss 2.4883 (2.8000) acc 40.6250 (34.2188) lr 2.2949e-04 eta 0:03:15\n",
            "epoch [41/50] batch [25/50] time 0.365 (0.398) data 0.001 (0.039) loss 2.1895 (2.7827) acc 46.8750 (34.3750) lr 2.2949e-04 eta 0:03:09\n",
            "epoch [41/50] batch [30/50] time 0.356 (0.392) data 0.001 (0.033) loss 2.2715 (2.7480) acc 46.8750 (35.9375) lr 2.2949e-04 eta 0:03:04\n",
            "epoch [41/50] batch [35/50] time 0.356 (0.387) data 0.001 (0.029) loss 2.5195 (2.7523) acc 34.3750 (35.8036) lr 2.2949e-04 eta 0:03:00\n",
            "epoch [41/50] batch [40/50] time 0.355 (0.384) data 0.000 (0.026) loss 2.1914 (2.7467) acc 46.8750 (35.7031) lr 2.2949e-04 eta 0:02:56\n",
            "epoch [41/50] batch [45/50] time 0.354 (0.380) data 0.000 (0.023) loss 2.9570 (2.7286) acc 37.5000 (36.4583) lr 2.2949e-04 eta 0:02:53\n",
            "epoch [41/50] batch [50/50] time 0.354 (0.378) data 0.000 (0.021) loss 2.9375 (2.7348) acc 37.5000 (36.6875) lr 1.9098e-04 eta 0:02:49\n",
            "epoch [42/50] batch [5/50] time 0.351 (0.579) data 0.000 (0.194) loss 2.0781 (2.5652) acc 50.0000 (40.0000) lr 1.9098e-04 eta 0:04:17\n",
            "epoch [42/50] batch [10/50] time 0.357 (0.467) data 0.001 (0.097) loss 3.1328 (2.6869) acc 25.0000 (37.5000) lr 1.9098e-04 eta 0:03:25\n",
            "epoch [42/50] batch [15/50] time 0.362 (0.432) data 0.001 (0.066) loss 2.1230 (2.6996) acc 53.1250 (37.7083) lr 1.9098e-04 eta 0:03:07\n",
            "epoch [42/50] batch [20/50] time 0.355 (0.413) data 0.001 (0.050) loss 2.5410 (2.7377) acc 37.5000 (37.3438) lr 1.9098e-04 eta 0:02:57\n",
            "epoch [42/50] batch [25/50] time 0.358 (0.403) data 0.001 (0.040) loss 3.3398 (2.7532) acc 21.8750 (36.8750) lr 1.9098e-04 eta 0:02:51\n",
            "epoch [42/50] batch [30/50] time 0.360 (0.396) data 0.002 (0.034) loss 2.4961 (2.7558) acc 37.5000 (37.0833) lr 1.9098e-04 eta 0:02:46\n",
            "epoch [42/50] batch [35/50] time 0.354 (0.392) data 0.001 (0.030) loss 2.8516 (2.7301) acc 43.7500 (37.0536) lr 1.9098e-04 eta 0:02:42\n",
            "epoch [42/50] batch [40/50] time 0.354 (0.387) data 0.000 (0.027) loss 3.1797 (2.7328) acc 34.3750 (37.3438) lr 1.9098e-04 eta 0:02:38\n",
            "epoch [42/50] batch [45/50] time 0.355 (0.383) data 0.000 (0.024) loss 2.8418 (2.7177) acc 34.3750 (37.3611) lr 1.9098e-04 eta 0:02:35\n",
            "epoch [42/50] batch [50/50] time 0.355 (0.380) data 0.000 (0.021) loss 2.9941 (2.7195) acc 31.2500 (37.3125) lr 1.5567e-04 eta 0:02:32\n",
            "epoch [43/50] batch [5/50] time 0.354 (0.512) data 0.000 (0.131) loss 2.6562 (2.8742) acc 43.7500 (33.7500) lr 1.5567e-04 eta 0:03:22\n",
            "epoch [43/50] batch [10/50] time 0.357 (0.434) data 0.001 (0.066) loss 2.2891 (2.7582) acc 37.5000 (35.3125) lr 1.5567e-04 eta 0:02:49\n",
            "epoch [43/50] batch [15/50] time 0.355 (0.410) data 0.001 (0.046) loss 2.2910 (2.7240) acc 46.8750 (37.2917) lr 1.5567e-04 eta 0:02:37\n",
            "epoch [43/50] batch [20/50] time 0.361 (0.397) data 0.007 (0.035) loss 2.7461 (2.7119) acc 31.2500 (36.4062) lr 1.5567e-04 eta 0:02:30\n",
            "epoch [43/50] batch [25/50] time 0.364 (0.390) data 0.007 (0.029) loss 3.6582 (2.7287) acc 18.7500 (36.5000) lr 1.5567e-04 eta 0:02:26\n",
            "epoch [43/50] batch [30/50] time 0.360 (0.385) data 0.005 (0.025) loss 3.4648 (2.7754) acc 21.8750 (35.8333) lr 1.5567e-04 eta 0:02:22\n",
            "epoch [43/50] batch [35/50] time 0.356 (0.381) data 0.001 (0.022) loss 2.8516 (2.7661) acc 37.5000 (36.3393) lr 1.5567e-04 eta 0:02:19\n",
            "epoch [43/50] batch [40/50] time 0.354 (0.378) data 0.000 (0.019) loss 3.0625 (2.7659) acc 28.1250 (36.0156) lr 1.5567e-04 eta 0:02:16\n",
            "epoch [43/50] batch [45/50] time 0.356 (0.375) data 0.000 (0.017) loss 3.0977 (2.7753) acc 28.1250 (35.5556) lr 1.5567e-04 eta 0:02:13\n",
            "epoch [43/50] batch [50/50] time 0.355 (0.373) data 0.000 (0.016) loss 3.0723 (2.7759) acc 37.5000 (36.0000) lr 1.2369e-04 eta 0:02:10\n",
            "epoch [44/50] batch [5/50] time 0.350 (0.584) data 0.001 (0.213) loss 2.7480 (2.5879) acc 21.8750 (37.5000) lr 1.2369e-04 eta 0:03:21\n",
            "epoch [44/50] batch [10/50] time 0.361 (0.471) data 0.001 (0.107) loss 3.7832 (2.8775) acc 12.5000 (33.7500) lr 1.2369e-04 eta 0:02:40\n",
            "epoch [44/50] batch [15/50] time 0.358 (0.435) data 0.002 (0.074) loss 2.5781 (2.7289) acc 34.3750 (37.7083) lr 1.2369e-04 eta 0:02:25\n",
            "epoch [44/50] batch [20/50] time 0.365 (0.417) data 0.007 (0.057) loss 2.6816 (2.7933) acc 34.3750 (35.9375) lr 1.2369e-04 eta 0:02:17\n",
            "epoch [44/50] batch [25/50] time 0.361 (0.405) data 0.001 (0.046) loss 3.1621 (2.8135) acc 34.3750 (35.0000) lr 1.2369e-04 eta 0:02:11\n",
            "epoch [44/50] batch [30/50] time 0.364 (0.399) data 0.008 (0.040) loss 2.3691 (2.7568) acc 46.8750 (36.1458) lr 1.2369e-04 eta 0:02:07\n",
            "epoch [44/50] batch [35/50] time 0.357 (0.394) data 0.001 (0.035) loss 3.0957 (2.7343) acc 25.0000 (36.6964) lr 1.2369e-04 eta 0:02:04\n",
            "epoch [44/50] batch [40/50] time 0.353 (0.389) data 0.000 (0.031) loss 3.4004 (2.7503) acc 15.6250 (36.0938) lr 1.2369e-04 eta 0:02:00\n",
            "epoch [44/50] batch [45/50] time 0.355 (0.385) data 0.000 (0.028) loss 2.3242 (2.7303) acc 46.8750 (36.3889) lr 1.2369e-04 eta 0:01:57\n",
            "epoch [44/50] batch [50/50] time 0.353 (0.382) data 0.000 (0.025) loss 2.6172 (2.7061) acc 43.7500 (37.3125) lr 9.5173e-05 eta 0:01:54\n",
            "epoch [45/50] batch [5/50] time 0.348 (0.597) data 0.000 (0.227) loss 2.2930 (2.5223) acc 56.2500 (45.6250) lr 9.5173e-05 eta 0:02:56\n",
            "epoch [45/50] batch [10/50] time 0.363 (0.478) data 0.008 (0.115) loss 2.5020 (2.6412) acc 37.5000 (41.5625) lr 9.5173e-05 eta 0:02:18\n",
            "epoch [45/50] batch [15/50] time 0.368 (0.439) data 0.012 (0.079) loss 2.5977 (2.6384) acc 50.0000 (40.6250) lr 9.5173e-05 eta 0:02:05\n",
            "epoch [45/50] batch [20/50] time 0.360 (0.420) data 0.001 (0.060) loss 2.7344 (2.6835) acc 25.0000 (39.0625) lr 9.5173e-05 eta 0:01:57\n",
            "epoch [45/50] batch [25/50] time 0.361 (0.408) data 0.007 (0.049) loss 2.4727 (2.7143) acc 40.6250 (38.6250) lr 9.5173e-05 eta 0:01:52\n",
            "epoch [45/50] batch [30/50] time 0.355 (0.400) data 0.001 (0.041) loss 2.4629 (2.6840) acc 50.0000 (39.8958) lr 9.5173e-05 eta 0:01:48\n",
            "epoch [45/50] batch [35/50] time 0.356 (0.394) data 0.001 (0.035) loss 2.6562 (2.6975) acc 34.3750 (39.2857) lr 9.5173e-05 eta 0:01:44\n",
            "epoch [45/50] batch [40/50] time 0.355 (0.389) data 0.000 (0.031) loss 2.4805 (2.6864) acc 34.3750 (39.0625) lr 9.5173e-05 eta 0:01:41\n",
            "epoch [45/50] batch [45/50] time 0.355 (0.385) data 0.000 (0.028) loss 2.4668 (2.6881) acc 40.6250 (38.6806) lr 9.5173e-05 eta 0:01:38\n",
            "epoch [45/50] batch [50/50] time 0.355 (0.382) data 0.000 (0.025) loss 2.9648 (2.7021) acc 31.2500 (38.2500) lr 7.0224e-05 eta 0:01:35\n",
            "epoch [46/50] batch [5/50] time 0.379 (0.622) data 0.001 (0.153) loss 2.4395 (2.9594) acc 40.6250 (34.3750) lr 7.0224e-05 eta 0:02:32\n",
            "epoch [46/50] batch [10/50] time 0.366 (0.492) data 0.008 (0.079) loss 3.0234 (2.8861) acc 31.2500 (35.3125) lr 7.0224e-05 eta 0:01:57\n",
            "epoch [46/50] batch [15/50] time 0.359 (0.449) data 0.001 (0.054) loss 2.2910 (2.7555) acc 43.7500 (36.8750) lr 7.0224e-05 eta 0:01:45\n",
            "epoch [46/50] batch [20/50] time 0.361 (0.426) data 0.001 (0.041) loss 2.4941 (2.7288) acc 40.6250 (37.0312) lr 7.0224e-05 eta 0:01:38\n",
            "epoch [46/50] batch [25/50] time 0.362 (0.413) data 0.007 (0.034) loss 2.7793 (2.7385) acc 31.2500 (36.7500) lr 7.0224e-05 eta 0:01:32\n",
            "epoch [46/50] batch [30/50] time 0.357 (0.405) data 0.002 (0.029) loss 2.6992 (2.7154) acc 40.6250 (37.9167) lr 7.0224e-05 eta 0:01:29\n",
            "epoch [46/50] batch [35/50] time 0.357 (0.398) data 0.001 (0.025) loss 2.9531 (2.7291) acc 37.5000 (37.9464) lr 7.0224e-05 eta 0:01:25\n",
            "epoch [46/50] batch [40/50] time 0.354 (0.392) data 0.000 (0.022) loss 2.3516 (2.7056) acc 37.5000 (38.3594) lr 7.0224e-05 eta 0:01:22\n",
            "epoch [46/50] batch [45/50] time 0.356 (0.388) data 0.000 (0.020) loss 2.6621 (2.7215) acc 40.6250 (37.7083) lr 7.0224e-05 eta 0:01:19\n",
            "epoch [46/50] batch [50/50] time 0.354 (0.385) data 0.000 (0.018) loss 2.0254 (2.6629) acc 53.1250 (38.8750) lr 4.8943e-05 eta 0:01:16\n",
            "epoch [47/50] batch [5/50] time 0.361 (0.559) data 0.007 (0.169) loss 2.2109 (2.5799) acc 46.8750 (40.0000) lr 4.8943e-05 eta 0:01:49\n",
            "epoch [47/50] batch [10/50] time 0.360 (0.462) data 0.002 (0.086) loss 2.9961 (2.7423) acc 28.1250 (34.3750) lr 4.8943e-05 eta 0:01:27\n",
            "epoch [47/50] batch [15/50] time 0.359 (0.429) data 0.001 (0.059) loss 2.8418 (2.7531) acc 37.5000 (35.4167) lr 4.8943e-05 eta 0:01:19\n",
            "epoch [47/50] batch [20/50] time 0.362 (0.412) data 0.006 (0.045) loss 2.5547 (2.7114) acc 34.3750 (36.7188) lr 4.8943e-05 eta 0:01:14\n",
            "epoch [47/50] batch [25/50] time 0.366 (0.402) data 0.008 (0.037) loss 2.4023 (2.6632) acc 53.1250 (37.7500) lr 4.8943e-05 eta 0:01:10\n",
            "epoch [47/50] batch [30/50] time 0.356 (0.395) data 0.000 (0.031) loss 2.1602 (2.6968) acc 56.2500 (38.1250) lr 4.8943e-05 eta 0:01:07\n",
            "epoch [47/50] batch [35/50] time 0.355 (0.389) data 0.001 (0.027) loss 2.8262 (2.7280) acc 50.0000 (37.5000) lr 4.8943e-05 eta 0:01:04\n",
            "epoch [47/50] batch [40/50] time 0.353 (0.385) data 0.000 (0.024) loss 3.1152 (2.7355) acc 31.2500 (37.2656) lr 4.8943e-05 eta 0:01:01\n",
            "epoch [47/50] batch [45/50] time 0.355 (0.381) data 0.000 (0.021) loss 2.1816 (2.7123) acc 50.0000 (37.9167) lr 4.8943e-05 eta 0:00:59\n",
            "epoch [47/50] batch [50/50] time 0.356 (0.379) data 0.000 (0.019) loss 3.2109 (2.7307) acc 28.1250 (37.7500) lr 3.1417e-05 eta 0:00:56\n",
            "epoch [48/50] batch [5/50] time 0.370 (0.606) data 0.008 (0.159) loss 3.5234 (2.7613) acc 18.7500 (38.1250) lr 3.1417e-05 eta 0:01:27\n",
            "epoch [48/50] batch [10/50] time 0.360 (0.485) data 0.007 (0.083) loss 2.6523 (2.7230) acc 40.6250 (38.1250) lr 3.1417e-05 eta 0:01:07\n",
            "epoch [48/50] batch [15/50] time 0.364 (0.445) data 0.007 (0.058) loss 2.7441 (2.7212) acc 34.3750 (37.9167) lr 3.1417e-05 eta 0:01:00\n",
            "epoch [48/50] batch [20/50] time 0.364 (0.425) data 0.007 (0.046) loss 3.2148 (2.7094) acc 34.3750 (39.5312) lr 3.1417e-05 eta 0:00:55\n",
            "epoch [48/50] batch [25/50] time 0.356 (0.413) data 0.000 (0.038) loss 2.6191 (2.6935) acc 40.6250 (39.3750) lr 3.1417e-05 eta 0:00:51\n",
            "epoch [48/50] batch [30/50] time 0.355 (0.404) data 0.001 (0.032) loss 2.3398 (2.6376) acc 43.7500 (40.0000) lr 3.1417e-05 eta 0:00:48\n",
            "epoch [48/50] batch [35/50] time 0.355 (0.397) data 0.001 (0.027) loss 2.4531 (2.6258) acc 34.3750 (39.9107) lr 3.1417e-05 eta 0:00:45\n",
            "epoch [48/50] batch [40/50] time 0.353 (0.392) data 0.000 (0.024) loss 3.4844 (2.6544) acc 25.0000 (39.4531) lr 3.1417e-05 eta 0:00:43\n",
            "epoch [48/50] batch [45/50] time 0.355 (0.387) data 0.000 (0.021) loss 2.4766 (2.6318) acc 40.6250 (39.7917) lr 3.1417e-05 eta 0:00:40\n",
            "epoch [48/50] batch [50/50] time 0.354 (0.384) data 0.000 (0.019) loss 3.0508 (2.6571) acc 31.2500 (39.6250) lr 1.7713e-05 eta 0:00:38\n",
            "epoch [49/50] batch [5/50] time 0.368 (0.675) data 0.004 (0.232) loss 2.3887 (2.6258) acc 28.1250 (36.8750) lr 1.7713e-05 eta 0:01:04\n",
            "epoch [49/50] batch [10/50] time 0.365 (0.520) data 0.009 (0.120) loss 2.6152 (2.6248) acc 43.7500 (39.0625) lr 1.7713e-05 eta 0:00:46\n",
            "epoch [49/50] batch [15/50] time 0.366 (0.468) data 0.007 (0.082) loss 2.4219 (2.6151) acc 40.6250 (39.5833) lr 1.7713e-05 eta 0:00:39\n",
            "epoch [49/50] batch [20/50] time 0.358 (0.443) data 0.004 (0.064) loss 2.2109 (2.6044) acc 40.6250 (39.6875) lr 1.7713e-05 eta 0:00:35\n",
            "epoch [49/50] batch [25/50] time 0.364 (0.428) data 0.008 (0.053) loss 2.5996 (2.6386) acc 43.7500 (39.5000) lr 1.7713e-05 eta 0:00:32\n",
            "epoch [49/50] batch [30/50] time 0.356 (0.416) data 0.001 (0.045) loss 2.2422 (2.6668) acc 43.7500 (38.7500) lr 1.7713e-05 eta 0:00:29\n",
            "epoch [49/50] batch [35/50] time 0.354 (0.407) data 0.001 (0.038) loss 2.2891 (2.6765) acc 50.0000 (38.3929) lr 1.7713e-05 eta 0:00:26\n",
            "epoch [49/50] batch [40/50] time 0.354 (0.400) data 0.000 (0.033) loss 2.6484 (2.7051) acc 40.6250 (38.3594) lr 1.7713e-05 eta 0:00:24\n",
            "epoch [49/50] batch [45/50] time 0.355 (0.395) data 0.000 (0.030) loss 2.5820 (2.7182) acc 28.1250 (37.7778) lr 1.7713e-05 eta 0:00:21\n",
            "epoch [49/50] batch [50/50] time 0.354 (0.391) data 0.000 (0.027) loss 3.4980 (2.7328) acc 31.2500 (37.6250) lr 7.8853e-06 eta 0:00:19\n",
            "epoch [50/50] batch [5/50] time 0.380 (0.591) data 0.019 (0.165) loss 3.2383 (3.0664) acc 21.8750 (28.1250) lr 7.8853e-06 eta 0:00:26\n",
            "epoch [50/50] batch [10/50] time 0.371 (0.478) data 0.012 (0.087) loss 2.6211 (2.8846) acc 31.2500 (31.8750) lr 7.8853e-06 eta 0:00:19\n",
            "epoch [50/50] batch [15/50] time 0.358 (0.440) data 0.001 (0.060) loss 2.0176 (2.7997) acc 46.8750 (33.3333) lr 7.8853e-06 eta 0:00:15\n",
            "epoch [50/50] batch [20/50] time 0.357 (0.421) data 0.005 (0.046) loss 3.6777 (2.8068) acc 15.6250 (34.8438) lr 7.8853e-06 eta 0:00:12\n",
            "epoch [50/50] batch [25/50] time 0.358 (0.408) data 0.000 (0.037) loss 2.8672 (2.8154) acc 31.2500 (35.0000) lr 7.8853e-06 eta 0:00:10\n",
            "epoch [50/50] batch [30/50] time 0.355 (0.399) data 0.001 (0.031) loss 2.6133 (2.7736) acc 40.6250 (36.4583) lr 7.8853e-06 eta 0:00:07\n",
            "epoch [50/50] batch [35/50] time 0.356 (0.393) data 0.001 (0.027) loss 2.5449 (2.7385) acc 53.1250 (36.7857) lr 7.8853e-06 eta 0:00:05\n",
            "epoch [50/50] batch [40/50] time 0.357 (0.388) data 0.000 (0.023) loss 2.7070 (2.7306) acc 28.1250 (36.7188) lr 7.8853e-06 eta 0:00:03\n",
            "epoch [50/50] batch [45/50] time 0.354 (0.385) data 0.000 (0.021) loss 2.5684 (2.7464) acc 34.3750 (36.3194) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [50/50] time 0.354 (0.382) data 0.000 (0.019) loss 2.6055 (2.7412) acc 40.6250 (36.6875) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed2/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:38<00:00,  2.59it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 51\n",
            "* accuracy: 0.5%\n",
            "* error: 99.5%\n",
            "* macro_f1: 1.0%\n",
            "Elapsed: 0:16:52\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '8']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed3\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 8\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Creating a 8-shot dataset\n",
            "Saving preprocessed few-shot data to /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_8-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  1,600\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed3/tensorboard)\n",
            "epoch [1/50] batch [5/50] time 0.357 (1.095) data 0.001 (0.216) loss 5.2383 (5.0492) acc 3.1250 (7.5000) lr 1.0000e-05 eta 0:45:31\n",
            "epoch [1/50] batch [10/50] time 0.359 (0.725) data 0.001 (0.109) loss 4.4414 (4.7137) acc 0.0000 (8.7500) lr 1.0000e-05 eta 0:30:06\n",
            "epoch [1/50] batch [15/50] time 0.353 (0.602) data 0.001 (0.073) loss 4.1914 (4.6156) acc 15.6250 (11.0417) lr 1.0000e-05 eta 0:24:56\n",
            "epoch [1/50] batch [20/50] time 0.359 (0.541) data 0.001 (0.055) loss 4.0117 (4.4778) acc 18.7500 (12.9688) lr 1.0000e-05 eta 0:22:22\n",
            "epoch [1/50] batch [25/50] time 0.355 (0.505) data 0.001 (0.044) loss 4.3633 (4.3985) acc 9.3750 (14.0000) lr 1.0000e-05 eta 0:20:48\n",
            "epoch [1/50] batch [30/50] time 0.355 (0.480) data 0.001 (0.037) loss 3.9473 (4.3473) acc 15.6250 (14.6875) lr 1.0000e-05 eta 0:19:45\n",
            "epoch [1/50] batch [35/50] time 0.355 (0.462) data 0.001 (0.032) loss 4.0000 (4.2967) acc 18.7500 (14.8214) lr 1.0000e-05 eta 0:18:58\n",
            "epoch [1/50] batch [40/50] time 0.360 (0.449) data 0.000 (0.028) loss 3.9785 (4.2545) acc 21.8750 (15.3906) lr 1.0000e-05 eta 0:18:24\n",
            "epoch [1/50] batch [45/50] time 0.356 (0.438) data 0.000 (0.025) loss 3.7715 (4.1831) acc 9.3750 (16.3194) lr 1.0000e-05 eta 0:17:56\n",
            "epoch [1/50] batch [50/50] time 0.356 (0.430) data 0.000 (0.022) loss 3.4941 (4.1515) acc 34.3750 (16.6875) lr 2.0000e-03 eta 0:17:34\n",
            "epoch [2/50] batch [5/50] time 0.371 (0.603) data 0.009 (0.193) loss 3.8281 (3.9188) acc 21.8750 (16.2500) lr 2.0000e-03 eta 0:24:34\n",
            "epoch [2/50] batch [10/50] time 0.366 (0.485) data 0.009 (0.100) loss 2.9805 (3.6229) acc 28.1250 (20.6250) lr 2.0000e-03 eta 0:19:44\n",
            "epoch [2/50] batch [15/50] time 0.360 (0.446) data 0.001 (0.068) loss 3.3965 (3.5493) acc 28.1250 (21.8750) lr 2.0000e-03 eta 0:18:06\n",
            "epoch [2/50] batch [20/50] time 0.371 (0.427) data 0.008 (0.053) loss 2.7988 (3.4741) acc 37.5000 (22.6562) lr 2.0000e-03 eta 0:17:16\n",
            "epoch [2/50] batch [25/50] time 0.361 (0.415) data 0.001 (0.043) loss 2.8945 (3.4372) acc 28.1250 (23.3750) lr 2.0000e-03 eta 0:16:45\n",
            "epoch [2/50] batch [30/50] time 0.360 (0.406) data 0.001 (0.036) loss 3.8516 (3.4205) acc 21.8750 (24.2708) lr 2.0000e-03 eta 0:16:22\n",
            "epoch [2/50] batch [35/50] time 0.362 (0.400) data 0.001 (0.031) loss 3.0391 (3.3911) acc 31.2500 (25.0893) lr 2.0000e-03 eta 0:16:05\n",
            "epoch [2/50] batch [40/50] time 0.361 (0.395) data 0.000 (0.027) loss 3.3887 (3.4024) acc 25.0000 (24.6875) lr 2.0000e-03 eta 0:15:52\n",
            "epoch [2/50] batch [45/50] time 0.361 (0.392) data 0.000 (0.024) loss 3.4336 (3.4285) acc 28.1250 (24.5833) lr 2.0000e-03 eta 0:15:41\n",
            "epoch [2/50] batch [50/50] time 0.365 (0.389) data 0.000 (0.022) loss 3.5039 (3.4268) acc 25.0000 (24.8750) lr 1.9980e-03 eta 0:15:32\n",
            "epoch [3/50] batch [5/50] time 0.382 (0.715) data 0.008 (0.231) loss 3.7402 (3.6809) acc 18.7500 (21.2500) lr 1.9980e-03 eta 0:28:31\n",
            "epoch [3/50] batch [10/50] time 0.362 (0.540) data 0.007 (0.117) loss 3.4941 (3.4756) acc 21.8750 (22.8125) lr 1.9980e-03 eta 0:21:29\n",
            "epoch [3/50] batch [15/50] time 0.361 (0.482) data 0.001 (0.080) loss 3.6367 (3.4984) acc 18.7500 (22.7083) lr 1.9980e-03 eta 0:19:08\n",
            "epoch [3/50] batch [20/50] time 0.360 (0.453) data 0.001 (0.061) loss 2.9023 (3.4470) acc 40.6250 (24.3750) lr 1.9980e-03 eta 0:17:59\n",
            "epoch [3/50] batch [25/50] time 0.356 (0.436) data 0.001 (0.050) loss 3.1758 (3.4210) acc 25.0000 (25.0000) lr 1.9980e-03 eta 0:17:15\n",
            "epoch [3/50] batch [30/50] time 0.359 (0.423) data 0.001 (0.042) loss 3.0898 (3.3242) acc 40.6250 (26.7708) lr 1.9980e-03 eta 0:16:43\n",
            "epoch [3/50] batch [35/50] time 0.358 (0.414) data 0.001 (0.036) loss 3.8262 (3.3115) acc 15.6250 (26.6964) lr 1.9980e-03 eta 0:16:18\n",
            "epoch [3/50] batch [40/50] time 0.354 (0.407) data 0.000 (0.032) loss 3.8652 (3.3229) acc 15.6250 (26.5625) lr 1.9980e-03 eta 0:15:59\n",
            "epoch [3/50] batch [45/50] time 0.355 (0.401) data 0.000 (0.028) loss 3.0039 (3.3165) acc 31.2500 (26.4583) lr 1.9980e-03 eta 0:15:44\n",
            "epoch [3/50] batch [50/50] time 0.357 (0.397) data 0.000 (0.026) loss 2.8047 (3.3155) acc 31.2500 (26.6875) lr 1.9921e-03 eta 0:15:31\n",
            "epoch [4/50] batch [5/50] time 0.365 (0.824) data 0.008 (0.389) loss 3.7129 (3.3336) acc 12.5000 (23.1250) lr 1.9921e-03 eta 0:32:13\n",
            "epoch [4/50] batch [10/50] time 0.369 (0.594) data 0.011 (0.198) loss 3.1699 (3.3553) acc 37.5000 (25.9375) lr 1.9921e-03 eta 0:23:11\n",
            "epoch [4/50] batch [15/50] time 0.353 (0.516) data 0.001 (0.133) loss 3.1816 (3.2379) acc 18.7500 (27.9167) lr 1.9921e-03 eta 0:20:04\n",
            "epoch [4/50] batch [20/50] time 0.353 (0.476) data 0.001 (0.100) loss 2.7949 (3.1681) acc 43.7500 (28.7500) lr 1.9921e-03 eta 0:18:28\n",
            "epoch [4/50] batch [25/50] time 0.354 (0.451) data 0.001 (0.080) loss 3.1758 (3.2355) acc 34.3750 (27.2500) lr 1.9921e-03 eta 0:17:29\n",
            "epoch [4/50] batch [30/50] time 0.353 (0.435) data 0.000 (0.067) loss 3.8281 (3.2447) acc 15.6250 (26.7708) lr 1.9921e-03 eta 0:16:49\n",
            "epoch [4/50] batch [35/50] time 0.355 (0.423) data 0.001 (0.058) loss 3.1113 (3.2428) acc 37.5000 (27.2321) lr 1.9921e-03 eta 0:16:20\n",
            "epoch [4/50] batch [40/50] time 0.353 (0.415) data 0.000 (0.050) loss 2.6035 (3.1927) acc 40.6250 (28.1250) lr 1.9921e-03 eta 0:15:57\n",
            "epoch [4/50] batch [45/50] time 0.355 (0.408) data 0.001 (0.045) loss 3.2207 (3.1790) acc 31.2500 (28.2639) lr 1.9921e-03 eta 0:15:40\n",
            "epoch [4/50] batch [50/50] time 0.356 (0.403) data 0.002 (0.040) loss 3.5273 (3.1922) acc 31.2500 (28.2500) lr 1.9823e-03 eta 0:15:25\n",
            "epoch [5/50] batch [5/50] time 0.375 (0.774) data 0.009 (0.296) loss 3.1641 (2.9414) acc 21.8750 (30.6250) lr 1.9823e-03 eta 0:29:35\n",
            "epoch [5/50] batch [10/50] time 0.353 (0.565) data 0.001 (0.149) loss 3.0664 (3.0611) acc 28.1250 (29.6875) lr 1.9823e-03 eta 0:21:33\n",
            "epoch [5/50] batch [15/50] time 0.354 (0.494) data 0.001 (0.099) loss 3.2090 (3.0979) acc 28.1250 (28.9583) lr 1.9823e-03 eta 0:18:49\n",
            "epoch [5/50] batch [20/50] time 0.355 (0.459) data 0.000 (0.075) loss 3.5215 (3.1074) acc 28.1250 (29.3750) lr 1.9823e-03 eta 0:17:27\n",
            "epoch [5/50] batch [25/50] time 0.353 (0.438) data 0.001 (0.060) loss 2.9805 (3.0823) acc 37.5000 (30.6250) lr 1.9823e-03 eta 0:16:36\n",
            "epoch [5/50] batch [30/50] time 0.353 (0.424) data 0.001 (0.050) loss 3.9238 (3.1294) acc 12.5000 (29.7917) lr 1.9823e-03 eta 0:16:02\n",
            "epoch [5/50] batch [35/50] time 0.355 (0.414) data 0.001 (0.043) loss 3.4395 (3.1440) acc 21.8750 (29.6429) lr 1.9823e-03 eta 0:15:38\n",
            "epoch [5/50] batch [40/50] time 0.356 (0.407) data 0.001 (0.038) loss 3.5195 (3.1529) acc 28.1250 (29.6875) lr 1.9823e-03 eta 0:15:19\n",
            "epoch [5/50] batch [45/50] time 0.356 (0.401) data 0.001 (0.034) loss 3.3262 (3.1507) acc 25.0000 (29.7222) lr 1.9823e-03 eta 0:15:04\n",
            "epoch [5/50] batch [50/50] time 0.357 (0.397) data 0.001 (0.030) loss 3.4531 (3.1680) acc 28.1250 (29.1250) lr 1.9686e-03 eta 0:14:53\n",
            "epoch [6/50] batch [5/50] time 0.352 (0.595) data 0.001 (0.217) loss 3.1953 (3.1738) acc 31.2500 (27.5000) lr 1.9686e-03 eta 0:22:16\n",
            "epoch [6/50] batch [10/50] time 0.359 (0.477) data 0.001 (0.109) loss 2.8184 (3.2133) acc 31.2500 (28.7500) lr 1.9686e-03 eta 0:17:47\n",
            "epoch [6/50] batch [15/50] time 0.356 (0.436) data 0.001 (0.073) loss 3.5488 (3.2279) acc 34.3750 (29.1667) lr 1.9686e-03 eta 0:16:15\n",
            "epoch [6/50] batch [20/50] time 0.355 (0.417) data 0.001 (0.055) loss 3.0625 (3.2303) acc 28.1250 (29.0625) lr 1.9686e-03 eta 0:15:29\n",
            "epoch [6/50] batch [25/50] time 0.358 (0.405) data 0.001 (0.044) loss 3.0566 (3.2350) acc 21.8750 (28.3750) lr 1.9686e-03 eta 0:15:01\n",
            "epoch [6/50] batch [30/50] time 0.370 (0.398) data 0.007 (0.038) loss 2.9902 (3.2445) acc 34.3750 (28.2292) lr 1.9686e-03 eta 0:14:44\n",
            "epoch [6/50] batch [35/50] time 0.360 (0.394) data 0.001 (0.034) loss 3.0098 (3.2483) acc 21.8750 (27.9464) lr 1.9686e-03 eta 0:14:32\n",
            "epoch [6/50] batch [40/50] time 0.359 (0.389) data 0.001 (0.029) loss 2.6777 (3.2410) acc 25.0000 (27.8906) lr 1.9686e-03 eta 0:14:20\n",
            "epoch [6/50] batch [45/50] time 0.358 (0.386) data 0.001 (0.026) loss 2.5977 (3.1876) acc 40.6250 (29.0278) lr 1.9686e-03 eta 0:14:11\n",
            "epoch [6/50] batch [50/50] time 0.360 (0.383) data 0.001 (0.024) loss 3.5996 (3.1854) acc 28.1250 (29.0000) lr 1.9511e-03 eta 0:14:03\n",
            "epoch [7/50] batch [5/50] time 0.352 (0.596) data 0.000 (0.210) loss 3.1035 (3.1621) acc 21.8750 (25.0000) lr 1.9511e-03 eta 0:21:48\n",
            "epoch [7/50] batch [10/50] time 0.358 (0.477) data 0.001 (0.105) loss 2.6797 (3.0852) acc 34.3750 (29.0625) lr 1.9511e-03 eta 0:17:25\n",
            "epoch [7/50] batch [15/50] time 0.357 (0.437) data 0.001 (0.070) loss 3.1855 (3.1349) acc 31.2500 (29.3750) lr 1.9511e-03 eta 0:15:55\n",
            "epoch [7/50] batch [20/50] time 0.356 (0.417) data 0.001 (0.053) loss 3.3008 (3.1442) acc 31.2500 (30.7812) lr 1.9511e-03 eta 0:15:09\n",
            "epoch [7/50] batch [25/50] time 0.356 (0.405) data 0.000 (0.043) loss 3.1211 (3.1129) acc 34.3750 (31.8750) lr 1.9511e-03 eta 0:14:41\n",
            "epoch [7/50] batch [30/50] time 0.368 (0.399) data 0.012 (0.037) loss 3.4082 (3.1275) acc 31.2500 (32.6042) lr 1.9511e-03 eta 0:14:26\n",
            "epoch [7/50] batch [35/50] time 0.361 (0.394) data 0.001 (0.032) loss 3.4941 (3.1219) acc 25.0000 (32.3214) lr 1.9511e-03 eta 0:14:12\n",
            "epoch [7/50] batch [40/50] time 0.357 (0.389) data 0.001 (0.028) loss 3.0234 (3.0875) acc 25.0000 (32.1875) lr 1.9511e-03 eta 0:14:00\n",
            "epoch [7/50] batch [45/50] time 0.358 (0.386) data 0.001 (0.025) loss 2.8301 (3.1168) acc 34.3750 (30.9722) lr 1.9511e-03 eta 0:13:51\n",
            "epoch [7/50] batch [50/50] time 0.356 (0.383) data 0.000 (0.022) loss 4.1562 (3.1448) acc 12.5000 (30.2500) lr 1.9298e-03 eta 0:13:42\n",
            "epoch [8/50] batch [5/50] time 0.364 (0.543) data 0.001 (0.156) loss 3.0781 (2.9859) acc 37.5000 (35.0000) lr 1.9298e-03 eta 0:19:25\n",
            "epoch [8/50] batch [10/50] time 0.354 (0.449) data 0.001 (0.078) loss 2.7285 (3.0219) acc 40.6250 (33.1250) lr 1.9298e-03 eta 0:16:01\n",
            "epoch [8/50] batch [15/50] time 0.361 (0.421) data 0.008 (0.054) loss 2.8711 (3.0315) acc 31.2500 (32.0833) lr 1.9298e-03 eta 0:14:59\n",
            "epoch [8/50] batch [20/50] time 0.356 (0.405) data 0.001 (0.041) loss 3.1152 (3.0565) acc 28.1250 (30.9375) lr 1.9298e-03 eta 0:14:22\n",
            "epoch [8/50] batch [25/50] time 0.362 (0.396) data 0.008 (0.034) loss 2.5918 (3.0562) acc 40.6250 (31.5000) lr 1.9298e-03 eta 0:14:02\n",
            "epoch [8/50] batch [30/50] time 0.356 (0.390) data 0.001 (0.028) loss 2.5566 (3.0331) acc 37.5000 (31.9792) lr 1.9298e-03 eta 0:13:45\n",
            "epoch [8/50] batch [35/50] time 0.357 (0.386) data 0.001 (0.025) loss 3.3496 (3.0310) acc 28.1250 (32.4107) lr 1.9298e-03 eta 0:13:35\n",
            "epoch [8/50] batch [40/50] time 0.355 (0.382) data 0.001 (0.022) loss 2.9043 (3.0491) acc 31.2500 (31.8750) lr 1.9298e-03 eta 0:13:25\n",
            "epoch [8/50] batch [45/50] time 0.357 (0.379) data 0.001 (0.019) loss 3.2617 (3.0691) acc 21.8750 (31.1806) lr 1.9298e-03 eta 0:13:17\n",
            "epoch [8/50] batch [50/50] time 0.356 (0.377) data 0.000 (0.018) loss 3.4551 (3.1001) acc 28.1250 (30.7500) lr 1.9048e-03 eta 0:13:10\n",
            "epoch [9/50] batch [5/50] time 0.352 (0.599) data 0.002 (0.213) loss 2.9688 (3.2313) acc 37.5000 (29.3750) lr 1.9048e-03 eta 0:20:55\n",
            "epoch [9/50] batch [10/50] time 0.356 (0.477) data 0.001 (0.107) loss 2.6172 (3.1668) acc 40.6250 (30.6250) lr 1.9048e-03 eta 0:16:37\n",
            "epoch [9/50] batch [15/50] time 0.354 (0.437) data 0.001 (0.071) loss 2.6113 (3.0371) acc 37.5000 (31.2500) lr 1.9048e-03 eta 0:15:10\n",
            "epoch [9/50] batch [20/50] time 0.354 (0.416) data 0.001 (0.054) loss 3.2344 (3.0698) acc 37.5000 (32.5000) lr 1.9048e-03 eta 0:14:25\n",
            "epoch [9/50] batch [25/50] time 0.365 (0.405) data 0.007 (0.044) loss 2.9375 (3.0184) acc 31.2500 (33.3750) lr 1.9048e-03 eta 0:13:59\n",
            "epoch [9/50] batch [30/50] time 0.359 (0.397) data 0.001 (0.037) loss 3.3184 (3.1075) acc 25.0000 (31.4583) lr 1.9048e-03 eta 0:13:41\n",
            "epoch [9/50] batch [35/50] time 0.355 (0.392) data 0.001 (0.032) loss 3.7129 (3.1110) acc 18.7500 (31.4286) lr 1.9048e-03 eta 0:13:28\n",
            "epoch [9/50] batch [40/50] time 0.356 (0.387) data 0.001 (0.028) loss 2.9375 (3.1117) acc 25.0000 (31.0938) lr 1.9048e-03 eta 0:13:17\n",
            "epoch [9/50] batch [45/50] time 0.357 (0.384) data 0.000 (0.025) loss 3.4844 (3.0927) acc 18.7500 (31.5278) lr 1.9048e-03 eta 0:13:08\n",
            "epoch [9/50] batch [50/50] time 0.357 (0.381) data 0.001 (0.022) loss 3.0059 (3.0864) acc 31.2500 (31.4375) lr 1.8763e-03 eta 0:13:00\n",
            "epoch [10/50] batch [5/50] time 0.349 (0.577) data 0.000 (0.210) loss 2.8105 (2.9922) acc 37.5000 (33.7500) lr 1.8763e-03 eta 0:19:39\n",
            "epoch [10/50] batch [10/50] time 0.355 (0.466) data 0.001 (0.105) loss 2.6211 (2.9070) acc 43.7500 (35.6250) lr 1.8763e-03 eta 0:15:51\n",
            "epoch [10/50] batch [15/50] time 0.357 (0.429) data 0.001 (0.070) loss 2.8555 (3.0099) acc 34.3750 (33.1250) lr 1.8763e-03 eta 0:14:33\n",
            "epoch [10/50] batch [20/50] time 0.364 (0.412) data 0.008 (0.054) loss 3.6133 (3.1134) acc 25.0000 (30.4688) lr 1.8763e-03 eta 0:13:56\n",
            "epoch [10/50] batch [25/50] time 0.370 (0.402) data 0.007 (0.044) loss 3.4062 (3.1305) acc 34.3750 (30.3750) lr 1.8763e-03 eta 0:13:34\n",
            "epoch [10/50] batch [30/50] time 0.368 (0.396) data 0.007 (0.038) loss 3.2891 (3.1270) acc 31.2500 (30.8333) lr 1.8763e-03 eta 0:13:19\n",
            "epoch [10/50] batch [35/50] time 0.358 (0.391) data 0.001 (0.033) loss 2.9160 (3.1013) acc 34.3750 (31.0714) lr 1.8763e-03 eta 0:13:07\n",
            "epoch [10/50] batch [40/50] time 0.358 (0.387) data 0.001 (0.029) loss 3.3691 (3.1071) acc 21.8750 (31.0156) lr 1.8763e-03 eta 0:12:57\n",
            "epoch [10/50] batch [45/50] time 0.356 (0.383) data 0.000 (0.026) loss 3.0020 (3.1063) acc 37.5000 (31.1806) lr 1.8763e-03 eta 0:12:48\n",
            "epoch [10/50] batch [50/50] time 0.356 (0.380) data 0.000 (0.023) loss 2.8730 (3.1011) acc 34.3750 (31.4375) lr 1.8443e-03 eta 0:12:40\n",
            "epoch [11/50] batch [5/50] time 0.357 (0.594) data 0.007 (0.198) loss 2.7988 (3.1660) acc 34.3750 (30.6250) lr 1.8443e-03 eta 0:19:45\n",
            "epoch [11/50] batch [10/50] time 0.359 (0.475) data 0.001 (0.100) loss 2.9316 (3.1012) acc 28.1250 (32.1875) lr 1.8443e-03 eta 0:15:45\n",
            "epoch [11/50] batch [15/50] time 0.355 (0.435) data 0.001 (0.067) loss 3.2168 (3.1292) acc 28.1250 (32.2917) lr 1.8443e-03 eta 0:14:24\n",
            "epoch [11/50] batch [20/50] time 0.362 (0.417) data 0.008 (0.051) loss 2.8535 (3.1043) acc 43.7500 (32.6562) lr 1.8443e-03 eta 0:13:45\n",
            "epoch [11/50] batch [25/50] time 0.366 (0.406) data 0.009 (0.041) loss 2.7754 (3.0946) acc 34.3750 (32.1250) lr 1.8443e-03 eta 0:13:22\n",
            "epoch [11/50] batch [30/50] time 0.365 (0.399) data 0.005 (0.035) loss 2.7012 (3.0561) acc 34.3750 (32.3958) lr 1.8443e-03 eta 0:13:06\n",
            "epoch [11/50] batch [35/50] time 0.356 (0.394) data 0.001 (0.032) loss 3.7207 (3.0699) acc 25.0000 (32.0536) lr 1.8443e-03 eta 0:12:54\n",
            "epoch [11/50] batch [40/50] time 0.356 (0.390) data 0.001 (0.028) loss 3.1348 (3.0741) acc 28.1250 (31.6406) lr 1.8443e-03 eta 0:12:44\n",
            "epoch [11/50] batch [45/50] time 0.355 (0.386) data 0.000 (0.025) loss 3.3066 (3.0636) acc 34.3750 (32.0833) lr 1.8443e-03 eta 0:12:34\n",
            "epoch [11/50] batch [50/50] time 0.357 (0.383) data 0.000 (0.022) loss 3.1543 (3.0648) acc 28.1250 (32.0625) lr 1.8090e-03 eta 0:12:27\n",
            "epoch [12/50] batch [5/50] time 0.352 (0.562) data 0.001 (0.179) loss 3.2871 (3.0410) acc 21.8750 (31.8750) lr 1.8090e-03 eta 0:18:13\n",
            "epoch [12/50] batch [10/50] time 0.355 (0.459) data 0.001 (0.090) loss 3.3477 (2.9912) acc 31.2500 (33.1250) lr 1.8090e-03 eta 0:14:50\n",
            "epoch [12/50] batch [15/50] time 0.356 (0.425) data 0.001 (0.060) loss 2.8418 (2.9464) acc 28.1250 (33.5417) lr 1.8090e-03 eta 0:13:42\n",
            "epoch [12/50] batch [20/50] time 0.362 (0.409) data 0.001 (0.046) loss 2.8945 (2.9358) acc 31.2500 (33.9062) lr 1.8090e-03 eta 0:13:09\n",
            "epoch [12/50] batch [25/50] time 0.359 (0.399) data 0.001 (0.038) loss 3.3398 (2.9356) acc 25.0000 (34.1250) lr 1.8090e-03 eta 0:12:48\n",
            "epoch [12/50] batch [30/50] time 0.363 (0.394) data 0.007 (0.032) loss 2.6426 (2.9378) acc 34.3750 (34.2708) lr 1.8090e-03 eta 0:12:36\n",
            "epoch [12/50] batch [35/50] time 0.356 (0.389) data 0.001 (0.028) loss 3.0586 (2.9083) acc 31.2500 (35.0893) lr 1.8090e-03 eta 0:12:24\n",
            "epoch [12/50] batch [40/50] time 0.356 (0.385) data 0.000 (0.025) loss 2.7910 (2.9336) acc 37.5000 (34.7656) lr 1.8090e-03 eta 0:12:15\n",
            "epoch [12/50] batch [45/50] time 0.356 (0.382) data 0.000 (0.022) loss 3.2891 (2.9336) acc 31.2500 (34.3056) lr 1.8090e-03 eta 0:12:06\n",
            "epoch [12/50] batch [50/50] time 0.356 (0.379) data 0.000 (0.020) loss 3.3887 (2.9607) acc 25.0000 (33.8750) lr 1.7705e-03 eta 0:11:59\n",
            "epoch [13/50] batch [5/50] time 0.355 (0.532) data 0.001 (0.151) loss 2.8516 (3.1707) acc 31.2500 (30.6250) lr 1.7705e-03 eta 0:16:49\n",
            "epoch [13/50] batch [10/50] time 0.356 (0.444) data 0.001 (0.076) loss 3.1113 (3.0963) acc 25.0000 (30.3125) lr 1.7705e-03 eta 0:14:00\n",
            "epoch [13/50] batch [15/50] time 0.363 (0.417) data 0.011 (0.053) loss 3.0996 (3.0719) acc 31.2500 (32.5000) lr 1.7705e-03 eta 0:13:06\n",
            "epoch [13/50] batch [20/50] time 0.366 (0.404) data 0.008 (0.041) loss 3.2852 (3.0894) acc 25.0000 (32.1875) lr 1.7705e-03 eta 0:12:38\n",
            "epoch [13/50] batch [25/50] time 0.366 (0.397) data 0.014 (0.036) loss 2.8770 (3.0636) acc 31.2500 (31.7500) lr 1.7705e-03 eta 0:12:23\n",
            "epoch [13/50] batch [30/50] time 0.370 (0.391) data 0.011 (0.031) loss 2.7695 (3.0180) acc 37.5000 (33.0208) lr 1.7705e-03 eta 0:12:11\n",
            "epoch [13/50] batch [35/50] time 0.358 (0.387) data 0.002 (0.027) loss 2.6348 (3.0171) acc 40.6250 (33.0357) lr 1.7705e-03 eta 0:12:02\n",
            "epoch [13/50] batch [40/50] time 0.355 (0.383) data 0.000 (0.024) loss 3.5840 (3.0012) acc 21.8750 (33.4375) lr 1.7705e-03 eta 0:11:52\n",
            "epoch [13/50] batch [45/50] time 0.355 (0.380) data 0.000 (0.021) loss 3.2520 (3.0098) acc 31.2500 (33.6806) lr 1.7705e-03 eta 0:11:44\n",
            "epoch [13/50] batch [50/50] time 0.354 (0.377) data 0.000 (0.019) loss 3.0000 (3.0027) acc 34.3750 (34.0625) lr 1.7290e-03 eta 0:11:38\n",
            "epoch [14/50] batch [5/50] time 0.350 (0.595) data 0.001 (0.227) loss 2.7695 (3.1453) acc 34.3750 (27.5000) lr 1.7290e-03 eta 0:18:17\n",
            "epoch [14/50] batch [10/50] time 0.368 (0.477) data 0.015 (0.115) loss 2.8809 (3.0426) acc 37.5000 (29.3750) lr 1.7290e-03 eta 0:14:37\n",
            "epoch [14/50] batch [15/50] time 0.370 (0.439) data 0.011 (0.079) loss 2.9844 (3.0464) acc 34.3750 (31.4583) lr 1.7290e-03 eta 0:13:24\n",
            "epoch [14/50] batch [20/50] time 0.357 (0.419) data 0.001 (0.060) loss 2.4570 (2.9593) acc 37.5000 (31.8750) lr 1.7290e-03 eta 0:12:46\n",
            "epoch [14/50] batch [25/50] time 0.363 (0.407) data 0.007 (0.049) loss 3.2012 (3.0387) acc 31.2500 (30.5000) lr 1.7290e-03 eta 0:12:23\n",
            "epoch [14/50] batch [30/50] time 0.363 (0.400) data 0.007 (0.041) loss 2.7910 (3.0312) acc 31.2500 (30.9375) lr 1.7290e-03 eta 0:12:07\n",
            "epoch [14/50] batch [35/50] time 0.355 (0.393) data 0.001 (0.035) loss 3.9297 (3.0405) acc 15.6250 (31.1607) lr 1.7290e-03 eta 0:11:53\n",
            "epoch [14/50] batch [40/50] time 0.354 (0.389) data 0.000 (0.031) loss 3.1172 (3.0559) acc 21.8750 (30.7031) lr 1.7290e-03 eta 0:11:43\n",
            "epoch [14/50] batch [45/50] time 0.354 (0.385) data 0.000 (0.028) loss 3.3164 (3.0510) acc 25.0000 (30.9028) lr 1.7290e-03 eta 0:11:34\n",
            "epoch [14/50] batch [50/50] time 0.356 (0.382) data 0.000 (0.025) loss 2.7441 (3.0093) acc 40.6250 (31.8750) lr 1.6845e-03 eta 0:11:27\n",
            "epoch [15/50] batch [5/50] time 0.368 (0.567) data 0.007 (0.184) loss 2.9219 (2.7723) acc 28.1250 (37.5000) lr 1.6845e-03 eta 0:16:57\n",
            "epoch [15/50] batch [10/50] time 0.367 (0.466) data 0.011 (0.096) loss 3.2090 (2.9010) acc 37.5000 (34.6875) lr 1.6845e-03 eta 0:13:54\n",
            "epoch [15/50] batch [15/50] time 0.371 (0.434) data 0.014 (0.068) loss 2.6758 (2.7814) acc 40.6250 (36.4583) lr 1.6845e-03 eta 0:12:54\n",
            "epoch [15/50] batch [20/50] time 0.365 (0.417) data 0.008 (0.053) loss 3.1328 (2.8287) acc 37.5000 (36.4062) lr 1.6845e-03 eta 0:12:21\n",
            "epoch [15/50] batch [25/50] time 0.359 (0.406) data 0.001 (0.043) loss 3.0137 (2.8038) acc 31.2500 (36.3750) lr 1.6845e-03 eta 0:12:00\n",
            "epoch [15/50] batch [30/50] time 0.355 (0.399) data 0.001 (0.037) loss 2.6699 (2.8092) acc 46.8750 (36.1458) lr 1.6845e-03 eta 0:11:45\n",
            "epoch [15/50] batch [35/50] time 0.358 (0.393) data 0.001 (0.032) loss 3.2402 (2.8424) acc 31.2500 (35.9821) lr 1.6845e-03 eta 0:11:32\n",
            "epoch [15/50] batch [40/50] time 0.356 (0.388) data 0.000 (0.028) loss 3.3125 (2.8653) acc 28.1250 (35.8594) lr 1.6845e-03 eta 0:11:22\n",
            "epoch [15/50] batch [45/50] time 0.358 (0.384) data 0.000 (0.025) loss 2.7812 (2.8779) acc 34.3750 (35.2778) lr 1.6845e-03 eta 0:11:14\n",
            "epoch [15/50] batch [50/50] time 0.356 (0.381) data 0.000 (0.022) loss 2.7559 (2.8899) acc 46.8750 (35.1875) lr 1.6374e-03 eta 0:11:07\n",
            "epoch [16/50] batch [5/50] time 0.367 (0.527) data 0.008 (0.139) loss 3.0820 (3.1344) acc 31.2500 (30.6250) lr 1.6374e-03 eta 0:15:19\n",
            "epoch [16/50] batch [10/50] time 0.360 (0.444) data 0.001 (0.073) loss 2.7578 (2.9982) acc 40.6250 (35.0000) lr 1.6374e-03 eta 0:12:52\n",
            "epoch [16/50] batch [15/50] time 0.365 (0.416) data 0.009 (0.049) loss 2.5449 (2.8656) acc 37.5000 (36.8750) lr 1.6374e-03 eta 0:12:01\n",
            "epoch [16/50] batch [20/50] time 0.372 (0.403) data 0.008 (0.038) loss 2.6875 (2.8600) acc 37.5000 (36.8750) lr 1.6374e-03 eta 0:11:37\n",
            "epoch [16/50] batch [25/50] time 0.364 (0.395) data 0.008 (0.032) loss 3.5176 (2.8688) acc 21.8750 (35.6250) lr 1.6374e-03 eta 0:11:21\n",
            "epoch [16/50] batch [30/50] time 0.357 (0.389) data 0.001 (0.027) loss 3.0117 (2.9184) acc 34.3750 (34.7917) lr 1.6374e-03 eta 0:11:08\n",
            "epoch [16/50] batch [35/50] time 0.355 (0.384) data 0.001 (0.023) loss 2.1836 (2.9037) acc 53.1250 (35.2679) lr 1.6374e-03 eta 0:10:58\n",
            "epoch [16/50] batch [40/50] time 0.356 (0.381) data 0.001 (0.020) loss 3.1074 (2.8828) acc 25.0000 (34.8438) lr 1.6374e-03 eta 0:10:50\n",
            "epoch [16/50] batch [45/50] time 0.354 (0.378) data 0.000 (0.018) loss 2.8691 (2.8838) acc 37.5000 (34.7222) lr 1.6374e-03 eta 0:10:44\n",
            "epoch [16/50] batch [50/50] time 0.354 (0.376) data 0.000 (0.016) loss 2.8750 (2.8912) acc 37.5000 (34.8125) lr 1.5878e-03 eta 0:10:38\n",
            "epoch [17/50] batch [5/50] time 0.364 (0.669) data 0.008 (0.223) loss 2.5938 (2.6988) acc 40.6250 (36.8750) lr 1.5878e-03 eta 0:18:53\n",
            "epoch [17/50] batch [10/50] time 0.370 (0.518) data 0.008 (0.115) loss 2.9355 (2.6246) acc 37.5000 (37.5000) lr 1.5878e-03 eta 0:14:34\n",
            "epoch [17/50] batch [15/50] time 0.368 (0.467) data 0.013 (0.079) loss 3.2168 (2.7039) acc 37.5000 (38.5417) lr 1.5878e-03 eta 0:13:06\n",
            "epoch [17/50] batch [20/50] time 0.367 (0.442) data 0.005 (0.061) loss 2.5078 (2.7938) acc 37.5000 (37.1875) lr 1.5878e-03 eta 0:12:22\n",
            "epoch [17/50] batch [25/50] time 0.355 (0.425) data 0.000 (0.050) loss 3.8359 (2.8594) acc 18.7500 (35.0000) lr 1.5878e-03 eta 0:11:52\n",
            "epoch [17/50] batch [30/50] time 0.356 (0.414) data 0.001 (0.042) loss 2.7090 (2.8374) acc 46.8750 (35.6250) lr 1.5878e-03 eta 0:11:30\n",
            "epoch [17/50] batch [35/50] time 0.356 (0.405) data 0.001 (0.036) loss 3.3770 (2.8550) acc 31.2500 (35.7143) lr 1.5878e-03 eta 0:11:14\n",
            "epoch [17/50] batch [40/50] time 0.355 (0.399) data 0.000 (0.031) loss 2.6133 (2.8544) acc 43.7500 (35.9375) lr 1.5878e-03 eta 0:11:02\n",
            "epoch [17/50] batch [45/50] time 0.356 (0.394) data 0.000 (0.028) loss 3.4180 (2.8540) acc 18.7500 (35.4167) lr 1.5878e-03 eta 0:10:52\n",
            "epoch [17/50] batch [50/50] time 0.354 (0.390) data 0.001 (0.025) loss 2.8164 (2.8712) acc 34.3750 (35.4375) lr 1.5358e-03 eta 0:10:44\n",
            "epoch [18/50] batch [5/50] time 0.365 (0.740) data 0.001 (0.289) loss 2.7520 (2.7375) acc 40.6250 (41.8750) lr 1.5358e-03 eta 0:20:17\n",
            "epoch [18/50] batch [10/50] time 0.365 (0.552) data 0.009 (0.148) loss 2.6191 (2.7818) acc 46.8750 (41.8750) lr 1.5358e-03 eta 0:15:05\n",
            "epoch [18/50] batch [15/50] time 0.363 (0.489) data 0.009 (0.100) loss 2.7871 (2.8125) acc 40.6250 (39.1667) lr 1.5358e-03 eta 0:13:20\n",
            "epoch [18/50] batch [20/50] time 0.355 (0.457) data 0.001 (0.076) loss 2.9277 (2.8375) acc 28.1250 (37.6562) lr 1.5358e-03 eta 0:12:25\n",
            "epoch [18/50] batch [25/50] time 0.355 (0.438) data 0.001 (0.062) loss 2.4160 (2.8773) acc 46.8750 (36.1250) lr 1.5358e-03 eta 0:11:52\n",
            "epoch [18/50] batch [30/50] time 0.353 (0.425) data 0.001 (0.052) loss 3.0996 (2.8503) acc 37.5000 (36.5625) lr 1.5358e-03 eta 0:11:27\n",
            "epoch [18/50] batch [35/50] time 0.356 (0.415) data 0.001 (0.045) loss 3.0293 (2.8410) acc 37.5000 (36.9643) lr 1.5358e-03 eta 0:11:09\n",
            "epoch [18/50] batch [40/50] time 0.355 (0.407) data 0.000 (0.039) loss 3.2090 (2.8965) acc 34.3750 (35.7812) lr 1.5358e-03 eta 0:10:55\n",
            "epoch [18/50] batch [45/50] time 0.356 (0.402) data 0.000 (0.035) loss 3.2168 (2.9093) acc 25.0000 (35.2778) lr 1.5358e-03 eta 0:10:44\n",
            "epoch [18/50] batch [50/50] time 0.356 (0.397) data 0.001 (0.031) loss 2.8867 (2.9068) acc 28.1250 (35.2500) lr 1.4818e-03 eta 0:10:35\n",
            "epoch [19/50] batch [5/50] time 0.386 (0.778) data 0.008 (0.271) loss 2.4082 (2.7316) acc 40.6250 (34.3750) lr 1.4818e-03 eta 0:20:40\n",
            "epoch [19/50] batch [10/50] time 0.366 (0.570) data 0.008 (0.139) loss 2.8398 (2.7080) acc 40.6250 (39.0625) lr 1.4818e-03 eta 0:15:06\n",
            "epoch [19/50] batch [15/50] time 0.357 (0.499) data 0.001 (0.093) loss 2.8203 (2.6564) acc 34.3750 (39.5833) lr 1.4818e-03 eta 0:13:11\n",
            "epoch [19/50] batch [20/50] time 0.357 (0.463) data 0.001 (0.070) loss 3.2832 (2.7617) acc 25.0000 (39.0625) lr 1.4818e-03 eta 0:12:12\n",
            "epoch [19/50] batch [25/50] time 0.356 (0.442) data 0.001 (0.056) loss 2.3809 (2.7536) acc 43.7500 (38.6250) lr 1.4818e-03 eta 0:11:35\n",
            "epoch [19/50] batch [30/50] time 0.356 (0.427) data 0.000 (0.047) loss 3.0566 (2.7813) acc 31.2500 (37.8125) lr 1.4818e-03 eta 0:11:11\n",
            "epoch [19/50] batch [35/50] time 0.354 (0.417) data 0.001 (0.040) loss 2.9941 (2.7617) acc 25.0000 (38.0357) lr 1.4818e-03 eta 0:10:52\n",
            "epoch [19/50] batch [40/50] time 0.354 (0.409) data 0.000 (0.035) loss 3.1758 (2.7856) acc 28.1250 (37.0312) lr 1.4818e-03 eta 0:10:38\n",
            "epoch [19/50] batch [45/50] time 0.357 (0.404) data 0.001 (0.031) loss 3.3008 (2.8183) acc 25.0000 (36.5278) lr 1.4818e-03 eta 0:10:27\n",
            "epoch [19/50] batch [50/50] time 0.356 (0.399) data 0.001 (0.028) loss 2.7793 (2.8289) acc 28.1250 (36.1250) lr 1.4258e-03 eta 0:10:18\n",
            "epoch [20/50] batch [5/50] time 0.352 (0.610) data 0.001 (0.240) loss 2.9219 (2.6680) acc 40.6250 (44.3750) lr 1.4258e-03 eta 0:15:42\n",
            "epoch [20/50] batch [10/50] time 0.360 (0.483) data 0.001 (0.121) loss 2.9023 (2.8320) acc 31.2500 (37.1875) lr 1.4258e-03 eta 0:12:24\n",
            "epoch [20/50] batch [15/50] time 0.355 (0.441) data 0.000 (0.081) loss 2.7656 (2.7553) acc 28.1250 (36.4583) lr 1.4258e-03 eta 0:11:16\n",
            "epoch [20/50] batch [20/50] time 0.357 (0.419) data 0.001 (0.061) loss 2.4707 (2.7885) acc 46.8750 (36.4062) lr 1.4258e-03 eta 0:10:41\n",
            "epoch [20/50] batch [25/50] time 0.356 (0.407) data 0.001 (0.049) loss 2.5410 (2.7788) acc 46.8750 (36.7500) lr 1.4258e-03 eta 0:10:20\n",
            "epoch [20/50] batch [30/50] time 0.359 (0.399) data 0.001 (0.041) loss 3.3691 (2.7957) acc 40.6250 (37.1875) lr 1.4258e-03 eta 0:10:07\n",
            "epoch [20/50] batch [35/50] time 0.356 (0.394) data 0.001 (0.036) loss 2.8066 (2.7699) acc 34.3750 (38.1250) lr 1.4258e-03 eta 0:09:57\n",
            "epoch [20/50] batch [40/50] time 0.357 (0.389) data 0.001 (0.032) loss 2.7129 (2.7597) acc 25.0000 (37.4219) lr 1.4258e-03 eta 0:09:48\n",
            "epoch [20/50] batch [45/50] time 0.356 (0.386) data 0.001 (0.028) loss 2.7246 (2.7909) acc 31.2500 (36.8750) lr 1.4258e-03 eta 0:09:40\n",
            "epoch [20/50] batch [50/50] time 0.354 (0.383) data 0.001 (0.025) loss 3.1562 (2.7955) acc 34.3750 (36.5000) lr 1.3681e-03 eta 0:09:34\n",
            "epoch [21/50] batch [5/50] time 0.353 (0.560) data 0.001 (0.173) loss 2.4199 (2.8305) acc 43.7500 (40.0000) lr 1.3681e-03 eta 0:13:57\n",
            "epoch [21/50] batch [10/50] time 0.358 (0.458) data 0.000 (0.087) loss 2.7129 (2.8771) acc 37.5000 (35.0000) lr 1.3681e-03 eta 0:11:22\n",
            "epoch [21/50] batch [15/50] time 0.356 (0.424) data 0.001 (0.058) loss 2.3809 (2.8398) acc 40.6250 (36.6667) lr 1.3681e-03 eta 0:10:29\n",
            "epoch [21/50] batch [20/50] time 0.357 (0.407) data 0.001 (0.044) loss 3.9453 (2.9497) acc 6.2500 (32.8125) lr 1.3681e-03 eta 0:10:02\n",
            "epoch [21/50] batch [25/50] time 0.356 (0.397) data 0.000 (0.035) loss 3.0938 (2.8767) acc 25.0000 (34.1250) lr 1.3681e-03 eta 0:09:45\n",
            "epoch [21/50] batch [30/50] time 0.361 (0.391) data 0.003 (0.030) loss 3.1465 (2.8451) acc 21.8750 (34.7917) lr 1.3681e-03 eta 0:09:34\n",
            "epoch [21/50] batch [35/50] time 0.356 (0.387) data 0.001 (0.026) loss 2.3887 (2.8134) acc 43.7500 (35.9821) lr 1.3681e-03 eta 0:09:26\n",
            "epoch [21/50] batch [40/50] time 0.355 (0.383) data 0.001 (0.023) loss 2.8457 (2.8291) acc 34.3750 (35.0781) lr 1.3681e-03 eta 0:09:19\n",
            "epoch [21/50] batch [45/50] time 0.357 (0.380) data 0.001 (0.021) loss 3.0098 (2.8190) acc 34.3750 (35.1389) lr 1.3681e-03 eta 0:09:13\n",
            "epoch [21/50] batch [50/50] time 0.357 (0.378) data 0.001 (0.019) loss 3.5430 (2.8505) acc 28.1250 (34.5000) lr 1.3090e-03 eta 0:09:07\n",
            "epoch [22/50] batch [5/50] time 0.359 (0.520) data 0.000 (0.117) loss 2.9297 (2.8723) acc 43.7500 (40.6250) lr 1.3090e-03 eta 0:12:31\n",
            "epoch [22/50] batch [10/50] time 0.354 (0.437) data 0.000 (0.059) loss 2.7422 (2.8623) acc 34.3750 (37.5000) lr 1.3090e-03 eta 0:10:29\n",
            "epoch [22/50] batch [15/50] time 0.354 (0.410) data 0.001 (0.040) loss 2.5977 (2.7740) acc 40.6250 (39.1667) lr 1.3090e-03 eta 0:09:48\n",
            "epoch [22/50] batch [20/50] time 0.356 (0.397) data 0.001 (0.030) loss 2.7285 (2.8363) acc 40.6250 (37.8125) lr 1.3090e-03 eta 0:09:27\n",
            "epoch [22/50] batch [25/50] time 0.358 (0.388) data 0.001 (0.024) loss 2.3398 (2.7596) acc 43.7500 (38.6250) lr 1.3090e-03 eta 0:09:13\n",
            "epoch [22/50] batch [30/50] time 0.362 (0.383) data 0.008 (0.021) loss 2.6309 (2.7054) acc 43.7500 (39.7917) lr 1.3090e-03 eta 0:09:04\n",
            "epoch [22/50] batch [35/50] time 0.358 (0.380) data 0.001 (0.018) loss 2.7402 (2.7323) acc 34.3750 (38.8393) lr 1.3090e-03 eta 0:08:57\n",
            "epoch [22/50] batch [40/50] time 0.356 (0.377) data 0.001 (0.016) loss 2.9629 (2.7402) acc 37.5000 (38.7500) lr 1.3090e-03 eta 0:08:51\n",
            "epoch [22/50] batch [45/50] time 0.358 (0.375) data 0.001 (0.014) loss 2.8125 (2.7507) acc 34.3750 (38.5417) lr 1.3090e-03 eta 0:08:46\n",
            "epoch [22/50] batch [50/50] time 0.357 (0.373) data 0.001 (0.013) loss 2.9395 (2.8024) acc 46.8750 (37.6250) lr 1.2487e-03 eta 0:08:42\n",
            "epoch [23/50] batch [5/50] time 0.350 (0.571) data 0.001 (0.176) loss 2.4531 (2.6266) acc 31.2500 (36.2500) lr 1.2487e-03 eta 0:13:16\n",
            "epoch [23/50] batch [10/50] time 0.357 (0.464) data 0.002 (0.088) loss 2.6934 (2.6848) acc 37.5000 (38.4375) lr 1.2487e-03 eta 0:10:44\n",
            "epoch [23/50] batch [15/50] time 0.357 (0.428) data 0.001 (0.059) loss 2.7676 (2.7585) acc 34.3750 (36.6667) lr 1.2487e-03 eta 0:09:52\n",
            "epoch [23/50] batch [20/50] time 0.357 (0.411) data 0.000 (0.045) loss 2.7852 (2.7843) acc 34.3750 (37.0312) lr 1.2487e-03 eta 0:09:27\n",
            "epoch [23/50] batch [25/50] time 0.353 (0.400) data 0.001 (0.037) loss 2.5664 (2.7769) acc 40.6250 (36.1250) lr 1.2487e-03 eta 0:09:09\n",
            "epoch [23/50] batch [30/50] time 0.360 (0.393) data 0.001 (0.031) loss 2.7793 (2.7735) acc 37.5000 (35.7292) lr 1.2487e-03 eta 0:08:58\n",
            "epoch [23/50] batch [35/50] time 0.357 (0.389) data 0.001 (0.027) loss 2.7441 (2.8034) acc 37.5000 (34.8214) lr 1.2487e-03 eta 0:08:50\n",
            "epoch [23/50] batch [40/50] time 0.354 (0.385) data 0.001 (0.024) loss 2.8438 (2.8086) acc 34.3750 (34.8438) lr 1.2487e-03 eta 0:08:43\n",
            "epoch [23/50] batch [45/50] time 0.355 (0.382) data 0.001 (0.021) loss 2.3867 (2.7797) acc 50.0000 (35.2083) lr 1.2487e-03 eta 0:08:37\n",
            "epoch [23/50] batch [50/50] time 0.356 (0.379) data 0.000 (0.019) loss 2.9043 (2.8018) acc 40.6250 (34.9375) lr 1.1874e-03 eta 0:08:31\n",
            "epoch [24/50] batch [5/50] time 0.350 (0.556) data 0.001 (0.166) loss 2.2109 (2.3574) acc 46.8750 (44.3750) lr 1.1874e-03 eta 0:12:27\n",
            "epoch [24/50] batch [10/50] time 0.359 (0.457) data 0.001 (0.084) loss 2.4707 (2.5059) acc 46.8750 (41.5625) lr 1.1874e-03 eta 0:10:11\n",
            "epoch [24/50] batch [15/50] time 0.357 (0.423) data 0.002 (0.056) loss 2.5586 (2.6197) acc 43.7500 (39.7917) lr 1.1874e-03 eta 0:09:24\n",
            "epoch [24/50] batch [20/50] time 0.356 (0.406) data 0.000 (0.042) loss 3.0645 (2.6776) acc 34.3750 (38.1250) lr 1.1874e-03 eta 0:09:00\n",
            "epoch [24/50] batch [25/50] time 0.363 (0.397) data 0.008 (0.034) loss 3.2090 (2.7113) acc 31.2500 (37.6250) lr 1.1874e-03 eta 0:08:45\n",
            "epoch [24/50] batch [30/50] time 0.359 (0.391) data 0.001 (0.029) loss 2.8789 (2.7382) acc 43.7500 (38.0208) lr 1.1874e-03 eta 0:08:35\n",
            "epoch [24/50] batch [35/50] time 0.355 (0.386) data 0.001 (0.025) loss 2.2539 (2.6975) acc 46.8750 (38.6607) lr 1.1874e-03 eta 0:08:27\n",
            "epoch [24/50] batch [40/50] time 0.354 (0.382) data 0.001 (0.022) loss 3.2852 (2.7820) acc 31.2500 (37.1094) lr 1.1874e-03 eta 0:08:20\n",
            "epoch [24/50] batch [45/50] time 0.355 (0.380) data 0.000 (0.020) loss 2.4434 (2.7982) acc 40.6250 (36.8750) lr 1.1874e-03 eta 0:08:15\n",
            "epoch [24/50] batch [50/50] time 0.355 (0.377) data 0.000 (0.018) loss 2.7559 (2.7746) acc 28.1250 (37.1875) lr 1.1253e-03 eta 0:08:10\n",
            "epoch [25/50] batch [5/50] time 0.354 (0.580) data 0.001 (0.197) loss 2.6895 (2.9508) acc 37.5000 (33.7500) lr 1.1253e-03 eta 0:12:31\n",
            "epoch [25/50] batch [10/50] time 0.357 (0.468) data 0.001 (0.099) loss 1.9521 (2.7794) acc 50.0000 (37.1875) lr 1.1253e-03 eta 0:10:03\n",
            "epoch [25/50] batch [15/50] time 0.356 (0.431) data 0.000 (0.066) loss 2.8828 (2.7984) acc 50.0000 (38.3333) lr 1.1253e-03 eta 0:09:13\n",
            "epoch [25/50] batch [20/50] time 0.359 (0.412) data 0.003 (0.050) loss 3.2754 (2.8414) acc 40.6250 (37.6562) lr 1.1253e-03 eta 0:08:47\n",
            "epoch [25/50] batch [25/50] time 0.360 (0.401) data 0.001 (0.040) loss 3.4434 (2.8762) acc 25.0000 (36.7500) lr 1.1253e-03 eta 0:08:31\n",
            "epoch [25/50] batch [30/50] time 0.356 (0.394) data 0.001 (0.034) loss 2.5762 (2.8746) acc 31.2500 (36.4583) lr 1.1253e-03 eta 0:08:20\n",
            "epoch [25/50] batch [35/50] time 0.355 (0.390) data 0.001 (0.030) loss 2.6816 (2.8435) acc 40.6250 (36.6071) lr 1.1253e-03 eta 0:08:13\n",
            "epoch [25/50] batch [40/50] time 0.354 (0.385) data 0.001 (0.026) loss 2.9961 (2.8214) acc 34.3750 (36.5625) lr 1.1253e-03 eta 0:08:05\n",
            "epoch [25/50] batch [45/50] time 0.354 (0.382) data 0.000 (0.023) loss 2.8164 (2.8231) acc 37.5000 (36.8750) lr 1.1253e-03 eta 0:07:59\n",
            "epoch [25/50] batch [50/50] time 0.357 (0.379) data 0.000 (0.021) loss 2.6465 (2.8277) acc 37.5000 (36.8125) lr 1.0628e-03 eta 0:07:54\n",
            "epoch [26/50] batch [5/50] time 0.357 (0.522) data 0.001 (0.127) loss 2.5859 (2.6848) acc 50.0000 (31.2500) lr 1.0628e-03 eta 0:10:50\n",
            "epoch [26/50] batch [10/50] time 0.355 (0.439) data 0.000 (0.064) loss 2.5176 (2.5947) acc 37.5000 (38.4375) lr 1.0628e-03 eta 0:09:04\n",
            "epoch [26/50] batch [15/50] time 0.355 (0.411) data 0.001 (0.043) loss 2.5195 (2.6618) acc 37.5000 (37.7083) lr 1.0628e-03 eta 0:08:27\n",
            "epoch [26/50] batch [20/50] time 0.355 (0.398) data 0.001 (0.033) loss 3.0000 (2.7155) acc 28.1250 (37.3438) lr 1.0628e-03 eta 0:08:10\n",
            "epoch [26/50] batch [25/50] time 0.361 (0.391) data 0.001 (0.028) loss 2.5918 (2.7114) acc 34.3750 (37.7500) lr 1.0628e-03 eta 0:07:59\n",
            "epoch [26/50] batch [30/50] time 0.372 (0.387) data 0.016 (0.024) loss 2.7207 (2.7253) acc 31.2500 (37.2917) lr 1.0628e-03 eta 0:07:51\n",
            "epoch [26/50] batch [35/50] time 0.360 (0.383) data 0.001 (0.022) loss 3.2344 (2.7413) acc 28.1250 (36.8750) lr 1.0628e-03 eta 0:07:45\n",
            "epoch [26/50] batch [40/50] time 0.356 (0.380) data 0.001 (0.019) loss 2.4102 (2.7291) acc 43.7500 (37.2656) lr 1.0628e-03 eta 0:07:39\n",
            "epoch [26/50] batch [45/50] time 0.356 (0.377) data 0.000 (0.017) loss 2.7285 (2.7595) acc 37.5000 (36.9444) lr 1.0628e-03 eta 0:07:34\n",
            "epoch [26/50] batch [50/50] time 0.356 (0.375) data 0.000 (0.015) loss 3.2422 (2.8035) acc 28.1250 (36.5625) lr 1.0000e-03 eta 0:07:30\n",
            "epoch [27/50] batch [5/50] time 0.351 (0.537) data 0.001 (0.158) loss 2.6738 (2.5262) acc 43.7500 (44.3750) lr 1.0000e-03 eta 0:10:42\n",
            "epoch [27/50] batch [10/50] time 0.362 (0.447) data 0.007 (0.080) loss 2.9121 (2.6111) acc 28.1250 (42.1875) lr 1.0000e-03 eta 0:08:52\n",
            "epoch [27/50] batch [15/50] time 0.355 (0.417) data 0.001 (0.053) loss 2.6855 (2.6451) acc 46.8750 (41.0417) lr 1.0000e-03 eta 0:08:14\n",
            "epoch [27/50] batch [20/50] time 0.366 (0.403) data 0.007 (0.041) loss 3.2598 (2.7218) acc 31.2500 (38.2812) lr 1.0000e-03 eta 0:07:55\n",
            "epoch [27/50] batch [25/50] time 0.358 (0.395) data 0.001 (0.034) loss 2.8711 (2.6589) acc 40.6250 (40.1250) lr 1.0000e-03 eta 0:07:43\n",
            "epoch [27/50] batch [30/50] time 0.362 (0.390) data 0.001 (0.029) loss 2.8711 (2.6981) acc 34.3750 (39.1667) lr 1.0000e-03 eta 0:07:35\n",
            "epoch [27/50] batch [35/50] time 0.356 (0.385) data 0.001 (0.025) loss 2.5449 (2.6917) acc 37.5000 (39.0179) lr 1.0000e-03 eta 0:07:28\n",
            "epoch [27/50] batch [40/50] time 0.356 (0.381) data 0.000 (0.022) loss 2.7344 (2.7121) acc 28.1250 (37.8906) lr 1.0000e-03 eta 0:07:22\n",
            "epoch [27/50] batch [45/50] time 0.355 (0.378) data 0.000 (0.020) loss 3.0332 (2.7391) acc 28.1250 (37.0139) lr 1.0000e-03 eta 0:07:17\n",
            "epoch [27/50] batch [50/50] time 0.355 (0.376) data 0.000 (0.018) loss 2.4883 (2.7328) acc 37.5000 (36.8125) lr 9.3721e-04 eta 0:07:12\n",
            "epoch [28/50] batch [5/50] time 0.351 (0.580) data 0.001 (0.200) loss 2.5801 (2.7355) acc 37.5000 (36.2500) lr 9.3721e-04 eta 0:11:04\n",
            "epoch [28/50] batch [10/50] time 0.356 (0.468) data 0.001 (0.100) loss 2.9961 (2.7172) acc 31.2500 (38.4375) lr 9.3721e-04 eta 0:08:53\n",
            "epoch [28/50] batch [15/50] time 0.361 (0.431) data 0.001 (0.068) loss 2.3750 (2.7237) acc 34.3750 (38.1250) lr 9.3721e-04 eta 0:08:09\n",
            "epoch [28/50] batch [20/50] time 0.372 (0.416) data 0.014 (0.054) loss 2.6484 (2.7139) acc 43.7500 (38.5938) lr 9.3721e-04 eta 0:07:49\n",
            "epoch [28/50] batch [25/50] time 0.365 (0.405) data 0.008 (0.044) loss 2.3926 (2.7369) acc 46.8750 (38.0000) lr 9.3721e-04 eta 0:07:36\n",
            "epoch [28/50] batch [30/50] time 0.359 (0.398) data 0.001 (0.038) loss 2.5977 (2.7357) acc 37.5000 (38.1250) lr 9.3721e-04 eta 0:07:25\n",
            "epoch [28/50] batch [35/50] time 0.357 (0.392) data 0.001 (0.033) loss 2.6172 (2.7357) acc 31.2500 (37.6786) lr 9.3721e-04 eta 0:07:17\n",
            "epoch [28/50] batch [40/50] time 0.357 (0.388) data 0.000 (0.029) loss 2.5234 (2.7027) acc 50.0000 (38.2031) lr 9.3721e-04 eta 0:07:10\n",
            "epoch [28/50] batch [45/50] time 0.354 (0.384) data 0.000 (0.026) loss 2.3418 (2.7169) acc 37.5000 (37.9861) lr 9.3721e-04 eta 0:07:04\n",
            "epoch [28/50] batch [50/50] time 0.355 (0.381) data 0.000 (0.023) loss 2.2109 (2.7141) acc 46.8750 (37.8125) lr 8.7467e-04 eta 0:06:59\n",
            "epoch [29/50] batch [5/50] time 0.350 (0.553) data 0.001 (0.162) loss 2.8809 (2.8527) acc 37.5000 (34.3750) lr 8.7467e-04 eta 0:10:05\n",
            "epoch [29/50] batch [10/50] time 0.357 (0.454) data 0.001 (0.081) loss 2.7188 (2.8381) acc 37.5000 (35.6250) lr 8.7467e-04 eta 0:08:15\n",
            "epoch [29/50] batch [15/50] time 0.355 (0.422) data 0.001 (0.055) loss 2.3633 (2.8225) acc 43.7500 (36.4583) lr 8.7467e-04 eta 0:07:38\n",
            "epoch [29/50] batch [20/50] time 0.357 (0.406) data 0.001 (0.042) loss 2.7559 (2.7316) acc 31.2500 (38.4375) lr 8.7467e-04 eta 0:07:18\n",
            "epoch [29/50] batch [25/50] time 0.364 (0.397) data 0.010 (0.034) loss 2.6621 (2.7423) acc 37.5000 (37.8750) lr 8.7467e-04 eta 0:07:06\n",
            "epoch [29/50] batch [30/50] time 0.373 (0.392) data 0.013 (0.030) loss 2.7461 (2.7395) acc 40.6250 (37.6042) lr 8.7467e-04 eta 0:06:59\n",
            "epoch [29/50] batch [35/50] time 0.355 (0.387) data 0.001 (0.026) loss 2.8066 (2.7542) acc 28.1250 (37.3214) lr 8.7467e-04 eta 0:06:51\n",
            "epoch [29/50] batch [40/50] time 0.357 (0.383) data 0.000 (0.022) loss 2.1582 (2.7343) acc 56.2500 (37.8906) lr 8.7467e-04 eta 0:06:45\n",
            "epoch [29/50] batch [45/50] time 0.355 (0.380) data 0.000 (0.020) loss 2.7891 (2.7247) acc 37.5000 (38.1944) lr 8.7467e-04 eta 0:06:40\n",
            "epoch [29/50] batch [50/50] time 0.357 (0.377) data 0.000 (0.018) loss 2.5957 (2.7458) acc 37.5000 (37.8750) lr 8.1262e-04 eta 0:06:36\n",
            "epoch [30/50] batch [5/50] time 0.350 (0.593) data 0.001 (0.205) loss 3.0430 (2.6434) acc 21.8750 (37.5000) lr 8.1262e-04 eta 0:10:19\n",
            "epoch [30/50] batch [10/50] time 0.365 (0.478) data 0.011 (0.106) loss 2.7148 (2.6900) acc 34.3750 (37.8125) lr 8.1262e-04 eta 0:08:17\n",
            "epoch [30/50] batch [15/50] time 0.363 (0.439) data 0.006 (0.072) loss 2.9219 (2.6448) acc 21.8750 (38.9583) lr 8.1262e-04 eta 0:07:34\n",
            "epoch [30/50] batch [20/50] time 0.359 (0.420) data 0.001 (0.055) loss 2.7207 (2.6938) acc 37.5000 (38.5938) lr 8.1262e-04 eta 0:07:13\n",
            "epoch [30/50] batch [25/50] time 0.370 (0.410) data 0.010 (0.046) loss 3.1152 (2.7352) acc 28.1250 (37.2500) lr 8.1262e-04 eta 0:06:59\n",
            "epoch [30/50] batch [30/50] time 0.369 (0.402) data 0.007 (0.039) loss 3.0762 (2.7543) acc 37.5000 (36.7708) lr 8.1262e-04 eta 0:06:50\n",
            "epoch [30/50] batch [35/50] time 0.354 (0.396) data 0.001 (0.034) loss 3.1172 (2.7429) acc 21.8750 (37.3214) lr 8.1262e-04 eta 0:06:41\n",
            "epoch [30/50] batch [40/50] time 0.356 (0.391) data 0.000 (0.030) loss 3.3418 (2.7725) acc 25.0000 (37.1094) lr 8.1262e-04 eta 0:06:34\n",
            "epoch [30/50] batch [45/50] time 0.356 (0.387) data 0.000 (0.027) loss 2.3359 (2.7635) acc 43.7500 (37.0833) lr 8.1262e-04 eta 0:06:28\n",
            "epoch [30/50] batch [50/50] time 0.354 (0.384) data 0.000 (0.024) loss 2.7832 (2.7754) acc 34.3750 (36.7500) lr 7.5131e-04 eta 0:06:23\n",
            "epoch [31/50] batch [5/50] time 0.354 (0.581) data 0.001 (0.198) loss 2.9863 (2.8023) acc 31.2500 (32.5000) lr 7.5131e-04 eta 0:09:38\n",
            "epoch [31/50] batch [10/50] time 0.359 (0.471) data 0.001 (0.102) loss 2.8750 (2.7029) acc 40.6250 (34.6875) lr 7.5131e-04 eta 0:07:46\n",
            "epoch [31/50] batch [15/50] time 0.357 (0.435) data 0.001 (0.069) loss 2.8848 (2.6714) acc 43.7500 (36.8750) lr 7.5131e-04 eta 0:07:08\n",
            "epoch [31/50] batch [20/50] time 0.358 (0.416) data 0.001 (0.053) loss 2.6602 (2.6516) acc 34.3750 (37.0312) lr 7.5131e-04 eta 0:06:47\n",
            "epoch [31/50] batch [25/50] time 0.362 (0.405) data 0.004 (0.043) loss 3.0488 (2.7121) acc 37.5000 (36.6250) lr 7.5131e-04 eta 0:06:34\n",
            "epoch [31/50] batch [30/50] time 0.355 (0.397) data 0.000 (0.037) loss 2.7754 (2.6763) acc 34.3750 (38.1250) lr 7.5131e-04 eta 0:06:25\n",
            "epoch [31/50] batch [35/50] time 0.356 (0.391) data 0.001 (0.031) loss 3.0566 (2.6632) acc 21.8750 (38.1250) lr 7.5131e-04 eta 0:06:17\n",
            "epoch [31/50] batch [40/50] time 0.356 (0.387) data 0.001 (0.027) loss 2.8047 (2.6844) acc 31.2500 (37.3438) lr 7.5131e-04 eta 0:06:11\n",
            "epoch [31/50] batch [45/50] time 0.356 (0.383) data 0.000 (0.024) loss 2.5215 (2.6816) acc 37.5000 (37.9167) lr 7.5131e-04 eta 0:06:06\n",
            "epoch [31/50] batch [50/50] time 0.359 (0.381) data 0.000 (0.022) loss 2.9766 (2.6954) acc 31.2500 (37.4375) lr 6.9098e-04 eta 0:06:01\n",
            "epoch [32/50] batch [5/50] time 0.373 (0.600) data 0.000 (0.161) loss 2.5059 (2.5141) acc 43.7500 (43.1250) lr 6.9098e-04 eta 0:09:27\n",
            "epoch [32/50] batch [10/50] time 0.364 (0.482) data 0.008 (0.084) loss 1.8340 (2.4762) acc 53.1250 (42.8125) lr 6.9098e-04 eta 0:07:32\n",
            "epoch [32/50] batch [15/50] time 0.360 (0.442) data 0.001 (0.057) loss 3.0234 (2.5566) acc 25.0000 (41.0417) lr 6.9098e-04 eta 0:06:53\n",
            "epoch [32/50] batch [20/50] time 0.361 (0.422) data 0.006 (0.044) loss 2.8164 (2.6167) acc 25.0000 (40.3125) lr 6.9098e-04 eta 0:06:32\n",
            "epoch [32/50] batch [25/50] time 0.355 (0.409) data 0.001 (0.035) loss 2.8008 (2.6506) acc 28.1250 (39.2500) lr 6.9098e-04 eta 0:06:18\n",
            "epoch [32/50] batch [30/50] time 0.356 (0.400) data 0.001 (0.029) loss 2.7383 (2.6184) acc 37.5000 (39.7917) lr 6.9098e-04 eta 0:06:08\n",
            "epoch [32/50] batch [35/50] time 0.357 (0.394) data 0.001 (0.025) loss 2.6816 (2.6279) acc 34.3750 (39.1071) lr 6.9098e-04 eta 0:06:00\n",
            "epoch [32/50] batch [40/50] time 0.354 (0.389) data 0.000 (0.022) loss 2.9375 (2.6397) acc 31.2500 (38.7500) lr 6.9098e-04 eta 0:05:53\n",
            "epoch [32/50] batch [45/50] time 0.355 (0.385) data 0.000 (0.020) loss 3.2402 (2.6830) acc 31.2500 (38.4722) lr 6.9098e-04 eta 0:05:48\n",
            "epoch [32/50] batch [50/50] time 0.357 (0.382) data 0.000 (0.018) loss 2.4023 (2.6787) acc 56.2500 (38.4375) lr 6.3188e-04 eta 0:05:43\n",
            "epoch [33/50] batch [5/50] time 0.359 (0.741) data 0.007 (0.315) loss 3.1094 (2.6688) acc 21.8750 (38.7500) lr 6.3188e-04 eta 0:11:03\n",
            "epoch [33/50] batch [10/50] time 0.373 (0.553) data 0.007 (0.160) loss 2.4102 (2.7205) acc 43.7500 (36.5625) lr 6.3188e-04 eta 0:08:11\n",
            "epoch [33/50] batch [15/50] time 0.370 (0.491) data 0.007 (0.109) loss 2.7109 (2.6701) acc 40.6250 (37.9167) lr 6.3188e-04 eta 0:07:14\n",
            "epoch [33/50] batch [20/50] time 0.363 (0.458) data 0.004 (0.083) loss 2.2090 (2.5770) acc 53.1250 (40.6250) lr 6.3188e-04 eta 0:06:43\n",
            "epoch [33/50] batch [25/50] time 0.355 (0.439) data 0.001 (0.067) loss 2.7734 (2.6211) acc 34.3750 (39.8750) lr 6.3188e-04 eta 0:06:23\n",
            "epoch [33/50] batch [30/50] time 0.356 (0.425) data 0.001 (0.056) loss 2.7480 (2.6454) acc 40.6250 (39.4792) lr 6.3188e-04 eta 0:06:09\n",
            "epoch [33/50] batch [35/50] time 0.356 (0.415) data 0.001 (0.048) loss 2.4297 (2.6401) acc 40.6250 (39.4643) lr 6.3188e-04 eta 0:05:58\n",
            "epoch [33/50] batch [40/50] time 0.355 (0.407) data 0.000 (0.042) loss 2.8320 (2.6198) acc 50.0000 (40.2344) lr 6.3188e-04 eta 0:05:50\n",
            "epoch [33/50] batch [45/50] time 0.354 (0.402) data 0.001 (0.037) loss 2.2793 (2.6372) acc 50.0000 (39.9306) lr 6.3188e-04 eta 0:05:43\n",
            "epoch [33/50] batch [50/50] time 0.354 (0.397) data 0.001 (0.034) loss 2.5762 (2.6604) acc 37.5000 (39.6250) lr 5.7422e-04 eta 0:05:37\n",
            "epoch [34/50] batch [5/50] time 0.378 (0.761) data 0.021 (0.318) loss 2.3086 (2.4545) acc 43.7500 (43.7500) lr 5.7422e-04 eta 0:10:42\n",
            "epoch [34/50] batch [10/50] time 0.356 (0.559) data 0.001 (0.159) loss 3.0469 (2.6390) acc 31.2500 (39.0625) lr 5.7422e-04 eta 0:07:49\n",
            "epoch [34/50] batch [15/50] time 0.355 (0.491) data 0.001 (0.106) loss 3.3633 (2.7021) acc 18.7500 (38.1250) lr 5.7422e-04 eta 0:06:50\n",
            "epoch [34/50] batch [20/50] time 0.356 (0.457) data 0.000 (0.080) loss 3.4355 (2.6646) acc 28.1250 (38.9062) lr 5.7422e-04 eta 0:06:19\n",
            "epoch [34/50] batch [25/50] time 0.356 (0.437) data 0.000 (0.064) loss 3.2285 (2.7334) acc 37.5000 (37.5000) lr 5.7422e-04 eta 0:06:00\n",
            "epoch [34/50] batch [30/50] time 0.360 (0.424) data 0.000 (0.053) loss 3.3984 (2.7116) acc 21.8750 (37.7083) lr 5.7422e-04 eta 0:05:47\n",
            "epoch [34/50] batch [35/50] time 0.354 (0.414) data 0.001 (0.046) loss 2.5508 (2.7418) acc 40.6250 (37.2321) lr 5.7422e-04 eta 0:05:37\n",
            "epoch [34/50] batch [40/50] time 0.356 (0.407) data 0.000 (0.040) loss 2.7812 (2.7110) acc 34.3750 (38.0469) lr 5.7422e-04 eta 0:05:29\n",
            "epoch [34/50] batch [45/50] time 0.357 (0.401) data 0.004 (0.036) loss 2.8008 (2.6916) acc 37.5000 (38.1944) lr 5.7422e-04 eta 0:05:22\n",
            "epoch [34/50] batch [50/50] time 0.358 (0.397) data 0.000 (0.032) loss 2.2988 (2.6839) acc 43.7500 (37.8125) lr 5.1825e-04 eta 0:05:17\n",
            "epoch [35/50] batch [5/50] time 0.351 (0.594) data 0.001 (0.216) loss 1.9824 (2.3805) acc 62.5000 (48.1250) lr 5.1825e-04 eta 0:07:52\n",
            "epoch [35/50] batch [10/50] time 0.357 (0.475) data 0.001 (0.108) loss 3.0059 (2.5121) acc 34.3750 (44.3750) lr 5.1825e-04 eta 0:06:15\n",
            "epoch [35/50] batch [15/50] time 0.357 (0.435) data 0.002 (0.072) loss 2.3086 (2.5370) acc 37.5000 (43.1250) lr 5.1825e-04 eta 0:05:41\n",
            "epoch [35/50] batch [20/50] time 0.357 (0.415) data 0.002 (0.055) loss 2.3281 (2.5726) acc 46.8750 (42.3438) lr 5.1825e-04 eta 0:05:24\n",
            "epoch [35/50] batch [25/50] time 0.356 (0.404) data 0.001 (0.044) loss 2.4258 (2.5920) acc 37.5000 (40.5000) lr 5.1825e-04 eta 0:05:12\n",
            "epoch [35/50] batch [30/50] time 0.374 (0.397) data 0.008 (0.037) loss 2.6074 (2.6590) acc 50.0000 (38.8542) lr 5.1825e-04 eta 0:05:05\n",
            "epoch [35/50] batch [35/50] time 0.360 (0.392) data 0.003 (0.033) loss 3.4629 (2.6463) acc 21.8750 (39.0179) lr 5.1825e-04 eta 0:04:59\n",
            "epoch [35/50] batch [40/50] time 0.356 (0.388) data 0.000 (0.029) loss 1.7686 (2.6525) acc 59.3750 (38.6719) lr 5.1825e-04 eta 0:04:54\n",
            "epoch [35/50] batch [45/50] time 0.357 (0.384) data 0.001 (0.025) loss 2.8184 (2.6578) acc 40.6250 (38.7500) lr 5.1825e-04 eta 0:04:50\n",
            "epoch [35/50] batch [50/50] time 0.354 (0.381) data 0.000 (0.023) loss 2.6035 (2.6686) acc 34.3750 (38.1875) lr 4.6417e-04 eta 0:04:45\n",
            "epoch [36/50] batch [5/50] time 0.352 (0.611) data 0.001 (0.234) loss 2.0742 (2.5336) acc 56.2500 (42.5000) lr 4.6417e-04 eta 0:07:35\n",
            "epoch [36/50] batch [10/50] time 0.357 (0.484) data 0.001 (0.117) loss 2.7520 (2.6686) acc 37.5000 (38.1250) lr 4.6417e-04 eta 0:05:58\n",
            "epoch [36/50] batch [15/50] time 0.356 (0.441) data 0.001 (0.078) loss 2.7715 (2.6977) acc 43.7500 (38.9583) lr 4.6417e-04 eta 0:05:24\n",
            "epoch [36/50] batch [20/50] time 0.355 (0.420) data 0.000 (0.059) loss 2.4824 (2.6860) acc 50.0000 (39.6875) lr 4.6417e-04 eta 0:05:06\n",
            "epoch [36/50] batch [25/50] time 0.362 (0.408) data 0.007 (0.048) loss 2.6348 (2.6862) acc 37.5000 (39.2500) lr 4.6417e-04 eta 0:04:55\n",
            "epoch [36/50] batch [30/50] time 0.371 (0.400) data 0.015 (0.041) loss 1.8340 (2.6721) acc 50.0000 (39.0625) lr 4.6417e-04 eta 0:04:48\n",
            "epoch [36/50] batch [35/50] time 0.363 (0.396) data 0.006 (0.036) loss 1.9766 (2.6992) acc 59.3750 (38.3929) lr 4.6417e-04 eta 0:04:43\n",
            "epoch [36/50] batch [40/50] time 0.356 (0.391) data 0.001 (0.032) loss 2.6523 (2.7275) acc 37.5000 (37.7344) lr 4.6417e-04 eta 0:04:37\n",
            "epoch [36/50] batch [45/50] time 0.356 (0.387) data 0.001 (0.029) loss 2.6855 (2.7310) acc 43.7500 (37.8472) lr 4.6417e-04 eta 0:04:33\n",
            "epoch [36/50] batch [50/50] time 0.355 (0.384) data 0.000 (0.026) loss 2.2637 (2.7321) acc 53.1250 (37.6250) lr 4.1221e-04 eta 0:04:28\n",
            "epoch [37/50] batch [5/50] time 0.353 (0.533) data 0.000 (0.145) loss 2.4531 (2.4730) acc 43.7500 (46.8750) lr 4.1221e-04 eta 0:06:10\n",
            "epoch [37/50] batch [10/50] time 0.357 (0.445) data 0.001 (0.073) loss 3.0312 (2.6219) acc 28.1250 (40.6250) lr 4.1221e-04 eta 0:05:06\n",
            "epoch [37/50] batch [15/50] time 0.356 (0.415) data 0.001 (0.049) loss 2.9668 (2.7079) acc 37.5000 (38.1250) lr 4.1221e-04 eta 0:04:44\n",
            "epoch [37/50] batch [20/50] time 0.354 (0.400) data 0.001 (0.037) loss 2.2305 (2.6603) acc 50.0000 (39.3750) lr 4.1221e-04 eta 0:04:32\n",
            "epoch [37/50] batch [25/50] time 0.357 (0.391) data 0.001 (0.030) loss 2.7422 (2.6618) acc 34.3750 (38.8750) lr 4.1221e-04 eta 0:04:24\n",
            "epoch [37/50] batch [30/50] time 0.362 (0.387) data 0.001 (0.025) loss 2.5469 (2.6307) acc 37.5000 (39.2708) lr 4.1221e-04 eta 0:04:19\n",
            "epoch [37/50] batch [35/50] time 0.355 (0.384) data 0.001 (0.023) loss 2.7383 (2.6259) acc 43.7500 (39.7321) lr 4.1221e-04 eta 0:04:15\n",
            "epoch [37/50] batch [40/50] time 0.356 (0.380) data 0.001 (0.020) loss 2.9668 (2.6495) acc 37.5000 (39.1406) lr 4.1221e-04 eta 0:04:10\n",
            "epoch [37/50] batch [45/50] time 0.354 (0.377) data 0.000 (0.018) loss 2.8496 (2.6775) acc 37.5000 (38.6806) lr 4.1221e-04 eta 0:04:07\n",
            "epoch [37/50] batch [50/50] time 0.354 (0.375) data 0.000 (0.016) loss 3.0059 (2.6702) acc 37.5000 (38.4375) lr 3.6258e-04 eta 0:04:03\n",
            "epoch [38/50] batch [5/50] time 0.351 (0.581) data 0.001 (0.214) loss 2.5566 (2.6926) acc 31.2500 (34.3750) lr 3.6258e-04 eta 0:06:14\n",
            "epoch [38/50] batch [10/50] time 0.359 (0.469) data 0.001 (0.107) loss 2.6758 (2.6301) acc 31.2500 (36.5625) lr 3.6258e-04 eta 0:05:00\n",
            "epoch [38/50] batch [15/50] time 0.355 (0.432) data 0.001 (0.072) loss 3.0078 (2.6522) acc 34.3750 (38.1250) lr 3.6258e-04 eta 0:04:34\n",
            "epoch [38/50] batch [20/50] time 0.359 (0.414) data 0.004 (0.055) loss 3.1582 (2.6874) acc 28.1250 (37.6562) lr 3.6258e-04 eta 0:04:20\n",
            "epoch [38/50] batch [25/50] time 0.363 (0.404) data 0.006 (0.045) loss 2.5000 (2.6461) acc 40.6250 (38.2500) lr 3.6258e-04 eta 0:04:12\n",
            "epoch [38/50] batch [30/50] time 0.360 (0.396) data 0.001 (0.037) loss 2.4102 (2.6292) acc 43.7500 (39.1667) lr 3.6258e-04 eta 0:04:05\n",
            "epoch [38/50] batch [35/50] time 0.355 (0.391) data 0.001 (0.033) loss 2.9102 (2.5956) acc 43.7500 (40.0893) lr 3.6258e-04 eta 0:04:00\n",
            "epoch [38/50] batch [40/50] time 0.355 (0.387) data 0.001 (0.029) loss 2.7383 (2.5978) acc 40.6250 (40.3125) lr 3.6258e-04 eta 0:03:55\n",
            "epoch [38/50] batch [45/50] time 0.355 (0.383) data 0.001 (0.025) loss 2.6777 (2.5798) acc 40.6250 (40.6250) lr 3.6258e-04 eta 0:03:51\n",
            "epoch [38/50] batch [50/50] time 0.356 (0.381) data 0.000 (0.023) loss 2.9590 (2.6003) acc 28.1250 (40.3125) lr 3.1545e-04 eta 0:03:48\n",
            "epoch [39/50] batch [5/50] time 0.350 (0.603) data 0.001 (0.232) loss 2.0547 (2.3680) acc 53.1250 (49.3750) lr 3.1545e-04 eta 0:05:58\n",
            "epoch [39/50] batch [10/50] time 0.364 (0.481) data 0.007 (0.117) loss 2.5195 (2.4218) acc 37.5000 (42.5000) lr 3.1545e-04 eta 0:04:43\n",
            "epoch [39/50] batch [15/50] time 0.356 (0.439) data 0.001 (0.078) loss 1.8740 (2.4833) acc 50.0000 (40.4167) lr 3.1545e-04 eta 0:04:16\n",
            "epoch [39/50] batch [20/50] time 0.355 (0.418) data 0.001 (0.059) loss 2.4551 (2.4630) acc 40.6250 (40.9375) lr 3.1545e-04 eta 0:04:02\n",
            "epoch [39/50] batch [25/50] time 0.360 (0.407) data 0.005 (0.048) loss 2.7344 (2.5604) acc 50.0000 (39.7500) lr 3.1545e-04 eta 0:03:53\n",
            "epoch [39/50] batch [30/50] time 0.364 (0.400) data 0.008 (0.041) loss 2.8184 (2.6115) acc 43.7500 (39.4792) lr 3.1545e-04 eta 0:03:47\n",
            "epoch [39/50] batch [35/50] time 0.357 (0.394) data 0.001 (0.036) loss 2.2207 (2.6097) acc 56.2500 (39.8214) lr 3.1545e-04 eta 0:03:42\n",
            "epoch [39/50] batch [40/50] time 0.355 (0.390) data 0.001 (0.032) loss 2.8652 (2.6209) acc 40.6250 (39.5312) lr 3.1545e-04 eta 0:03:38\n",
            "epoch [39/50] batch [45/50] time 0.356 (0.386) data 0.000 (0.028) loss 2.6328 (2.6248) acc 37.5000 (39.3750) lr 3.1545e-04 eta 0:03:34\n",
            "epoch [39/50] batch [50/50] time 0.356 (0.383) data 0.000 (0.025) loss 2.9512 (2.6437) acc 34.3750 (38.5625) lr 2.7103e-04 eta 0:03:30\n",
            "epoch [40/50] batch [5/50] time 0.357 (0.714) data 0.001 (0.272) loss 2.0098 (2.5363) acc 53.1250 (39.3750) lr 2.7103e-04 eta 0:06:29\n",
            "epoch [40/50] batch [10/50] time 0.358 (0.536) data 0.001 (0.136) loss 2.7773 (2.5135) acc 46.8750 (41.5625) lr 2.7103e-04 eta 0:04:49\n",
            "epoch [40/50] batch [15/50] time 0.361 (0.476) data 0.002 (0.091) loss 2.2031 (2.5135) acc 50.0000 (42.7083) lr 2.7103e-04 eta 0:04:14\n",
            "epoch [40/50] batch [20/50] time 0.359 (0.447) data 0.001 (0.069) loss 1.9541 (2.4932) acc 46.8750 (43.4375) lr 2.7103e-04 eta 0:03:56\n",
            "epoch [40/50] batch [25/50] time 0.367 (0.430) data 0.008 (0.056) loss 2.2969 (2.5656) acc 43.7500 (41.5000) lr 2.7103e-04 eta 0:03:45\n",
            "epoch [40/50] batch [30/50] time 0.357 (0.418) data 0.001 (0.047) loss 2.5879 (2.5699) acc 40.6250 (41.3542) lr 2.7103e-04 eta 0:03:37\n",
            "epoch [40/50] batch [35/50] time 0.356 (0.410) data 0.001 (0.042) loss 2.6699 (2.5736) acc 40.6250 (40.9821) lr 2.7103e-04 eta 0:03:31\n",
            "epoch [40/50] batch [40/50] time 0.355 (0.403) data 0.000 (0.036) loss 2.3027 (2.5459) acc 50.0000 (41.5625) lr 2.7103e-04 eta 0:03:25\n",
            "epoch [40/50] batch [45/50] time 0.356 (0.398) data 0.000 (0.032) loss 2.3789 (2.5622) acc 40.6250 (41.4583) lr 2.7103e-04 eta 0:03:21\n",
            "epoch [40/50] batch [50/50] time 0.354 (0.394) data 0.000 (0.029) loss 2.8574 (2.5656) acc 31.2500 (41.4375) lr 2.2949e-04 eta 0:03:16\n",
            "epoch [41/50] batch [5/50] time 0.356 (0.555) data 0.001 (0.161) loss 2.1758 (2.5119) acc 50.0000 (36.8750) lr 2.2949e-04 eta 0:04:34\n",
            "epoch [41/50] batch [10/50] time 0.356 (0.456) data 0.001 (0.081) loss 2.3457 (2.4745) acc 40.6250 (39.0625) lr 2.2949e-04 eta 0:03:43\n",
            "epoch [41/50] batch [15/50] time 0.359 (0.424) data 0.003 (0.055) loss 2.7305 (2.4277) acc 40.6250 (40.8333) lr 2.2949e-04 eta 0:03:25\n",
            "epoch [41/50] batch [20/50] time 0.358 (0.408) data 0.001 (0.042) loss 3.0801 (2.4981) acc 31.2500 (40.3125) lr 2.2949e-04 eta 0:03:15\n",
            "epoch [41/50] batch [25/50] time 0.368 (0.400) data 0.009 (0.035) loss 3.1348 (2.5871) acc 25.0000 (38.1250) lr 2.2949e-04 eta 0:03:09\n",
            "epoch [41/50] batch [30/50] time 0.368 (0.393) data 0.012 (0.030) loss 2.5215 (2.5982) acc 37.5000 (38.4375) lr 2.2949e-04 eta 0:03:04\n",
            "epoch [41/50] batch [35/50] time 0.357 (0.389) data 0.001 (0.026) loss 2.6543 (2.5820) acc 40.6250 (39.5536) lr 2.2949e-04 eta 0:03:00\n",
            "epoch [41/50] batch [40/50] time 0.356 (0.385) data 0.000 (0.023) loss 2.6367 (2.6101) acc 37.5000 (39.1406) lr 2.2949e-04 eta 0:02:56\n",
            "epoch [41/50] batch [45/50] time 0.355 (0.381) data 0.000 (0.020) loss 2.6211 (2.5876) acc 34.3750 (39.7917) lr 2.2949e-04 eta 0:02:53\n",
            "epoch [41/50] batch [50/50] time 0.356 (0.379) data 0.000 (0.018) loss 2.8398 (2.5920) acc 31.2500 (39.8125) lr 1.9098e-04 eta 0:02:50\n",
            "epoch [42/50] batch [5/50] time 0.351 (0.569) data 0.001 (0.176) loss 2.4453 (2.8883) acc 40.6250 (35.6250) lr 1.9098e-04 eta 0:04:13\n",
            "epoch [42/50] batch [10/50] time 0.368 (0.464) data 0.007 (0.089) loss 2.2676 (2.7967) acc 40.6250 (34.6875) lr 1.9098e-04 eta 0:03:24\n",
            "epoch [42/50] batch [15/50] time 0.357 (0.429) data 0.001 (0.060) loss 1.9785 (2.6743) acc 53.1250 (37.7083) lr 1.9098e-04 eta 0:03:06\n",
            "epoch [42/50] batch [20/50] time 0.367 (0.413) data 0.013 (0.047) loss 2.9004 (2.7038) acc 46.8750 (37.5000) lr 1.9098e-04 eta 0:02:57\n",
            "epoch [42/50] batch [25/50] time 0.367 (0.404) data 0.008 (0.039) loss 2.4355 (2.6592) acc 43.7500 (37.1250) lr 1.9098e-04 eta 0:02:51\n",
            "epoch [42/50] batch [30/50] time 0.355 (0.397) data 0.001 (0.033) loss 3.2305 (2.6844) acc 31.2500 (37.1875) lr 1.9098e-04 eta 0:02:46\n",
            "epoch [42/50] batch [35/50] time 0.356 (0.391) data 0.001 (0.029) loss 2.1270 (2.6239) acc 40.6250 (38.3929) lr 1.9098e-04 eta 0:02:42\n",
            "epoch [42/50] batch [40/50] time 0.356 (0.386) data 0.000 (0.025) loss 2.6387 (2.6250) acc 40.6250 (38.9062) lr 1.9098e-04 eta 0:02:38\n",
            "epoch [42/50] batch [45/50] time 0.354 (0.383) data 0.000 (0.022) loss 2.7207 (2.6266) acc 37.5000 (39.0278) lr 1.9098e-04 eta 0:02:35\n",
            "epoch [42/50] batch [50/50] time 0.355 (0.380) data 0.000 (0.020) loss 2.9180 (2.6250) acc 40.6250 (39.3750) lr 1.5567e-04 eta 0:02:31\n",
            "epoch [43/50] batch [5/50] time 0.356 (0.537) data 0.000 (0.150) loss 2.1641 (2.3066) acc 50.0000 (43.7500) lr 1.5567e-04 eta 0:03:32\n",
            "epoch [43/50] batch [10/50] time 0.364 (0.452) data 0.008 (0.078) loss 2.6523 (2.4630) acc 40.6250 (42.5000) lr 1.5567e-04 eta 0:02:56\n",
            "epoch [43/50] batch [15/50] time 0.365 (0.423) data 0.011 (0.054) loss 3.1230 (2.4475) acc 18.7500 (43.1250) lr 1.5567e-04 eta 0:02:42\n",
            "epoch [43/50] batch [20/50] time 0.361 (0.409) data 0.003 (0.043) loss 2.6855 (2.4823) acc 40.6250 (42.8125) lr 1.5567e-04 eta 0:02:35\n",
            "epoch [43/50] batch [25/50] time 0.361 (0.400) data 0.007 (0.035) loss 1.8057 (2.4805) acc 40.6250 (42.3750) lr 1.5567e-04 eta 0:02:30\n",
            "epoch [43/50] batch [30/50] time 0.355 (0.393) data 0.000 (0.030) loss 2.6953 (2.5468) acc 40.6250 (41.2500) lr 1.5567e-04 eta 0:02:25\n",
            "epoch [43/50] batch [35/50] time 0.353 (0.388) data 0.001 (0.026) loss 3.1953 (2.5563) acc 31.2500 (40.9821) lr 1.5567e-04 eta 0:02:21\n",
            "epoch [43/50] batch [40/50] time 0.357 (0.384) data 0.000 (0.023) loss 2.9512 (2.5671) acc 37.5000 (40.7031) lr 1.5567e-04 eta 0:02:18\n",
            "epoch [43/50] batch [45/50] time 0.355 (0.381) data 0.001 (0.020) loss 2.3594 (2.5562) acc 31.2500 (40.6944) lr 1.5567e-04 eta 0:02:15\n",
            "epoch [43/50] batch [50/50] time 0.356 (0.378) data 0.000 (0.018) loss 2.4766 (2.5925) acc 40.6250 (39.8125) lr 1.2369e-04 eta 0:02:12\n",
            "epoch [44/50] batch [5/50] time 0.360 (0.589) data 0.008 (0.210) loss 2.4492 (2.6008) acc 40.6250 (43.1250) lr 1.2369e-04 eta 0:03:23\n",
            "epoch [44/50] batch [10/50] time 0.365 (0.476) data 0.008 (0.108) loss 2.1465 (2.6504) acc 53.1250 (42.5000) lr 1.2369e-04 eta 0:02:41\n",
            "epoch [44/50] batch [15/50] time 0.366 (0.438) data 0.008 (0.074) loss 3.0000 (2.6693) acc 37.5000 (41.0417) lr 1.2369e-04 eta 0:02:26\n",
            "epoch [44/50] batch [20/50] time 0.356 (0.419) data 0.001 (0.056) loss 2.6855 (2.6682) acc 34.3750 (40.3125) lr 1.2369e-04 eta 0:02:18\n",
            "epoch [44/50] batch [25/50] time 0.353 (0.407) data 0.001 (0.046) loss 2.1309 (2.6201) acc 50.0000 (41.2500) lr 1.2369e-04 eta 0:02:12\n",
            "epoch [44/50] batch [30/50] time 0.355 (0.399) data 0.000 (0.038) loss 2.4902 (2.5867) acc 34.3750 (41.2500) lr 1.2369e-04 eta 0:02:07\n",
            "epoch [44/50] batch [35/50] time 0.355 (0.392) data 0.001 (0.033) loss 2.3867 (2.5674) acc 40.6250 (41.6071) lr 1.2369e-04 eta 0:02:03\n",
            "epoch [44/50] batch [40/50] time 0.354 (0.388) data 0.000 (0.029) loss 2.5234 (2.5718) acc 40.6250 (41.4062) lr 1.2369e-04 eta 0:02:00\n",
            "epoch [44/50] batch [45/50] time 0.354 (0.384) data 0.000 (0.026) loss 2.8262 (2.5747) acc 40.6250 (41.6667) lr 1.2369e-04 eta 0:01:57\n",
            "epoch [44/50] batch [50/50] time 0.355 (0.381) data 0.000 (0.023) loss 2.9297 (2.5663) acc 34.3750 (41.8750) lr 9.5173e-05 eta 0:01:54\n",
            "epoch [45/50] batch [5/50] time 0.356 (0.739) data 0.001 (0.301) loss 2.8926 (2.4355) acc 40.6250 (48.1250) lr 9.5173e-05 eta 0:03:37\n",
            "epoch [45/50] batch [10/50] time 0.361 (0.553) data 0.003 (0.155) loss 2.4453 (2.5350) acc 50.0000 (45.9375) lr 9.5173e-05 eta 0:02:40\n",
            "epoch [45/50] batch [15/50] time 0.361 (0.489) data 0.001 (0.104) loss 2.3691 (2.4719) acc 43.7500 (44.5833) lr 9.5173e-05 eta 0:02:19\n",
            "epoch [45/50] batch [20/50] time 0.368 (0.457) data 0.013 (0.079) loss 2.6973 (2.5406) acc 37.5000 (42.8125) lr 9.5173e-05 eta 0:02:08\n",
            "epoch [45/50] batch [25/50] time 0.356 (0.438) data 0.001 (0.064) loss 2.3438 (2.5832) acc 40.6250 (41.7500) lr 9.5173e-05 eta 0:02:00\n",
            "epoch [45/50] batch [30/50] time 0.355 (0.424) data 0.001 (0.054) loss 2.0723 (2.5622) acc 50.0000 (41.7708) lr 9.5173e-05 eta 0:01:54\n",
            "epoch [45/50] batch [35/50] time 0.357 (0.415) data 0.001 (0.046) loss 2.5293 (2.5429) acc 40.6250 (41.5179) lr 9.5173e-05 eta 0:01:49\n",
            "epoch [45/50] batch [40/50] time 0.355 (0.407) data 0.000 (0.040) loss 2.8809 (2.5358) acc 28.1250 (41.9531) lr 9.5173e-05 eta 0:01:45\n",
            "epoch [45/50] batch [45/50] time 0.357 (0.401) data 0.000 (0.036) loss 2.3105 (2.5492) acc 37.5000 (41.3194) lr 9.5173e-05 eta 0:01:42\n",
            "epoch [45/50] batch [50/50] time 0.357 (0.397) data 0.000 (0.032) loss 2.0645 (2.5357) acc 43.7500 (41.2500) lr 7.0224e-05 eta 0:01:39\n",
            "epoch [46/50] batch [5/50] time 0.361 (0.740) data 0.001 (0.259) loss 3.1367 (2.8074) acc 31.2500 (33.7500) lr 7.0224e-05 eta 0:03:01\n",
            "epoch [46/50] batch [10/50] time 0.362 (0.551) data 0.006 (0.131) loss 2.3203 (2.6893) acc 46.8750 (38.7500) lr 7.0224e-05 eta 0:02:12\n",
            "epoch [46/50] batch [15/50] time 0.366 (0.487) data 0.006 (0.088) loss 2.3809 (2.6589) acc 50.0000 (38.5417) lr 7.0224e-05 eta 0:01:54\n",
            "epoch [46/50] batch [20/50] time 0.356 (0.455) data 0.002 (0.066) loss 2.8594 (2.5943) acc 34.3750 (39.6875) lr 7.0224e-05 eta 0:01:44\n",
            "epoch [46/50] batch [25/50] time 0.354 (0.435) data 0.000 (0.053) loss 2.1855 (2.5703) acc 53.1250 (40.2500) lr 7.0224e-05 eta 0:01:37\n",
            "epoch [46/50] batch [30/50] time 0.358 (0.422) data 0.001 (0.045) loss 2.6406 (2.5571) acc 43.7500 (40.5208) lr 7.0224e-05 eta 0:01:32\n",
            "epoch [46/50] batch [35/50] time 0.358 (0.413) data 0.001 (0.038) loss 2.8223 (2.5635) acc 37.5000 (39.9107) lr 7.0224e-05 eta 0:01:28\n",
            "epoch [46/50] batch [40/50] time 0.357 (0.406) data 0.000 (0.034) loss 2.5820 (2.5574) acc 46.8750 (40.3125) lr 7.0224e-05 eta 0:01:25\n",
            "epoch [46/50] batch [45/50] time 0.358 (0.400) data 0.001 (0.030) loss 2.6133 (2.5821) acc 40.6250 (40.3472) lr 7.0224e-05 eta 0:01:22\n",
            "epoch [46/50] batch [50/50] time 0.356 (0.396) data 0.001 (0.027) loss 3.0020 (2.5956) acc 34.3750 (40.3750) lr 4.8943e-05 eta 0:01:19\n",
            "epoch [47/50] batch [5/50] time 0.351 (0.772) data 0.001 (0.370) loss 3.2227 (2.7820) acc 34.3750 (38.1250) lr 4.8943e-05 eta 0:02:30\n",
            "epoch [47/50] batch [10/50] time 0.357 (0.564) data 0.001 (0.185) loss 2.2422 (2.6100) acc 43.7500 (40.0000) lr 4.8943e-05 eta 0:01:47\n",
            "epoch [47/50] batch [15/50] time 0.356 (0.495) data 0.001 (0.124) loss 1.9248 (2.6165) acc 56.2500 (40.6250) lr 4.8943e-05 eta 0:01:31\n",
            "epoch [47/50] batch [20/50] time 0.355 (0.460) data 0.001 (0.093) loss 2.4453 (2.5676) acc 40.6250 (41.4062) lr 4.8943e-05 eta 0:01:22\n",
            "epoch [47/50] batch [25/50] time 0.355 (0.439) data 0.001 (0.075) loss 2.5254 (2.5763) acc 34.3750 (40.7500) lr 4.8943e-05 eta 0:01:16\n",
            "epoch [47/50] batch [30/50] time 0.366 (0.426) data 0.011 (0.063) loss 2.2734 (2.5530) acc 37.5000 (41.6667) lr 4.8943e-05 eta 0:01:12\n",
            "epoch [47/50] batch [35/50] time 0.355 (0.417) data 0.001 (0.055) loss 2.5957 (2.5421) acc 40.6250 (41.6964) lr 4.8943e-05 eta 0:01:08\n",
            "epoch [47/50] batch [40/50] time 0.358 (0.409) data 0.001 (0.048) loss 2.8613 (2.5382) acc 43.7500 (41.9531) lr 4.8943e-05 eta 0:01:05\n",
            "epoch [47/50] batch [45/50] time 0.356 (0.403) data 0.001 (0.043) loss 2.6348 (2.5237) acc 43.7500 (42.2222) lr 4.8943e-05 eta 0:01:02\n",
            "epoch [47/50] batch [50/50] time 0.357 (0.399) data 0.001 (0.039) loss 3.2480 (2.5554) acc 21.8750 (41.3750) lr 3.1417e-05 eta 0:00:59\n",
            "epoch [48/50] batch [5/50] time 0.350 (0.581) data 0.000 (0.206) loss 2.5820 (2.5676) acc 43.7500 (43.7500) lr 3.1417e-05 eta 0:01:24\n",
            "epoch [48/50] batch [10/50] time 0.356 (0.468) data 0.001 (0.103) loss 2.5020 (2.5836) acc 43.7500 (41.8750) lr 3.1417e-05 eta 0:01:05\n",
            "epoch [48/50] batch [15/50] time 0.356 (0.431) data 0.001 (0.069) loss 2.5469 (2.5573) acc 34.3750 (41.6667) lr 3.1417e-05 eta 0:00:58\n",
            "epoch [48/50] batch [20/50] time 0.362 (0.412) data 0.003 (0.052) loss 2.3379 (2.5866) acc 37.5000 (40.4688) lr 3.1417e-05 eta 0:00:53\n",
            "epoch [48/50] batch [25/50] time 0.354 (0.403) data 0.001 (0.042) loss 2.5371 (2.5505) acc 31.2500 (41.3750) lr 3.1417e-05 eta 0:00:50\n",
            "epoch [48/50] batch [30/50] time 0.355 (0.395) data 0.001 (0.035) loss 2.6074 (2.5975) acc 46.8750 (40.6250) lr 3.1417e-05 eta 0:00:47\n",
            "epoch [48/50] batch [35/50] time 0.356 (0.390) data 0.001 (0.031) loss 2.9941 (2.6242) acc 34.3750 (40.0893) lr 3.1417e-05 eta 0:00:44\n",
            "epoch [48/50] batch [40/50] time 0.353 (0.386) data 0.001 (0.027) loss 3.5000 (2.6400) acc 18.7500 (38.7500) lr 3.1417e-05 eta 0:00:42\n",
            "epoch [48/50] batch [45/50] time 0.357 (0.383) data 0.001 (0.024) loss 2.6250 (2.6183) acc 46.8750 (38.8889) lr 3.1417e-05 eta 0:00:40\n",
            "epoch [48/50] batch [50/50] time 0.355 (0.380) data 0.000 (0.022) loss 2.2656 (2.5978) acc 46.8750 (39.3750) lr 1.7713e-05 eta 0:00:37\n",
            "epoch [49/50] batch [5/50] time 0.355 (0.598) data 0.001 (0.225) loss 2.5527 (2.6184) acc 50.0000 (42.5000) lr 1.7713e-05 eta 0:00:56\n",
            "epoch [49/50] batch [10/50] time 0.354 (0.477) data 0.001 (0.113) loss 2.1113 (2.4945) acc 50.0000 (44.0625) lr 1.7713e-05 eta 0:00:42\n",
            "epoch [49/50] batch [15/50] time 0.357 (0.437) data 0.001 (0.076) loss 2.2090 (2.5195) acc 50.0000 (42.9167) lr 1.7713e-05 eta 0:00:37\n",
            "epoch [49/50] batch [20/50] time 0.355 (0.417) data 0.001 (0.057) loss 2.2461 (2.5440) acc 43.7500 (42.0312) lr 1.7713e-05 eta 0:00:33\n",
            "epoch [49/50] batch [25/50] time 0.361 (0.406) data 0.001 (0.046) loss 2.5820 (2.5794) acc 43.7500 (40.2500) lr 1.7713e-05 eta 0:00:30\n",
            "epoch [49/50] batch [30/50] time 0.360 (0.398) data 0.001 (0.039) loss 2.3926 (2.5729) acc 37.5000 (40.4167) lr 1.7713e-05 eta 0:00:27\n",
            "epoch [49/50] batch [35/50] time 0.356 (0.393) data 0.001 (0.034) loss 2.7598 (2.5813) acc 34.3750 (40.6250) lr 1.7713e-05 eta 0:00:25\n",
            "epoch [49/50] batch [40/50] time 0.356 (0.388) data 0.001 (0.030) loss 2.5234 (2.6060) acc 37.5000 (40.0000) lr 1.7713e-05 eta 0:00:23\n",
            "epoch [49/50] batch [45/50] time 0.358 (0.385) data 0.001 (0.026) loss 2.2441 (2.5891) acc 43.7500 (40.3472) lr 1.7713e-05 eta 0:00:21\n",
            "epoch [49/50] batch [50/50] time 0.357 (0.382) data 0.000 (0.024) loss 2.7344 (2.5758) acc 37.5000 (40.3125) lr 7.8853e-06 eta 0:00:19\n",
            "epoch [50/50] batch [5/50] time 0.357 (0.581) data 0.007 (0.185) loss 2.6289 (2.3998) acc 34.3750 (44.3750) lr 7.8853e-06 eta 0:00:26\n",
            "epoch [50/50] batch [10/50] time 0.357 (0.469) data 0.001 (0.093) loss 2.1875 (2.5058) acc 53.1250 (43.7500) lr 7.8853e-06 eta 0:00:18\n",
            "epoch [50/50] batch [15/50] time 0.356 (0.432) data 0.001 (0.062) loss 2.6094 (2.4697) acc 46.8750 (43.5417) lr 7.8853e-06 eta 0:00:15\n",
            "epoch [50/50] batch [20/50] time 0.356 (0.413) data 0.001 (0.047) loss 2.1602 (2.4415) acc 50.0000 (43.7500) lr 7.8853e-06 eta 0:00:12\n",
            "epoch [50/50] batch [25/50] time 0.358 (0.402) data 0.001 (0.039) loss 2.2207 (2.4841) acc 50.0000 (44.0000) lr 7.8853e-06 eta 0:00:10\n",
            "epoch [50/50] batch [30/50] time 0.360 (0.395) data 0.003 (0.032) loss 2.2812 (2.5025) acc 50.0000 (43.8542) lr 7.8853e-06 eta 0:00:07\n",
            "epoch [50/50] batch [35/50] time 0.358 (0.390) data 0.001 (0.028) loss 2.6719 (2.5225) acc 37.5000 (43.0357) lr 7.8853e-06 eta 0:00:05\n",
            "epoch [50/50] batch [40/50] time 0.356 (0.386) data 0.001 (0.025) loss 2.5078 (2.5221) acc 46.8750 (42.9688) lr 7.8853e-06 eta 0:00:03\n",
            "epoch [50/50] batch [45/50] time 0.357 (0.383) data 0.000 (0.022) loss 2.0430 (2.5153) acc 50.0000 (43.0556) lr 7.8853e-06 eta 0:00:01\n",
            "epoch [50/50] batch [50/50] time 0.355 (0.380) data 0.000 (0.020) loss 2.2363 (2.5329) acc 46.8750 (42.9375) lr 1.9733e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed3/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:35<00:00,  2.80it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 50\n",
            "* accuracy: 0.5%\n",
            "* error: 99.5%\n",
            "* macro_f1: 1.0%\n",
            "Elapsed: 0:16:50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIP + CoOp (M=16, end) + 1 shot + CSC\n",
        "!bash scripts/coop/main.sh imagenet rn50_ep50 end 16 1 True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRDkaQCCqTxP",
        "outputId": "17ce57aa-870d-439b-e350-a9450d127f1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'True', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed1\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: True\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Loading preprocessed few-shot data from /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_1-seed_1.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  200\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Building custom CLIP\n",
            "Initializing class-specific contexts\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed1/tensorboard)\n",
            "epoch [1/50] batch [5/6] time 0.334 (0.891) data 0.000 (0.173) loss 4.7266 (5.1484) acc 6.2500 (6.8750) lr 1.0000e-05 eta 0:04:22\n",
            "epoch [2/50] batch [5/6] time 0.338 (0.574) data 0.001 (0.237) loss 5.1680 (5.0156) acc 9.3750 (8.7500) lr 2.0000e-03 eta 0:02:45\n",
            "epoch [3/50] batch [5/6] time 0.336 (0.504) data 0.000 (0.167) loss 4.5742 (4.3074) acc 21.8750 (20.6250) lr 1.9980e-03 eta 0:02:22\n",
            "epoch [4/50] batch [5/6] time 0.331 (0.450) data 0.000 (0.105) loss 4.3516 (3.6512) acc 21.8750 (25.6250) lr 1.9921e-03 eta 0:02:04\n",
            "epoch [5/50] batch [5/6] time 0.338 (0.473) data 0.000 (0.135) loss 3.2285 (2.9227) acc 43.7500 (43.7500) lr 1.9823e-03 eta 0:02:08\n",
            "epoch [6/50] batch [5/6] time 0.340 (0.469) data 0.001 (0.127) loss 2.8594 (2.5520) acc 37.5000 (45.6250) lr 1.9686e-03 eta 0:02:04\n",
            "epoch [7/50] batch [5/6] time 0.339 (0.583) data 0.001 (0.245) loss 2.1504 (1.9035) acc 50.0000 (64.3750) lr 1.9511e-03 eta 0:02:31\n",
            "epoch [8/50] batch [5/6] time 0.337 (0.488) data 0.000 (0.150) loss 2.2070 (1.9992) acc 68.7500 (67.5000) lr 1.9298e-03 eta 0:02:03\n",
            "epoch [9/50] batch [5/6] time 0.340 (0.479) data 0.000 (0.138) loss 1.9971 (1.6359) acc 59.3750 (65.0000) lr 1.9048e-03 eta 0:01:58\n",
            "epoch [10/50] batch [5/6] time 0.339 (0.475) data 0.000 (0.131) loss 0.8027 (1.2748) acc 90.6250 (78.1250) lr 1.8763e-03 eta 0:01:54\n",
            "epoch [11/50] batch [5/6] time 0.340 (0.436) data 0.001 (0.094) loss 1.2168 (1.2342) acc 71.8750 (76.2500) lr 1.8443e-03 eta 0:01:42\n",
            "epoch [12/50] batch [5/6] time 0.340 (0.555) data 0.000 (0.215) loss 1.0244 (1.2062) acc 84.3750 (79.3750) lr 1.8090e-03 eta 0:02:07\n",
            "epoch [13/50] batch [5/6] time 0.339 (0.492) data 0.000 (0.151) loss 1.7881 (1.1920) acc 62.5000 (76.2500) lr 1.7705e-03 eta 0:01:49\n",
            "epoch [14/50] batch [5/6] time 0.344 (0.489) data 0.000 (0.146) loss 1.1357 (1.0340) acc 71.8750 (81.2500) lr 1.7290e-03 eta 0:01:46\n",
            "epoch [15/50] batch [5/6] time 0.341 (0.459) data 0.000 (0.112) loss 0.7568 (0.8080) acc 84.3750 (82.5000) lr 1.6845e-03 eta 0:01:36\n",
            "epoch [16/50] batch [5/6] time 0.342 (0.479) data 0.001 (0.137) loss 0.9897 (0.9984) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:01:38\n",
            "epoch [17/50] batch [5/6] time 0.345 (0.585) data 0.004 (0.240) loss 1.0400 (0.8973) acc 75.0000 (81.2500) lr 1.5878e-03 eta 0:01:56\n",
            "epoch [18/50] batch [5/6] time 0.342 (0.503) data 0.000 (0.160) loss 0.3586 (0.7679) acc 90.6250 (81.8750) lr 1.5358e-03 eta 0:01:37\n",
            "epoch [19/50] batch [5/6] time 0.346 (0.432) data 0.000 (0.086) loss 0.6226 (0.6172) acc 90.6250 (90.0000) lr 1.4818e-03 eta 0:01:20\n",
            "epoch [20/50] batch [5/6] time 0.344 (0.482) data 0.000 (0.137) loss 0.7476 (0.5882) acc 84.3750 (89.3750) lr 1.4258e-03 eta 0:01:27\n",
            "epoch [21/50] batch [5/6] time 0.345 (0.470) data 0.001 (0.121) loss 0.9102 (0.7733) acc 78.1250 (83.1250) lr 1.3681e-03 eta 0:01:22\n",
            "epoch [22/50] batch [5/6] time 0.347 (0.575) data 0.001 (0.229) loss 0.9058 (0.6513) acc 84.3750 (88.7500) lr 1.3090e-03 eta 0:01:37\n",
            "epoch [23/50] batch [5/6] time 0.346 (0.498) data 0.000 (0.150) loss 0.6978 (0.6583) acc 87.5000 (87.5000) lr 1.2487e-03 eta 0:01:21\n",
            "epoch [24/50] batch [5/6] time 0.345 (0.472) data 0.000 (0.126) loss 0.5283 (0.6516) acc 93.7500 (85.0000) lr 1.1874e-03 eta 0:01:14\n",
            "epoch [25/50] batch [5/6] time 0.346 (0.482) data 0.000 (0.133) loss 0.7573 (0.5594) acc 84.3750 (89.3750) lr 1.1253e-03 eta 0:01:12\n",
            "epoch [26/50] batch [5/6] time 0.349 (0.484) data 0.001 (0.129) loss 0.3594 (0.4687) acc 93.7500 (90.6250) lr 1.0628e-03 eta 0:01:10\n",
            "epoch [27/50] batch [5/6] time 0.351 (0.575) data 0.001 (0.222) loss 0.3586 (0.5363) acc 96.8750 (89.3750) lr 1.0000e-03 eta 0:01:19\n",
            "epoch [28/50] batch [5/6] time 0.349 (0.542) data 0.000 (0.194) loss 0.7549 (0.5484) acc 87.5000 (90.6250) lr 9.3721e-04 eta 0:01:12\n",
            "epoch [29/50] batch [5/6] time 0.349 (0.457) data 0.000 (0.103) loss 0.3096 (0.4696) acc 93.7500 (91.8750) lr 8.7467e-04 eta 0:00:58\n",
            "epoch [30/50] batch [5/6] time 0.350 (0.497) data 0.000 (0.147) loss 0.3787 (0.4539) acc 96.8750 (91.2500) lr 8.1262e-04 eta 0:01:00\n",
            "epoch [31/50] batch [5/6] time 0.350 (0.492) data 0.001 (0.140) loss 0.2781 (0.5158) acc 96.8750 (88.7500) lr 7.5131e-04 eta 0:00:56\n",
            "epoch [32/50] batch [5/6] time 0.351 (0.576) data 0.001 (0.221) loss 0.3503 (0.5755) acc 96.8750 (88.7500) lr 6.9098e-04 eta 0:01:02\n",
            "epoch [33/50] batch [5/6] time 0.351 (0.479) data 0.000 (0.125) loss 0.5703 (0.5467) acc 90.6250 (90.0000) lr 6.3188e-04 eta 0:00:49\n",
            "epoch [34/50] batch [5/6] time 0.351 (0.470) data 0.000 (0.111) loss 0.5244 (0.3712) acc 87.5000 (91.8750) lr 5.7422e-04 eta 0:00:45\n",
            "epoch [35/50] batch [5/6] time 0.353 (0.485) data 0.000 (0.131) loss 0.5317 (0.5296) acc 93.7500 (90.0000) lr 5.1825e-04 eta 0:00:44\n",
            "epoch [36/50] batch [5/6] time 0.355 (0.486) data 0.001 (0.129) loss 0.6797 (0.5911) acc 81.2500 (87.5000) lr 4.6417e-04 eta 0:00:41\n",
            "epoch [37/50] batch [5/6] time 0.355 (0.573) data 0.001 (0.209) loss 0.7144 (0.5695) acc 87.5000 (88.1250) lr 4.1221e-04 eta 0:00:45\n",
            "epoch [38/50] batch [5/6] time 0.355 (0.584) data 0.000 (0.225) loss 0.1957 (0.3696) acc 100.0000 (96.2500) lr 3.6258e-04 eta 0:00:42\n",
            "epoch [39/50] batch [5/6] time 0.354 (0.480) data 0.000 (0.121) loss 0.6631 (0.5042) acc 84.3750 (90.6250) lr 3.1545e-04 eta 0:00:32\n",
            "epoch [40/50] batch [5/6] time 0.355 (0.502) data 0.000 (0.143) loss 0.6035 (0.4191) acc 87.5000 (91.8750) lr 2.7103e-04 eta 0:00:30\n",
            "epoch [41/50] batch [5/6] time 0.356 (0.497) data 0.001 (0.137) loss 0.5825 (0.4220) acc 84.3750 (92.5000) lr 2.2949e-04 eta 0:00:27\n",
            "epoch [42/50] batch [5/6] time 0.356 (0.594) data 0.001 (0.235) loss 0.4297 (0.4631) acc 96.8750 (90.6250) lr 1.9098e-04 eta 0:00:29\n",
            "epoch [43/50] batch [5/6] time 0.355 (0.502) data 0.000 (0.142) loss 0.2683 (0.4843) acc 93.7500 (89.3750) lr 1.5567e-04 eta 0:00:21\n",
            "epoch [44/50] batch [5/6] time 0.355 (0.480) data 0.000 (0.117) loss 0.3928 (0.3608) acc 93.7500 (95.0000) lr 1.2369e-04 eta 0:00:17\n",
            "epoch [45/50] batch [5/6] time 0.355 (0.468) data 0.000 (0.097) loss 0.4692 (0.3973) acc 90.6250 (91.8750) lr 9.5173e-05 eta 0:00:14\n",
            "epoch [46/50] batch [5/6] time 0.354 (0.502) data 0.001 (0.143) loss 0.4800 (0.4397) acc 87.5000 (91.8750) lr 7.0224e-05 eta 0:00:12\n",
            "epoch [47/50] batch [5/6] time 0.355 (0.583) data 0.001 (0.226) loss 0.2372 (0.4803) acc 93.7500 (91.2500) lr 4.8943e-05 eta 0:00:11\n",
            "epoch [48/50] batch [5/6] time 0.354 (0.516) data 0.000 (0.159) loss 0.4497 (0.4146) acc 93.7500 (94.3750) lr 3.1417e-05 eta 0:00:06\n",
            "epoch [49/50] batch [5/6] time 0.353 (0.480) data 0.000 (0.124) loss 0.3545 (0.4374) acc 93.7500 (91.2500) lr 1.7713e-05 eta 0:00:03\n",
            "epoch [50/50] batch [5/6] time 0.352 (0.470) data 0.000 (0.110) loss 0.4822 (0.4386) acc 93.7500 (91.8750) lr 7.8853e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed1/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:37<00:00,  2.69it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 39\n",
            "* accuracy: 0.4%\n",
            "* error: 99.6%\n",
            "* macro_f1: 0.8%\n",
            "Elapsed: 0:03:12\n",
            "Setting fixed seed: 2\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'True', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed2\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 2\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed2\n",
            "RESUME: \n",
            "SEED: 2\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: True\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Loading preprocessed few-shot data from /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_1-seed_2.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  200\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Building custom CLIP\n",
            "Initializing class-specific contexts\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed2/tensorboard)\n",
            "epoch [1/50] batch [5/6] time 0.350 (0.892) data 0.000 (0.153) loss 5.4688 (5.4211) acc 6.2500 (5.6250) lr 1.0000e-05 eta 0:04:23\n",
            "epoch [2/50] batch [5/6] time 0.350 (0.462) data 0.001 (0.106) loss 4.5195 (5.1695) acc 15.6250 (5.6250) lr 2.0000e-03 eta 0:02:13\n",
            "epoch [3/50] batch [5/6] time 0.353 (0.592) data 0.001 (0.238) loss 4.9297 (4.6273) acc 18.7500 (17.5000) lr 1.9980e-03 eta 0:02:47\n",
            "epoch [4/50] batch [5/6] time 0.351 (0.500) data 0.000 (0.146) loss 3.9492 (3.7363) acc 25.0000 (27.5000) lr 1.9921e-03 eta 0:02:18\n",
            "epoch [5/50] batch [5/6] time 0.346 (0.481) data 0.000 (0.127) loss 2.9316 (2.8789) acc 40.6250 (41.2500) lr 1.9823e-03 eta 0:02:10\n",
            "epoch [6/50] batch [5/6] time 0.354 (0.463) data 0.000 (0.104) loss 3.0527 (2.5609) acc 34.3750 (49.3750) lr 1.9686e-03 eta 0:02:02\n",
            "epoch [7/50] batch [5/6] time 0.354 (0.493) data 0.001 (0.135) loss 2.0820 (1.8740) acc 46.8750 (58.7500) lr 1.9511e-03 eta 0:02:07\n",
            "epoch [8/50] batch [5/6] time 0.356 (0.596) data 0.000 (0.239) loss 1.9902 (1.7566) acc 50.0000 (63.1250) lr 1.9298e-03 eta 0:02:30\n",
            "epoch [9/50] batch [5/6] time 0.354 (0.462) data 0.000 (0.103) loss 2.3770 (1.7170) acc 56.2500 (68.1250) lr 1.9048e-03 eta 0:01:54\n",
            "epoch [10/50] batch [5/6] time 0.355 (0.478) data 0.001 (0.110) loss 1.4160 (1.3900) acc 65.6250 (73.7500) lr 1.8763e-03 eta 0:01:55\n",
            "epoch [11/50] batch [5/6] time 0.357 (0.469) data 0.000 (0.100) loss 1.3301 (1.3713) acc 68.7500 (73.1250) lr 1.8443e-03 eta 0:01:50\n",
            "epoch [12/50] batch [5/6] time 0.357 (0.509) data 0.001 (0.147) loss 0.9092 (1.0418) acc 81.2500 (77.5000) lr 1.8090e-03 eta 0:01:56\n",
            "epoch [13/50] batch [5/6] time 0.347 (0.564) data 0.001 (0.190) loss 1.3418 (1.0377) acc 71.8750 (78.7500) lr 1.7705e-03 eta 0:02:05\n",
            "epoch [14/50] batch [5/6] time 0.356 (0.499) data 0.000 (0.135) loss 0.8789 (0.9100) acc 81.2500 (81.2500) lr 1.7290e-03 eta 0:01:48\n",
            "epoch [15/50] batch [5/6] time 0.356 (0.492) data 0.000 (0.127) loss 0.9028 (0.8673) acc 84.3750 (83.1250) lr 1.6845e-03 eta 0:01:43\n",
            "epoch [16/50] batch [5/6] time 0.356 (0.503) data 0.000 (0.144) loss 0.7554 (0.7342) acc 93.7500 (88.1250) lr 1.6374e-03 eta 0:01:43\n",
            "epoch [17/50] batch [5/6] time 0.354 (0.497) data 0.001 (0.138) loss 1.0996 (0.7826) acc 84.3750 (85.6250) lr 1.5878e-03 eta 0:01:38\n",
            "epoch [18/50] batch [5/6] time 0.352 (0.570) data 0.001 (0.205) loss 0.6338 (0.7765) acc 87.5000 (84.3750) lr 1.5358e-03 eta 0:01:49\n",
            "epoch [19/50] batch [5/6] time 0.353 (0.503) data 0.000 (0.147) loss 1.2061 (0.8526) acc 78.1250 (83.1250) lr 1.4818e-03 eta 0:01:33\n",
            "epoch [20/50] batch [5/6] time 0.353 (0.491) data 0.000 (0.136) loss 0.7876 (0.8342) acc 81.2500 (83.7500) lr 1.4258e-03 eta 0:01:28\n",
            "epoch [21/50] batch [5/6] time 0.353 (0.479) data 0.000 (0.124) loss 0.6553 (0.6732) acc 90.6250 (89.3750) lr 1.3681e-03 eta 0:01:23\n",
            "epoch [22/50] batch [5/6] time 0.354 (0.490) data 0.001 (0.135) loss 0.5146 (0.4908) acc 90.6250 (92.5000) lr 1.3090e-03 eta 0:01:22\n",
            "epoch [23/50] batch [5/6] time 0.352 (0.571) data 0.001 (0.214) loss 0.6074 (0.6262) acc 87.5000 (86.8750) lr 1.2487e-03 eta 0:01:33\n",
            "epoch [24/50] batch [5/6] time 0.351 (0.450) data 0.000 (0.096) loss 0.3792 (0.6702) acc 93.7500 (87.5000) lr 1.1874e-03 eta 0:01:10\n",
            "epoch [25/50] batch [5/6] time 0.350 (0.482) data 0.000 (0.128) loss 0.6045 (0.7861) acc 84.3750 (83.1250) lr 1.1253e-03 eta 0:01:12\n",
            "epoch [26/50] batch [5/6] time 0.350 (0.485) data 0.000 (0.133) loss 0.7007 (0.6294) acc 81.2500 (88.1250) lr 1.0628e-03 eta 0:01:10\n",
            "epoch [27/50] batch [5/6] time 0.350 (0.462) data 0.001 (0.097) loss 0.5254 (0.6482) acc 84.3750 (85.6250) lr 1.0000e-03 eta 0:01:04\n",
            "epoch [28/50] batch [5/6] time 0.353 (0.570) data 0.001 (0.216) loss 0.5459 (0.5528) acc 90.6250 (89.3750) lr 9.3721e-04 eta 0:01:15\n",
            "epoch [29/50] batch [5/6] time 0.352 (0.481) data 0.000 (0.126) loss 0.5415 (0.5243) acc 87.5000 (91.2500) lr 8.7467e-04 eta 0:01:01\n",
            "epoch [30/50] batch [5/6] time 0.351 (0.457) data 0.000 (0.100) loss 0.3760 (0.5005) acc 93.7500 (90.0000) lr 8.1262e-04 eta 0:00:55\n",
            "epoch [31/50] batch [5/6] time 0.351 (0.482) data 0.000 (0.128) loss 0.5835 (0.5190) acc 87.5000 (90.0000) lr 7.5131e-04 eta 0:00:55\n",
            "epoch [32/50] batch [5/6] time 0.351 (0.486) data 0.001 (0.131) loss 0.4646 (0.3885) acc 87.5000 (93.1250) lr 6.9098e-04 eta 0:00:52\n",
            "epoch [33/50] batch [5/6] time 0.352 (0.617) data 0.000 (0.261) loss 0.3318 (0.5202) acc 93.7500 (91.2500) lr 6.3188e-04 eta 0:01:03\n",
            "epoch [34/50] batch [5/6] time 0.353 (0.500) data 0.000 (0.145) loss 0.4551 (0.5294) acc 87.5000 (87.5000) lr 5.7422e-04 eta 0:00:48\n",
            "epoch [35/50] batch [5/6] time 0.354 (0.503) data 0.000 (0.146) loss 0.4048 (0.4312) acc 93.7500 (93.7500) lr 5.1825e-04 eta 0:00:45\n",
            "epoch [36/50] batch [5/6] time 0.354 (0.494) data 0.000 (0.138) loss 0.3738 (0.4716) acc 93.7500 (91.8750) lr 4.6417e-04 eta 0:00:41\n",
            "epoch [37/50] batch [5/6] time 0.353 (0.486) data 0.001 (0.127) loss 0.8174 (0.5527) acc 87.5000 (88.7500) lr 4.1221e-04 eta 0:00:38\n",
            "epoch [38/50] batch [5/6] time 0.353 (0.613) data 0.000 (0.255) loss 0.6416 (0.3892) acc 84.3750 (93.1250) lr 3.6258e-04 eta 0:00:44\n",
            "epoch [39/50] batch [5/6] time 0.353 (0.444) data 0.000 (0.083) loss 0.2052 (0.3586) acc 96.8750 (93.1250) lr 3.1545e-04 eta 0:00:29\n",
            "epoch [40/50] batch [5/6] time 0.351 (0.472) data 0.000 (0.110) loss 0.6528 (0.4896) acc 84.3750 (88.7500) lr 2.7103e-04 eta 0:00:28\n",
            "epoch [41/50] batch [5/6] time 0.354 (0.485) data 0.000 (0.128) loss 0.4734 (0.3528) acc 93.7500 (94.3750) lr 2.2949e-04 eta 0:00:26\n",
            "epoch [42/50] batch [5/6] time 0.357 (0.496) data 0.001 (0.138) loss 0.3296 (0.4269) acc 96.8750 (93.1250) lr 1.9098e-04 eta 0:00:24\n",
            "epoch [43/50] batch [5/6] time 0.356 (0.585) data 0.004 (0.225) loss 0.0793 (0.2958) acc 100.0000 (95.0000) lr 1.5567e-04 eta 0:00:25\n",
            "epoch [44/50] batch [5/6] time 0.352 (0.488) data 0.000 (0.122) loss 0.6914 (0.3420) acc 84.3750 (93.7500) lr 1.2369e-04 eta 0:00:18\n",
            "epoch [45/50] batch [5/6] time 0.354 (0.469) data 0.000 (0.109) loss 0.5552 (0.4986) acc 90.6250 (91.8750) lr 9.5173e-05 eta 0:00:14\n",
            "epoch [46/50] batch [5/6] time 0.353 (0.472) data 0.000 (0.114) loss 0.1997 (0.3537) acc 93.7500 (91.8750) lr 7.0224e-05 eta 0:00:11\n",
            "epoch [47/50] batch [5/6] time 0.351 (0.471) data 0.001 (0.114) loss 0.4158 (0.3251) acc 90.6250 (94.3750) lr 4.8943e-05 eta 0:00:08\n",
            "epoch [48/50] batch [5/6] time 0.353 (0.578) data 0.001 (0.222) loss 0.3042 (0.3185) acc 100.0000 (96.2500) lr 3.1417e-05 eta 0:00:07\n",
            "epoch [49/50] batch [5/6] time 0.352 (0.493) data 0.000 (0.138) loss 0.6304 (0.5975) acc 84.3750 (87.5000) lr 1.7713e-05 eta 0:00:03\n",
            "epoch [50/50] batch [5/6] time 0.350 (0.482) data 0.000 (0.127) loss 0.2861 (0.4182) acc 96.8750 (91.8750) lr 7.8853e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed2/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:35<00:00,  2.83it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 58\n",
            "* accuracy: 0.6%\n",
            "* error: 99.4%\n",
            "* macro_f1: 1.2%\n",
            "Elapsed: 0:03:11\n",
            "Setting fixed seed: 3\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/CoOp/rn50_ep50.yaml\n",
            "dataset_config_file: configs/datasets/imagenet.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['TRAINER.COOP.N_CTX', '16', 'TRAINER.COOP.CSC', 'True', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1']\n",
            "output_dir: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed3\n",
            "resume: \n",
            "root: ../../DATA\n",
            "seed: 3\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: CoOp\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: ImageNet\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 1\n",
            "  ROOT: ../../DATA\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: all\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: RN50\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.002\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed3\n",
            "RESUME: \n",
            "SEED: 3\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 5\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: True\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.4\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.5\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.32\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==2.1.3\n",
            "[pip3] torch==2.5.1\n",
            "[pip3] torchvision==0.20.1\n",
            "[pip3] triton==3.1.0\n",
            "[conda] blas                      1.0                         mkl  \n",
            "[conda] cudatoolkit               10.2.89              hfd86e86_1  \n",
            "[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\n",
            "[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch\n",
            "[conda] mkl                       2023.1.0         h213fc3f_46344  \n",
            "[conda] mkl-service               2.4.0            py38h5eee18b_1  \n",
            "[conda] mkl_fft                   1.3.8            py38h5eee18b_0  \n",
            "[conda] mkl_random                1.2.4            py38hdb19cb5_0  \n",
            "[conda] numpy                     1.24.3           py38hf6e8229_1  \n",
            "[conda] numpy-base                1.24.3           py38h060ed82_1  \n",
            "[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch\n",
            "[conda] pytorch-mutex             1.0                         cpu    pytorch\n",
            "[conda] torchvision               0.20.0                 py38_cpu    pytorch\n",
            "        Pillow (11.0.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: ImageNet\n",
            "Loading preprocessed few-shot data from /content/Capstone-ood/DATA/imagenet/split_fewshot/shot_1-seed_3.pkl\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  --------\n",
            "Dataset    ImageNet\n",
            "# classes  200\n",
            "# train_x  200\n",
            "# val      10,000\n",
            "# test     10,000\n",
            "---------  --------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Building custom CLIP\n",
            "Initializing class-specific contexts\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "/usr/local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed3/tensorboard)\n",
            "epoch [1/50] batch [5/6] time 0.351 (1.131) data 0.001 (0.207) loss 5.2109 (5.2352) acc 6.2500 (8.1250) lr 1.0000e-05 eta 0:05:33\n",
            "epoch [2/50] batch [5/6] time 0.351 (0.504) data 0.000 (0.153) loss 4.8750 (5.0359) acc 15.6250 (7.5000) lr 2.0000e-03 eta 0:02:25\n",
            "epoch [3/50] batch [5/6] time 0.351 (0.481) data 0.000 (0.130) loss 4.7812 (4.4500) acc 9.3750 (19.3750) lr 1.9980e-03 eta 0:02:16\n",
            "epoch [4/50] batch [5/6] time 0.353 (0.488) data 0.000 (0.134) loss 4.1719 (3.7418) acc 15.6250 (26.2500) lr 1.9921e-03 eta 0:02:15\n",
            "epoch [5/50] batch [5/6] time 0.347 (0.494) data 0.001 (0.141) loss 3.5508 (2.8262) acc 25.0000 (46.2500) lr 1.9823e-03 eta 0:02:13\n",
            "epoch [6/50] batch [5/6] time 0.353 (0.575) data 0.001 (0.212) loss 2.5996 (2.4816) acc 53.1250 (53.1250) lr 1.9686e-03 eta 0:02:32\n",
            "epoch [7/50] batch [5/6] time 0.353 (0.486) data 0.000 (0.130) loss 2.4004 (2.0252) acc 56.2500 (61.8750) lr 1.9511e-03 eta 0:02:05\n",
            "epoch [8/50] batch [5/6] time 0.355 (0.492) data 0.000 (0.135) loss 1.4150 (1.6627) acc 81.2500 (66.8750) lr 1.9298e-03 eta 0:02:04\n",
            "epoch [9/50] batch [5/6] time 0.354 (0.508) data 0.001 (0.150) loss 1.3672 (1.3857) acc 68.7500 (70.0000) lr 1.9048e-03 eta 0:02:05\n",
            "epoch [10/50] batch [5/6] time 0.355 (0.488) data 0.001 (0.129) loss 1.3242 (1.2893) acc 81.2500 (78.1250) lr 1.8763e-03 eta 0:01:57\n",
            "epoch [11/50] batch [5/6] time 0.356 (0.578) data 0.001 (0.218) loss 1.4102 (1.4062) acc 75.0000 (73.1250) lr 1.8443e-03 eta 0:02:15\n",
            "epoch [12/50] batch [5/6] time 0.356 (0.493) data 0.000 (0.132) loss 1.0762 (1.1768) acc 75.0000 (76.2500) lr 1.8090e-03 eta 0:01:52\n",
            "epoch [13/50] batch [5/6] time 0.356 (0.510) data 0.000 (0.151) loss 1.0283 (1.0048) acc 81.2500 (81.2500) lr 1.7705e-03 eta 0:01:53\n",
            "epoch [14/50] batch [5/6] time 0.357 (0.495) data 0.000 (0.134) loss 1.0303 (1.0501) acc 75.0000 (79.3750) lr 1.7290e-03 eta 0:01:47\n",
            "epoch [15/50] batch [5/6] time 0.356 (0.446) data 0.001 (0.085) loss 1.0703 (1.0458) acc 81.2500 (81.8750) lr 1.6845e-03 eta 0:01:34\n",
            "epoch [16/50] batch [5/6] time 0.354 (0.570) data 0.001 (0.208) loss 0.8408 (0.9530) acc 81.2500 (81.8750) lr 1.6374e-03 eta 0:01:56\n",
            "epoch [17/50] batch [5/6] time 0.353 (0.516) data 0.000 (0.159) loss 0.9673 (0.9177) acc 71.8750 (79.3750) lr 1.5878e-03 eta 0:01:42\n",
            "epoch [18/50] batch [5/6] time 0.352 (0.489) data 0.000 (0.132) loss 0.8130 (0.8014) acc 81.2500 (84.3750) lr 1.5358e-03 eta 0:01:34\n",
            "epoch [19/50] batch [5/6] time 0.353 (0.461) data 0.001 (0.104) loss 0.3506 (0.5892) acc 93.7500 (89.3750) lr 1.4818e-03 eta 0:01:26\n",
            "epoch [20/50] batch [5/6] time 0.354 (0.495) data 0.001 (0.137) loss 0.6812 (0.7243) acc 90.6250 (86.2500) lr 1.4258e-03 eta 0:01:29\n",
            "epoch [21/50] batch [5/6] time 0.352 (0.566) data 0.000 (0.203) loss 0.8008 (0.6788) acc 84.3750 (86.8750) lr 1.3681e-03 eta 0:01:39\n",
            "epoch [22/50] batch [5/6] time 0.350 (0.496) data 0.000 (0.143) loss 0.7324 (0.5898) acc 78.1250 (85.6250) lr 1.3090e-03 eta 0:01:23\n",
            "epoch [23/50] batch [5/6] time 0.352 (0.464) data 0.000 (0.107) loss 0.9238 (0.6679) acc 87.5000 (88.7500) lr 1.2487e-03 eta 0:01:15\n",
            "epoch [24/50] batch [5/6] time 0.352 (0.483) data 0.000 (0.129) loss 1.1250 (0.5427) acc 68.7500 (88.7500) lr 1.1874e-03 eta 0:01:15\n",
            "epoch [25/50] batch [5/6] time 0.351 (0.492) data 0.000 (0.139) loss 0.7607 (0.6633) acc 81.2500 (84.3750) lr 1.1253e-03 eta 0:01:14\n",
            "epoch [26/50] batch [5/6] time 0.351 (0.568) data 0.000 (0.214) loss 0.5127 (0.4729) acc 90.6250 (90.6250) lr 1.0628e-03 eta 0:01:22\n",
            "epoch [27/50] batch [5/6] time 0.349 (0.486) data 0.000 (0.132) loss 0.4055 (0.4645) acc 93.7500 (91.2500) lr 1.0000e-03 eta 0:01:07\n",
            "epoch [28/50] batch [5/6] time 0.350 (0.485) data 0.000 (0.132) loss 0.5767 (0.5092) acc 87.5000 (90.0000) lr 9.3721e-04 eta 0:01:04\n",
            "epoch [29/50] batch [5/6] time 0.349 (0.496) data 0.000 (0.144) loss 0.4011 (0.3960) acc 96.8750 (95.0000) lr 8.7467e-04 eta 0:01:02\n",
            "epoch [30/50] batch [5/6] time 0.354 (0.490) data 0.001 (0.136) loss 0.4434 (0.5303) acc 100.0000 (90.6250) lr 8.1262e-04 eta 0:00:59\n",
            "epoch [31/50] batch [5/6] time 0.353 (0.591) data 0.001 (0.237) loss 0.8096 (0.6079) acc 81.2500 (87.5000) lr 7.5131e-04 eta 0:01:07\n",
            "epoch [32/50] batch [5/6] time 0.351 (0.468) data 0.000 (0.108) loss 0.4766 (0.5926) acc 90.6250 (89.3750) lr 6.9098e-04 eta 0:00:51\n",
            "epoch [33/50] batch [5/6] time 0.352 (0.486) data 0.000 (0.131) loss 0.2201 (0.5248) acc 96.8750 (90.0000) lr 6.3188e-04 eta 0:00:50\n",
            "epoch [34/50] batch [5/6] time 0.351 (0.495) data 0.000 (0.141) loss 0.6987 (0.5206) acc 78.1250 (87.5000) lr 5.7422e-04 eta 0:00:47\n",
            "epoch [35/50] batch [5/6] time 0.354 (0.452) data 0.001 (0.091) loss 0.6616 (0.4375) acc 81.2500 (91.8750) lr 5.1825e-04 eta 0:00:41\n",
            "epoch [36/50] batch [5/6] time 0.355 (0.586) data 0.001 (0.229) loss 0.2430 (0.3633) acc 96.8750 (93.7500) lr 4.6417e-04 eta 0:00:49\n",
            "epoch [37/50] batch [5/6] time 0.353 (0.476) data 0.000 (0.115) loss 0.7910 (0.5780) acc 84.3750 (87.5000) lr 4.1221e-04 eta 0:00:37\n",
            "epoch [38/50] batch [5/6] time 0.354 (0.488) data 0.000 (0.131) loss 0.6260 (0.3711) acc 90.6250 (94.3750) lr 3.6258e-04 eta 0:00:35\n",
            "epoch [39/50] batch [5/6] time 0.353 (0.492) data 0.000 (0.136) loss 0.3059 (0.3644) acc 96.8750 (93.1250) lr 3.1545e-04 eta 0:00:32\n",
            "epoch [40/50] batch [5/6] time 0.354 (0.495) data 0.001 (0.136) loss 0.2231 (0.4425) acc 96.8750 (93.1250) lr 2.7103e-04 eta 0:00:30\n",
            "epoch [41/50] batch [5/6] time 0.354 (0.583) data 0.000 (0.226) loss 0.5322 (0.4592) acc 87.5000 (91.2500) lr 2.2949e-04 eta 0:00:32\n",
            "epoch [42/50] batch [5/6] time 0.354 (0.460) data 0.000 (0.100) loss 0.1616 (0.3817) acc 93.7500 (91.8750) lr 1.9098e-04 eta 0:00:22\n",
            "epoch [43/50] batch [5/6] time 0.355 (0.490) data 0.000 (0.133) loss 0.9971 (0.6426) acc 78.1250 (85.6250) lr 1.5567e-04 eta 0:00:21\n",
            "epoch [44/50] batch [5/6] time 0.353 (0.486) data 0.000 (0.126) loss 0.2690 (0.4315) acc 96.8750 (91.8750) lr 1.2369e-04 eta 0:00:17\n",
            "epoch [45/50] batch [5/6] time 0.353 (0.495) data 0.001 (0.136) loss 0.4253 (0.4335) acc 96.8750 (93.1250) lr 9.5173e-05 eta 0:00:15\n",
            "epoch [46/50] batch [5/6] time 0.354 (0.560) data 0.001 (0.194) loss 0.5981 (0.4344) acc 87.5000 (90.0000) lr 7.0224e-05 eta 0:00:14\n",
            "epoch [47/50] batch [5/6] time 0.353 (0.510) data 0.000 (0.155) loss 0.2018 (0.3505) acc 100.0000 (94.3750) lr 4.8943e-05 eta 0:00:09\n",
            "epoch [48/50] batch [5/6] time 0.353 (0.484) data 0.000 (0.129) loss 0.2754 (0.4231) acc 96.8750 (93.7500) lr 3.1417e-05 eta 0:00:06\n",
            "epoch [49/50] batch [5/6] time 0.353 (0.489) data 0.000 (0.134) loss 0.2374 (0.3344) acc 100.0000 (95.6250) lr 1.7713e-05 eta 0:00:03\n",
            "epoch [50/50] batch [5/6] time 0.353 (0.502) data 0.001 (0.147) loss 0.3198 (0.3983) acc 100.0000 (93.7500) lr 7.8853e-06 eta 0:00:00\n",
            "Checkpoint saved to output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed3/prompt_learner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 100/100 [00:37<00:00,  2.63it/s]\n",
            "=> result\n",
            "* total: 10,000\n",
            "* correct: 13\n",
            "* accuracy: 0.1%\n",
            "* error: 99.9%\n",
            "* macro_f1: 0.3%\n",
            "Elapsed: 0:03:15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate\n",
        "!python parse_test_res.py output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGe9WokdqT5F",
        "outputId": "2919c917-1f7c-4da8-80d6-22f496750985"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing files in output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend\n",
            "file: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed1/log.txt. accuracy: 0.40%. \n",
            "file: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed2/log.txt. accuracy: 0.50%. \n",
            "file: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend/seed3/log.txt. accuracy: 0.60%. \n",
            "===\n",
            "Summary of directory: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscFalse_ctpend\n",
            "* accuracy: 0.50% +- 0.08%\n",
            "===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python parse_test_res.py output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUmm8SXJ2nzZ",
        "outputId": "48d4caaa-8ca0-4787-a3f4-2ddf7bed76b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing files in output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend\n",
            "file: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed1/log.txt. accuracy: 0.40%. \n",
            "file: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed2/log.txt. accuracy: 0.60%. \n",
            "file: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend/seed3/log.txt. accuracy: 0.10%. \n",
            "===\n",
            "Summary of directory: output/imagenet/CoOp/rn50_ep50_1shots/nctx16_cscTrue_ctpend\n",
            "* accuracy: 0.37% +- 0.21%\n",
            "===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python parse_test_res.py output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toEaiX8Y2pkZ",
        "outputId": "9ee270ce-48e3-4657-d548-b751cf35a1de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing files in output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend\n",
            "file: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed1/log.txt. accuracy: 0.50%. \n",
            "file: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed2/log.txt. accuracy: 0.50%. \n",
            "file: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend/seed3/log.txt. accuracy: 0.50%. \n",
            "===\n",
            "Summary of directory: output/imagenet/CoOp/rn50_ep50_8shots/nctx16_cscFalse_ctpend\n",
            "* accuracy: 0.50% +- 0.00%\n",
            "===\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}