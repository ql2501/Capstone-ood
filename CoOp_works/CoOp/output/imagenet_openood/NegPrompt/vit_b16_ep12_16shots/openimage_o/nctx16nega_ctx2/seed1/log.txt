***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/NegPrompt/vit_b16_ep12.yaml
dataset_config_file: configs/datasets/imagenet_openood.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.NEGPROMPT.NEGA_CTX', '2', 'DATASET.NUM_SHOTS', '16']
output_dir: output/imagenet_openood/NegPrompt/vit_b16_ep12_16shots/nctx16nega_ctx2/seed1
resume: 
root: ../../DATA
seed: 1
source_domains: None
target_domains: None
trainer: NegPrompt
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Imagenet_Openood
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ../../DATA
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 12
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/imagenet_openood/NegPrompt/vit_b16_ep12_16shots/nctx16nega_ctx2/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a '{}'
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: NegPrompt
  NEGPROMPT:
    CSC: 0
    DISTANCE_WEIGHT: 0.1
    NEGA_CTX: 2
    NEGA_NEGA_WEIGHT: 0.05
    NETATIVE_WEIGHT: 1
    N_CTX: 16
    OPEN_SCORE: msp
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.5.1+cu124
Is debug build: False
CUDA used to build PyTorch: 12.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: version 3.30.5
Libc version: glibc-2.35

Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.1.85+-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 535.104.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        46 bits physical, 48 bits virtual
Byte Order:                           Little Endian
CPU(s):                               2
On-line CPU(s) list:                  0,1
Vendor ID:                            GenuineIntel
Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz
CPU family:                           6
Model:                                85
Thread(s) per core:                   2
Core(s) per socket:                   1
Socket(s):                            1
Stepping:                             3
BogoMIPS:                             4000.38
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities
Hypervisor vendor:                    KVM
Virtualization type:                  full
L1d cache:                            32 KiB (1 instance)
L1i cache:                            32 KiB (1 instance)
L2 cache:                             1 MiB (1 instance)
L3 cache:                             38.5 MiB (1 instance)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0,1
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Mitigation; PTE Inversion
Vulnerability Mds:                    Vulnerable; SMT Host state unknown
Vulnerability Meltdown:               Vulnerable
Vulnerability Mmio stale data:        Vulnerable
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Vulnerable
Vulnerability Spec rstack overflow:   Not affected
Vulnerability Spec store bypass:      Vulnerable
Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Vulnerable

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==2.1.3
[pip3] torch==2.5.1
[pip3] torchvision==0.20.1
[pip3] triton==3.1.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] libjpeg-turbo             2.0.0                h9bf148f_0    pytorch
[conda] mkl                       2023.1.0         h213fc3f_46344  
[conda] mkl-service               2.4.0            py38h5eee18b_1  
[conda] mkl_fft                   1.3.8            py38h5eee18b_0  
[conda] mkl_random                1.2.4            py38hdb19cb5_0  
[conda] numpy                     1.24.3           py38hf6e8229_1  
[conda] numpy-base                1.24.3           py38h060ed82_1  
[conda] pytorch                   2.4.1               py3.8_cpu_0    pytorch
[conda] pytorch-mutex             1.0                         cpu    pytorch
[conda] torchvision               0.20.0                 py38_cpu    pytorch
        Pillow (11.0.0)

Loading trainer: NegPrompt
Loading dataset: Imagenet_Openood
Creating a 16-shot dataset
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------------
Dataset    Imagenet_Openood
# classes  100
# train_x  1,600
# val      22,632
# test     22,632
---------  ----------------
Loading CLIP (backbone: ViT-B/16)
Successfully loading CLIP (backbone: ViT-B/16)
--------------------------------------------------------------------------------
Building NegPrompt's custom CLIP
Initial context: "a photo of number: "
Number of context words (tokens): 4
Successfully building NegPrompt's custom CLIP
--------------------------------------------------------------------------------
Turning off gradients in both the image and the text encoder...
Remaining active gradient in prompt_learner.ctx_negative, paramter shape torch.Size([2, 4, 512])
Start loading positive prompts from model path: output/imagenet/CoOp/vit_b16_ep50_16shots/nctx16_cscFalse_ctpend/seed1/prompt_learner/model.pth.tar-50
Loaded positive prompts: 
tensor([[ 0.0594,  0.0089,  0.0527,  ..., -0.0013,  0.0163, -0.0125],
        [ 0.0443, -0.0388,  0.0515,  ..., -0.0027,  0.0936,  0.0673],
        [ 0.0344, -0.0373,  0.0508,  ..., -0.1120,  0.0122, -0.0126],
        [-0.0042, -0.0094, -0.0069,  ...,  0.0381, -0.0061,  0.0036]],
       device='cuda:0', dtype=torch.float16)
After update, the shape of ctx_positive is torch.Size([1, 4, 512])
Positive prompt from Pretrained CoOp loaded
Adding ctx_negative to optimizer
Finished building model NegPrompt
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/imagenet_openood/NegPrompt/vit_b16_ep12_16shots/nctx16nega_ctx2/seed1/tensorboard)
epoch [1/12] batch [5/50] time 0.808 (1.963) data 0.008 (0.564) loss 20.0000 (20.4000) loss_positive 4.6992 (4.7375) NIS Loss 20.0000 (20.4000) NND Loss 0.0000 (-0.0000) NPD Loss -0.0005 (-0.0001) lr 1.0000e-05 eta 0:19:27
epoch [1/12] batch [10/50] time 0.594 (1.294) data 0.001 (0.283) loss 19.2960 (20.0232) loss_positive 4.6484 (4.7270) NIS Loss 19.2969 (20.0234) NND Loss -0.0000 (0.0000) NPD Loss -0.0083 (-0.0021) lr 1.0000e-05 eta 0:12:43
epoch [1/12] batch [15/50] time 0.549 (1.049) data 0.001 (0.189) loss 18.3716 (19.6335) loss_positive 4.6016 (4.7273) NIS Loss 18.3750 (19.6344) NND Loss 0.0000 (0.0000) NPD Loss -0.0343 (-0.0088) lr 1.0000e-05 eta 0:10:13
epoch [1/12] batch [20/50] time 0.565 (0.927) data 0.001 (0.142) loss 17.6642 (19.1455) loss_positive 4.8086 (4.7273) NIS Loss 17.6719 (19.1477) NND Loss -0.0000 (0.0000) NPD Loss -0.0770 (-0.0214) lr 1.0000e-05 eta 0:08:57
epoch [1/12] batch [25/50] time 0.820 (0.884) data 0.003 (0.114) loss 16.5030 (18.6912) loss_positive 4.6406 (4.7156) NIS Loss 16.5156 (18.6950) NND Loss 0.0000 (0.0000) NPD Loss -0.1263 (-0.0383) lr 1.0000e-05 eta 0:08:28
epoch [1/12] batch [30/50] time 0.845 (0.866) data 0.001 (0.096) loss 15.7241 (18.2418) loss_positive 4.4805 (4.7046) NIS Loss 15.7422 (18.2477) NND Loss -0.0000 (0.0000) NPD Loss -0.1805 (-0.0584) lr 1.0000e-05 eta 0:08:13
epoch [1/12] batch [35/50] time 0.627 (0.855) data 0.002 (0.083) loss 14.4215 (17.8100) loss_positive 4.7461 (4.7075) NIS Loss 14.4453 (17.8181) NND Loss 0.0000 (0.0000) NPD Loss -0.2381 (-0.0807) lr 1.0000e-05 eta 0:08:03
epoch [1/12] batch [40/50] time 0.533 (0.816) data 0.000 (0.073) loss 14.3532 (17.3870) loss_positive 4.6641 (4.7111) NIS Loss 14.3828 (17.3975) NND Loss -0.0000 (0.0000) NPD Loss -0.2964 (-0.1048) lr 1.0000e-05 eta 0:07:36
epoch [1/12] batch [45/50] time 0.522 (0.783) data 0.000 (0.065) loss 12.8240 (16.8663) loss_positive 4.5977 (4.7204) NIS Loss 12.8594 (16.8793) NND Loss 0.0000 (0.0000) NPD Loss -0.3536 (-0.1299) lr 1.0000e-05 eta 0:07:14
epoch [1/12] batch [50/50] time 0.530 (0.758) data 0.000 (0.058) loss 10.6701 (16.3546) loss_positive 4.8945 (4.7198) NIS Loss 10.7109 (16.3702) NND Loss -0.0000 (0.0000) NPD Loss -0.4088 (-0.1556) lr 2.0000e-03 eta 0:06:56
epoch [2/12] batch [5/50] time 0.851 (1.525) data 0.008 (0.392) loss 9.8098 (13.8326) loss_positive 4.6758 (4.7070) NIS Loss 9.8984 (13.8797) NND Loss -0.0034 (-0.0008) NPD Loss -0.8846 (-0.4709) lr 2.0000e-03 eta 0:13:51
epoch [2/12] batch [10/50] time 0.571 (1.150) data 0.001 (0.201) loss 6.7338 (10.4385) loss_positive 4.7930 (4.7039) NIS Loss 6.8047 (10.5000) NND Loss -0.0033 (-0.0019) NPD Loss -0.7069 (-0.6138) lr 2.0000e-03 eta 0:10:20
epoch [2/12] batch [15/50] time 0.796 (0.974) data 0.001 (0.134) loss 7.0529 (9.2663) loss_positive 4.9023 (4.7083) NIS Loss 7.1445 (9.3354) NND Loss -0.0047 (-0.0028) NPD Loss -0.9143 (-0.6897) lr 2.0000e-03 eta 0:08:41
epoch [2/12] batch [20/50] time 0.558 (0.890) data 0.001 (0.101) loss 6.9633 (8.6596) loss_positive 4.6836 (4.7150) NIS Loss 7.0625 (8.7357) NND Loss -0.0075 (-0.0036) NPD Loss -0.9879 (-0.7593) lr 2.0000e-03 eta 0:07:51
epoch [2/12] batch [25/50] time 0.569 (0.828) data 0.001 (0.081) loss 6.7459 (8.2799) loss_positive 4.7305 (4.7008) NIS Loss 6.8438 (8.3606) NND Loss -0.0087 (-0.0046) NPD Loss -0.9745 (-0.8045) lr 2.0000e-03 eta 0:07:14
epoch [2/12] batch [30/50] time 0.794 (0.817) data 0.001 (0.068) loss 6.6218 (8.0071) loss_positive 4.5742 (4.6936) NIS Loss 6.7188 (8.0905) NND Loss -0.0091 (-0.0053) NPD Loss -0.9651 (-0.8311) lr 2.0000e-03 eta 0:07:04
epoch [2/12] batch [35/50] time 0.611 (0.810) data 0.003 (0.059) loss 6.7521 (7.8303) loss_positive 4.7422 (4.7066) NIS Loss 6.8516 (7.9158) NND Loss -0.0098 (-0.0059) NPD Loss -0.9899 (-0.8523) lr 2.0000e-03 eta 0:06:57
epoch [2/12] batch [40/50] time 0.539 (0.780) data 0.000 (0.052) loss 6.7444 (7.6960) loss_positive 4.9023 (4.7174) NIS Loss 6.8477 (7.7836) NND Loss -0.0103 (-0.0064) NPD Loss -1.0279 (-0.8723) lr 2.0000e-03 eta 0:06:37
epoch [2/12] batch [45/50] time 0.544 (0.753) data 0.000 (0.046) loss 6.7060 (7.5962) loss_positive 4.9961 (4.7232) NIS Loss 6.8125 (7.6858) NND Loss -0.0105 (-0.0069) NPD Loss -1.0600 (-0.8919) lr 2.0000e-03 eta 0:06:20
epoch [2/12] batch [50/50] time 0.538 (0.732) data 0.000 (0.041) loss 6.5962 (7.5159) loss_positive 4.6875 (4.7252) NIS Loss 6.7031 (7.6072) NND Loss -0.0110 (-0.0073) NPD Loss -1.0641 (-0.9091) lr 1.9659e-03 eta 0:06:05
epoch [3/12] batch [5/50] time 0.844 (1.554) data 0.009 (0.369) loss 6.5167 (6.7642) loss_positive 4.8438 (4.7680) NIS Loss 6.6250 (6.8719) NND Loss -0.0117 (-0.0114) NPD Loss -1.0769 (-1.0714) lr 1.9659e-03 eta 0:12:49
epoch [3/12] batch [10/50] time 0.813 (1.185) data 0.001 (0.186) loss 6.7328 (6.7264) loss_positive 4.7344 (4.7621) NIS Loss 6.8438 (6.8352) NND Loss -0.0123 (-0.0117) NPD Loss -1.1032 (-1.0816) lr 1.9659e-03 eta 0:09:40
epoch [3/12] batch [15/50] time 0.579 (0.996) data 0.001 (0.125) loss 6.6866 (6.7331) loss_positive 4.5391 (4.7271) NIS Loss 6.7969 (6.8427) NND Loss -0.0129 (-0.0120) NPD Loss -1.0968 (-1.0897) lr 1.9659e-03 eta 0:08:03
epoch [3/12] batch [20/50] time 0.656 (0.896) data 0.001 (0.094) loss 6.8188 (6.7484) loss_positive 4.8828 (4.7209) NIS Loss 6.9258 (6.8576) NND Loss -0.0140 (-0.0124) NPD Loss -1.0626 (-1.0857) lr 1.9659e-03 eta 0:07:10
epoch [3/12] batch [25/50] time 0.605 (0.836) data 0.002 (0.075) loss 6.5148 (6.7354) loss_positive 4.6484 (4.7197) NIS Loss 6.6211 (6.8441) NND Loss -0.0147 (-0.0128) NPD Loss -1.0551 (-1.0800) lr 1.9659e-03 eta 0:06:37
epoch [3/12] batch [30/50] time 0.792 (0.817) data 0.001 (0.063) loss 6.6421 (6.7373) loss_positive 4.7305 (4.7193) NIS Loss 6.7500 (6.8457) NND Loss -0.0152 (-0.0131) NPD Loss -1.0714 (-1.0772) lr 1.9659e-03 eta 0:06:24
epoch [3/12] batch [35/50] time 0.643 (0.809) data 0.003 (0.055) loss 6.8693 (6.7431) loss_positive 4.8359 (4.7228) NIS Loss 6.9805 (6.8517) NND Loss -0.0158 (-0.0135) NPD Loss -1.1037 (-1.0791) lr 1.9659e-03 eta 0:06:16
epoch [3/12] batch [40/50] time 0.553 (0.785) data 0.000 (0.048) loss 7.0108 (6.7405) loss_positive 4.6641 (4.7254) NIS Loss 7.1250 (6.8496) NND Loss -0.0157 (-0.0138) NPD Loss -1.1339 (-1.0845) lr 1.9659e-03 eta 0:06:01
epoch [3/12] batch [45/50] time 0.557 (0.760) data 0.000 (0.043) loss 6.7709 (6.7396) loss_positive 4.7031 (4.7201) NIS Loss 6.8867 (6.8494) NND Loss -0.0153 (-0.0139) NPD Loss -1.1504 (-1.0913) lr 1.9659e-03 eta 0:05:45
epoch [3/12] batch [50/50] time 0.561 (0.740) data 0.000 (0.038) loss 6.7205 (6.7391) loss_positive 4.7617 (4.7213) NIS Loss 6.8359 (6.8495) NND Loss -0.0161 (-0.0141) NPD Loss -1.1467 (-1.0971) lr 1.8660e-03 eta 0:05:32
epoch [4/12] batch [5/50] time 0.851 (1.366) data 0.007 (0.519) loss 6.7053 (6.6887) loss_positive 4.7070 (4.7406) NIS Loss 6.8203 (6.8039) NND Loss -0.0171 (-0.0166) NPD Loss -1.1420 (-1.1436) lr 1.8660e-03 eta 0:10:07
epoch [4/12] batch [10/50] time 0.859 (1.121) data 0.011 (0.263) loss 7.0021 (6.7552) loss_positive 4.8125 (4.7313) NIS Loss 7.1172 (6.8703) NND Loss -0.0178 (-0.0170) NPD Loss -1.1421 (-1.1427) lr 1.8660e-03 eta 0:08:13
epoch [4/12] batch [15/50] time 0.812 (1.036) data 0.001 (0.176) loss 6.8841 (6.7475) loss_positive 4.7617 (4.7115) NIS Loss 7.0000 (6.8628) NND Loss -0.0179 (-0.0173) NPD Loss -1.1503 (-1.1441) lr 1.8660e-03 eta 0:07:30
epoch [4/12] batch [20/50] time 0.776 (0.990) data 0.001 (0.133) loss 6.6997 (6.7462) loss_positive 4.8438 (4.6947) NIS Loss 6.8164 (6.8617) NND Loss -0.0179 (-0.0175) NPD Loss -1.1578 (-1.1467) lr 1.8660e-03 eta 0:07:05
epoch [4/12] batch [25/50] time 0.577 (0.911) data 0.001 (0.107) loss 6.7380 (6.7396) loss_positive 4.6992 (4.7050) NIS Loss 6.8555 (6.8555) NND Loss -0.0176 (-0.0175) NPD Loss -1.1663 (-1.1500) lr 1.8660e-03 eta 0:06:27
epoch [4/12] batch [30/50] time 0.577 (0.856) data 0.001 (0.089) loss 6.5899 (6.7309) loss_positive 4.7578 (4.6977) NIS Loss 6.7070 (6.8470) NND Loss -0.0182 (-0.0176) NPD Loss -1.1618 (-1.1523) lr 1.8660e-03 eta 0:05:59
epoch [4/12] batch [35/50] time 0.558 (0.818) data 0.002 (0.077) loss 6.7517 (6.7327) loss_positive 4.7227 (4.7136) NIS Loss 6.8672 (6.8489) NND Loss -0.0197 (-0.0177) NPD Loss -1.1446 (-1.1526) lr 1.8660e-03 eta 0:05:39
epoch [4/12] batch [40/50] time 0.659 (0.790) data 0.001 (0.067) loss 6.7680 (6.7308) loss_positive 4.5430 (4.7125) NIS Loss 6.8828 (6.8468) NND Loss -0.0204 (-0.0181) NPD Loss -1.1383 (-1.1509) lr 1.8660e-03 eta 0:05:23
epoch [4/12] batch [45/50] time 0.640 (0.773) data 0.001 (0.060) loss 6.8409 (6.7373) loss_positive 4.7930 (4.7147) NIS Loss 6.9570 (6.8532) NND Loss -0.0204 (-0.0183) NPD Loss -1.1506 (-1.1502) lr 1.8660e-03 eta 0:05:12
epoch [4/12] batch [50/50] time 0.645 (0.759) data 0.001 (0.054) loss 6.6443 (6.7329) loss_positive 4.6875 (4.7173) NIS Loss 6.7617 (6.8490) NND Loss -0.0202 (-0.0185) NPD Loss -1.1645 (-1.1512) lr 1.7071e-03 eta 0:05:03
epoch [5/12] batch [5/50] time 0.602 (1.240) data 0.001 (0.248) loss 6.8118 (6.7621) loss_positive 4.7539 (4.7570) NIS Loss 6.9297 (6.8797) NND Loss -0.0201 (-0.0203) NPD Loss -1.1687 (-1.1661) lr 1.7071e-03 eta 0:08:09
epoch [5/12] batch [10/50] time 0.586 (0.922) data 0.001 (0.125) loss 6.7568 (6.7337) loss_positive 4.7969 (4.7242) NIS Loss 6.8750 (6.8516) NND Loss -0.0199 (-0.0201) NPD Loss -1.1717 (-1.1684) lr 1.7071e-03 eta 0:05:59
epoch [5/12] batch [15/50] time 0.840 (0.873) data 0.012 (0.085) loss 6.6780 (6.7439) loss_positive 4.8359 (4.7289) NIS Loss 6.7969 (6.8620) NND Loss -0.0196 (-0.0200) NPD Loss -1.1785 (-1.1706) lr 1.7071e-03 eta 0:05:36
epoch [5/12] batch [20/50] time 0.839 (0.865) data 0.012 (0.065) loss 6.7687 (6.7458) loss_positive 4.7461 (4.7189) NIS Loss 6.8867 (6.8641) NND Loss -0.0210 (-0.0200) NPD Loss -1.1699 (-1.1722) lr 1.7071e-03 eta 0:05:28
epoch [5/12] batch [25/50] time 0.606 (0.827) data 0.001 (0.053) loss 6.6888 (6.7401) loss_positive 4.6016 (4.7108) NIS Loss 6.8047 (6.8580) NND Loss -0.0228 (-0.0205) NPD Loss -1.1470 (-1.1686) lr 1.7071e-03 eta 0:05:10
epoch [5/12] batch [30/50] time 0.573 (0.786) data 0.001 (0.045) loss 6.5093 (6.7368) loss_positive 4.6602 (4.7203) NIS Loss 6.6250 (6.8543) NND Loss -0.0236 (-0.0209) NPD Loss -1.1448 (-1.1645) lr 1.7071e-03 eta 0:04:50
epoch [5/12] batch [35/50] time 0.549 (0.756) data 0.001 (0.038) loss 6.9730 (6.7470) loss_positive 4.7188 (4.7204) NIS Loss 7.0898 (6.8643) NND Loss -0.0240 (-0.0214) NPD Loss -1.1568 (-1.1627) lr 1.7071e-03 eta 0:04:36
epoch [5/12] batch [40/50] time 0.606 (0.732) data 0.000 (0.034) loss 6.6471 (6.7404) loss_positive 4.7148 (4.7250) NIS Loss 6.7656 (6.8578) NND Loss -0.0239 (-0.0217) NPD Loss -1.1738 (-1.1631) lr 1.7071e-03 eta 0:04:23
epoch [5/12] batch [45/50] time 0.627 (0.720) data 0.001 (0.030) loss 6.7481 (6.7364) loss_positive 4.6719 (4.7251) NIS Loss 6.8672 (6.8540) NND Loss -0.0231 (-0.0219) NPD Loss -1.1795 (-1.1649) lr 1.7071e-03 eta 0:04:15
epoch [5/12] batch [50/50] time 0.625 (0.711) data 0.001 (0.027) loss 6.8464 (6.7350) loss_positive 4.7344 (4.7259) NIS Loss 6.9648 (6.8527) NND Loss -0.0241 (-0.0221) NPD Loss -1.1728 (-1.1659) lr 1.5000e-03 eta 0:04:08
epoch [6/12] batch [5/50] time 0.599 (1.216) data 0.001 (0.431) loss 6.7016 (6.7361) loss_positive 4.6836 (4.6844) NIS Loss 6.8203 (6.8547) NND Loss -0.0242 (-0.0241) NPD Loss -1.1751 (-1.1735) lr 1.5000e-03 eta 0:06:59
epoch [6/12] batch [10/50] time 0.606 (0.910) data 0.001 (0.216) loss 6.7825 (6.7459) loss_positive 4.7266 (4.7238) NIS Loss 6.9023 (6.8648) NND Loss -0.0234 (-0.0240) NPD Loss -1.1865 (-1.1773) lr 1.5000e-03 eta 0:05:09
epoch [6/12] batch [15/50] time 0.776 (0.844) data 0.006 (0.146) loss 7.0000 (6.7761) loss_positive 4.5117 (4.7057) NIS Loss 7.1211 (6.8956) NND Loss -0.0224 (-0.0236) NPD Loss -1.1995 (-1.1832) lr 1.5000e-03 eta 0:04:42
epoch [6/12] batch [20/50] time 0.825 (0.836) data 0.002 (0.110) loss 6.8007 (6.7590) loss_positive 4.5820 (4.6918) NIS Loss 6.9219 (6.8789) NND Loss -0.0223 (-0.0232) NPD Loss -1.2005 (-1.1878) lr 1.5000e-03 eta 0:04:35
epoch [6/12] batch [25/50] time 0.568 (0.803) data 0.001 (0.089) loss 6.5828 (6.7502) loss_positive 4.7422 (4.6859) NIS Loss 6.7031 (6.8703) NND Loss -0.0235 (-0.0232) NPD Loss -1.1914 (-1.1893) lr 1.5000e-03 eta 0:04:21
epoch [6/12] batch [30/50] time 0.575 (0.767) data 0.001 (0.074) loss 6.7666 (6.7365) loss_positive 4.9844 (4.7048) NIS Loss 6.8867 (6.8566) NND Loss -0.0240 (-0.0233) NPD Loss -1.1888 (-1.1893) lr 1.5000e-03 eta 0:04:05
epoch [6/12] batch [35/50] time 0.557 (0.740) data 0.002 (0.064) loss 6.6179 (6.7336) loss_positive 4.4844 (4.7099) NIS Loss 6.7383 (6.8537) NND Loss -0.0237 (-0.0234) NPD Loss -1.1920 (-1.1895) lr 1.5000e-03 eta 0:03:53
epoch [6/12] batch [40/50] time 0.562 (0.717) data 0.000 (0.056) loss 6.7775 (6.7368) loss_positive 4.8711 (4.7115) NIS Loss 6.8984 (6.8569) NND Loss -0.0233 (-0.0234) NPD Loss -1.1973 (-1.1901) lr 1.5000e-03 eta 0:03:42
epoch [6/12] batch [45/50] time 0.654 (0.708) data 0.001 (0.050) loss 6.9488 (6.7405) loss_positive 4.5781 (4.7120) NIS Loss 7.0703 (6.8609) NND Loss -0.0229 (-0.0234) NPD Loss -1.2036 (-1.1913) lr 1.5000e-03 eta 0:03:35
epoch [6/12] batch [50/50] time 0.629 (0.700) data 0.001 (0.045) loss 6.9452 (6.7458) loss_positive 4.7695 (4.7192) NIS Loss 7.0664 (6.8662) NND Loss -0.0242 (-0.0234) NPD Loss -1.2001 (-1.1924) lr 1.2588e-03 eta 0:03:30
epoch [7/12] batch [5/50] time 0.583 (1.131) data 0.000 (0.300) loss 6.7035 (6.6354) loss_positive 4.7227 (4.6680) NIS Loss 6.8242 (6.7562) NND Loss -0.0252 (-0.0249) NPD Loss -1.1943 (-1.1965) lr 1.2588e-03 eta 0:05:33
epoch [7/12] batch [10/50] time 0.580 (0.859) data 0.001 (0.150) loss 6.6373 (6.6750) loss_positive 4.6406 (4.7113) NIS Loss 6.7578 (6.7957) NND Loss -0.0258 (-0.0253) NPD Loss -1.1926 (-1.1946) lr 1.2588e-03 eta 0:04:09
epoch [7/12] batch [15/50] time 0.863 (0.793) data 0.001 (0.101) loss 6.8246 (6.7158) loss_positive 4.6914 (4.7310) NIS Loss 6.9453 (6.8365) NND Loss -0.0260 (-0.0255) NPD Loss -1.1941 (-1.1941) lr 1.2588e-03 eta 0:03:46
epoch [7/12] batch [20/50] time 0.736 (0.786) data 0.001 (0.077) loss 6.7384 (6.7375) loss_positive 4.6953 (4.7221) NIS Loss 6.8594 (6.8582) NND Loss -0.0257 (-0.0256) NPD Loss -1.1971 (-1.1946) lr 1.2588e-03 eta 0:03:40
epoch [7/12] batch [25/50] time 0.578 (0.769) data 0.001 (0.062) loss 6.7107 (6.7250) loss_positive 4.8125 (4.7213) NIS Loss 6.8320 (6.8458) NND Loss -0.0256 (-0.0256) NPD Loss -1.2004 (-1.1955) lr 1.2588e-03 eta 0:03:31
epoch [7/12] batch [30/50] time 0.583 (0.738) data 0.001 (0.052) loss 6.5075 (6.7165) loss_positive 4.6484 (4.7103) NIS Loss 6.6289 (6.8374) NND Loss -0.0258 (-0.0256) NPD Loss -1.2007 (-1.1963) lr 1.2588e-03 eta 0:03:19
epoch [7/12] batch [35/50] time 0.551 (0.716) data 0.001 (0.044) loss 6.7456 (6.7187) loss_positive 4.6797 (4.7054) NIS Loss 6.8672 (6.8397) NND Loss -0.0257 (-0.0256) NPD Loss -1.2033 (-1.1971) lr 1.2588e-03 eta 0:03:09
epoch [7/12] batch [40/50] time 0.613 (0.699) data 0.001 (0.039) loss 6.8587 (6.7093) loss_positive 4.7148 (4.7133) NIS Loss 6.9805 (6.8304) NND Loss -0.0257 (-0.0257) NPD Loss -1.2052 (-1.1980) lr 1.2588e-03 eta 0:03:01
epoch [7/12] batch [45/50] time 0.609 (0.690) data 0.001 (0.035) loss 6.6982 (6.7198) loss_positive 4.6133 (4.7172) NIS Loss 6.8203 (6.8410) NND Loss -0.0252 (-0.0256) NPD Loss -1.2083 (-1.1991) lr 1.2588e-03 eta 0:02:55
epoch [7/12] batch [50/50] time 0.643 (0.684) data 0.001 (0.031) loss 6.6980 (6.7167) loss_positive 4.8516 (4.7190) NIS Loss 6.8203 (6.8380) NND Loss -0.0250 (-0.0256) NPD Loss -1.2102 (-1.2001) lr 1.0000e-03 eta 0:02:50
epoch [8/12] batch [5/50] time 0.572 (1.137) data 0.001 (0.301) loss 6.8695 (6.7681) loss_positive 4.7734 (4.7000) NIS Loss 6.9922 (6.8906) NND Loss -0.0244 (-0.0246) NPD Loss -1.2149 (-1.2127) lr 1.0000e-03 eta 0:04:38
epoch [8/12] batch [10/50] time 0.567 (0.859) data 0.001 (0.151) loss 6.7637 (6.7101) loss_positive 4.6250 (4.7246) NIS Loss 6.8867 (6.8328) NND Loss -0.0239 (-0.0243) NPD Loss -1.2179 (-1.2147) lr 1.0000e-03 eta 0:03:26
epoch [8/12] batch [15/50] time 0.687 (0.805) data 0.009 (0.102) loss 6.6077 (6.7150) loss_positive 4.6680 (4.7286) NIS Loss 6.7305 (6.8378) NND Loss -0.0242 (-0.0242) NPD Loss -1.2155 (-1.2155) lr 1.0000e-03 eta 0:03:09
epoch [8/12] batch [20/50] time 0.748 (0.798) data 0.001 (0.077) loss 6.8422 (6.7298) loss_positive 4.6367 (4.7186) NIS Loss 6.9648 (6.8525) NND Loss -0.0246 (-0.0243) NPD Loss -1.2140 (-1.2152) lr 1.0000e-03 eta 0:03:03
epoch [8/12] batch [25/50] time 0.564 (0.772) data 0.000 (0.063) loss 6.7954 (6.7231) loss_positive 4.6328 (4.7161) NIS Loss 6.9180 (6.8458) NND Loss -0.0247 (-0.0243) NPD Loss -1.2137 (-1.2149) lr 1.0000e-03 eta 0:02:53
epoch [8/12] batch [30/50] time 0.581 (0.740) data 0.001 (0.053) loss 6.6042 (6.7194) loss_positive 4.8008 (4.7148) NIS Loss 6.7266 (6.8421) NND Loss -0.0249 (-0.0244) NPD Loss -1.2116 (-1.2145) lr 1.0000e-03 eta 0:02:42
epoch [8/12] batch [35/50] time 0.613 (0.719) data 0.003 (0.045) loss 6.6471 (6.7247) loss_positive 4.7422 (4.7192) NIS Loss 6.7695 (6.8473) NND Loss -0.0250 (-0.0245) NPD Loss -1.2117 (-1.2141) lr 1.0000e-03 eta 0:02:34
epoch [8/12] batch [40/50] time 0.612 (0.702) data 0.001 (0.040) loss 6.7141 (6.7294) loss_positive 4.7031 (4.7139) NIS Loss 6.8359 (6.8520) NND Loss -0.0261 (-0.0246) NPD Loss -1.2054 (-1.2134) lr 1.0000e-03 eta 0:02:27
epoch [8/12] batch [45/50] time 0.622 (0.693) data 0.001 (0.035) loss 6.8510 (6.7322) loss_positive 4.8047 (4.7144) NIS Loss 6.9727 (6.8547) NND Loss -0.0267 (-0.0249) NPD Loss -1.2029 (-1.2123) lr 1.0000e-03 eta 0:02:22
epoch [8/12] batch [50/50] time 0.633 (0.686) data 0.001 (0.032) loss 6.6672 (6.7311) loss_positive 4.7109 (4.7213) NIS Loss 6.7891 (6.8535) NND Loss -0.0265 (-0.0250) NPD Loss -1.2050 (-1.2114) lr 7.4118e-04 eta 0:02:17
epoch [9/12] batch [5/50] time 0.566 (1.161) data 0.001 (0.328) loss 6.6785 (6.6997) loss_positive 4.6992 (4.7531) NIS Loss 6.8008 (6.8219) NND Loss -0.0261 (-0.0263) NPD Loss -1.2100 (-1.2081) lr 7.4118e-04 eta 0:03:46
epoch [9/12] batch [10/50] time 0.576 (0.874) data 0.001 (0.164) loss 6.7561 (6.7296) loss_positive 4.7148 (4.6965) NIS Loss 6.8789 (6.8520) NND Loss -0.0257 (-0.0261) NPD Loss -1.2149 (-1.2107) lr 7.4118e-04 eta 0:02:46
epoch [9/12] batch [15/50] time 0.771 (0.808) data 0.012 (0.111) loss 6.4706 (6.7329) loss_positive 4.5859 (4.7018) NIS Loss 6.5938 (6.8555) NND Loss -0.0252 (-0.0258) NPD Loss -1.2185 (-1.2129) lr 7.4118e-04 eta 0:02:29
epoch [9/12] batch [20/50] time 0.853 (0.804) data 0.008 (0.084) loss 6.6502 (6.7318) loss_positive 4.6406 (4.7211) NIS Loss 6.7734 (6.8545) NND Loss -0.0251 (-0.0257) NPD Loss -1.2202 (-1.2146) lr 7.4118e-04 eta 0:02:24
epoch [9/12] batch [25/50] time 0.594 (0.788) data 0.001 (0.067) loss 6.6421 (6.7284) loss_positive 4.7188 (4.7278) NIS Loss 6.7656 (6.8513) NND Loss -0.0251 (-0.0256) NPD Loss -1.2223 (-1.2160) lr 7.4118e-04 eta 0:02:17
epoch [9/12] batch [30/50] time 0.827 (0.767) data 0.001 (0.056) loss 6.7398 (6.7257) loss_positive 4.6367 (4.7357) NIS Loss 6.8633 (6.8487) NND Loss -0.0252 (-0.0255) NPD Loss -1.2225 (-1.2171) lr 7.4118e-04 eta 0:02:10
epoch [9/12] batch [35/50] time 0.559 (0.743) data 0.001 (0.048) loss 6.5329 (6.7182) loss_positive 4.8828 (4.7377) NIS Loss 6.6562 (6.8413) NND Loss -0.0254 (-0.0254) NPD Loss -1.2208 (-1.2178) lr 7.4118e-04 eta 0:02:02
epoch [9/12] batch [40/50] time 0.618 (0.721) data 0.000 (0.042) loss 6.6855 (6.7219) loss_positive 4.6445 (4.7354) NIS Loss 6.8086 (6.8449) NND Loss -0.0255 (-0.0254) NPD Loss -1.2181 (-1.2179) lr 7.4118e-04 eta 0:01:55
epoch [9/12] batch [45/50] time 0.621 (0.709) data 0.001 (0.038) loss 6.9947 (6.7273) loss_positive 4.7109 (4.7283) NIS Loss 7.1172 (6.8503) NND Loss -0.0263 (-0.0255) NPD Loss -1.2122 (-1.2176) lr 7.4118e-04 eta 0:01:49
epoch [9/12] batch [50/50] time 0.622 (0.701) data 0.001 (0.034) loss 6.7567 (6.7262) loss_positive 4.8281 (4.7241) NIS Loss 6.8789 (6.8491) NND Loss -0.0269 (-0.0256) NPD Loss -1.2086 (-1.2168) lr 5.0000e-04 eta 0:01:45
epoch [10/12] batch [5/50] time 0.570 (1.141) data 0.001 (0.306) loss 6.8583 (6.8067) loss_positive 4.6719 (4.7266) NIS Loss 6.9805 (6.9289) NND Loss -0.0270 (-0.0269) NPD Loss -1.2085 (-1.2085) lr 5.0000e-04 eta 0:02:45
epoch [10/12] batch [10/50] time 0.595 (0.874) data 0.001 (0.154) loss 6.8192 (6.7434) loss_positive 4.5898 (4.7395) NIS Loss 6.9414 (6.8656) NND Loss -0.0269 (-0.0269) NPD Loss -1.2089 (-1.2087) lr 5.0000e-04 eta 0:02:02
epoch [10/12] batch [15/50] time 0.793 (0.812) data 0.001 (0.103) loss 6.7879 (6.7509) loss_positive 4.5781 (4.7349) NIS Loss 6.9102 (6.8732) NND Loss -0.0269 (-0.0269) NPD Loss -1.2094 (-1.2089) lr 5.0000e-04 eta 0:01:49
epoch [10/12] batch [20/50] time 0.774 (0.802) data 0.001 (0.078) loss 6.6120 (6.7447) loss_positive 4.6875 (4.7393) NIS Loss 6.7344 (6.8670) NND Loss -0.0267 (-0.0269) NPD Loss -1.2106 (-1.2092) lr 5.0000e-04 eta 0:01:44
epoch [10/12] batch [25/50] time 0.785 (0.795) data 0.001 (0.062) loss 6.9087 (6.7464) loss_positive 4.7812 (4.7433) NIS Loss 7.0312 (6.8688) NND Loss -0.0267 (-0.0268) NPD Loss -1.2119 (-1.2096) lr 5.0000e-04 eta 0:01:39
epoch [10/12] batch [30/50] time 0.579 (0.764) data 0.001 (0.052) loss 6.7721 (6.7324) loss_positive 4.4961 (4.7313) NIS Loss 6.8945 (6.8547) NND Loss -0.0266 (-0.0268) NPD Loss -1.2112 (-1.2099) lr 5.0000e-04 eta 0:01:31
epoch [10/12] batch [35/50] time 0.551 (0.737) data 0.001 (0.045) loss 6.7486 (6.7313) loss_positive 4.6992 (4.7383) NIS Loss 6.8711 (6.8537) NND Loss -0.0266 (-0.0268) NPD Loss -1.2117 (-1.2102) lr 5.0000e-04 eta 0:01:24
epoch [10/12] batch [40/50] time 0.595 (0.717) data 0.001 (0.039) loss 6.8657 (6.7377) loss_positive 4.6992 (4.7316) NIS Loss 6.9883 (6.8601) NND Loss -0.0264 (-0.0267) NPD Loss -1.2129 (-1.2104) lr 5.0000e-04 eta 0:01:18
epoch [10/12] batch [45/50] time 0.628 (0.707) data 0.001 (0.035) loss 6.6547 (6.7443) loss_positive 4.8086 (4.7293) NIS Loss 6.7773 (6.8667) NND Loss -0.0265 (-0.0267) NPD Loss -1.2130 (-1.2107) lr 5.0000e-04 eta 0:01:14
epoch [10/12] batch [50/50] time 0.623 (0.699) data 0.001 (0.032) loss 6.7172 (6.7464) loss_positive 4.7070 (4.7256) NIS Loss 6.8398 (6.8688) NND Loss -0.0265 (-0.0267) NPD Loss -1.2137 (-1.2110) lr 2.9289e-04 eta 0:01:09
epoch [11/12] batch [5/50] time 0.587 (1.190) data 0.001 (0.422) loss 6.7757 (6.7015) loss_positive 4.7227 (4.7578) NIS Loss 6.8984 (6.8242) NND Loss -0.0265 (-0.0265) NPD Loss -1.2138 (-1.2138) lr 2.9289e-04 eta 0:01:53
epoch [11/12] batch [10/50] time 0.592 (0.887) data 0.001 (0.211) loss 6.9124 (6.7109) loss_positive 4.7109 (4.7801) NIS Loss 7.0352 (6.8336) NND Loss -0.0265 (-0.0265) NPD Loss -1.2139 (-1.2139) lr 2.9289e-04 eta 0:01:19
epoch [11/12] batch [15/50] time 0.808 (0.823) data 0.001 (0.142) loss 6.6585 (6.7109) loss_positive 4.6133 (4.7458) NIS Loss 6.7812 (6.8336) NND Loss -0.0264 (-0.0265) NPD Loss -1.2143 (-1.2140) lr 2.9289e-04 eta 0:01:09
epoch [11/12] batch [20/50] time 0.845 (0.816) data 0.001 (0.107) loss 6.7484 (6.7345) loss_positive 4.6133 (4.7295) NIS Loss 6.8711 (6.8572) NND Loss -0.0266 (-0.0265) NPD Loss -1.2138 (-1.2140) lr 2.9289e-04 eta 0:01:05
epoch [11/12] batch [25/50] time 0.597 (0.813) data 0.000 (0.087) loss 6.8070 (6.7318) loss_positive 4.7656 (4.7319) NIS Loss 6.9297 (6.8545) NND Loss -0.0266 (-0.0265) NPD Loss -1.2137 (-1.2140) lr 2.9289e-04 eta 0:01:00
epoch [11/12] batch [30/50] time 0.584 (0.776) data 0.001 (0.073) loss 6.6390 (6.7291) loss_positive 4.6719 (4.7241) NIS Loss 6.7617 (6.8518) NND Loss -0.0266 (-0.0265) NPD Loss -1.2143 (-1.2140) lr 2.9289e-04 eta 0:00:54
epoch [11/12] batch [35/50] time 0.550 (0.748) data 0.001 (0.062) loss 6.7131 (6.7102) loss_positive 4.7773 (4.7146) NIS Loss 6.8359 (6.8329) NND Loss -0.0266 (-0.0265) NPD Loss -1.2149 (-1.2141) lr 2.9289e-04 eta 0:00:48
epoch [11/12] batch [40/50] time 0.552 (0.725) data 0.000 (0.055) loss 6.6467 (6.7019) loss_positive 4.6953 (4.7183) NIS Loss 6.7695 (6.8246) NND Loss -0.0264 (-0.0265) NPD Loss -1.2154 (-1.2142) lr 2.9289e-04 eta 0:00:43
epoch [11/12] batch [45/50] time 0.642 (0.712) data 0.000 (0.049) loss 6.9474 (6.7087) loss_positive 4.7500 (4.7179) NIS Loss 7.0703 (6.8314) NND Loss -0.0263 (-0.0265) NPD Loss -1.2164 (-1.2144) lr 2.9289e-04 eta 0:00:39
epoch [11/12] batch [50/50] time 0.626 (0.704) data 0.001 (0.044) loss 6.8614 (6.7143) loss_positive 4.7852 (4.7167) NIS Loss 6.9844 (6.8371) NND Loss -0.0263 (-0.0265) NPD Loss -1.2166 (-1.2146) lr 1.3397e-04 eta 0:00:35
epoch [12/12] batch [5/50] time 0.587 (1.158) data 0.001 (0.408) loss 6.6426 (6.6981) loss_positive 4.5234 (4.6797) NIS Loss 6.7656 (6.8211) NND Loss -0.0263 (-0.0263) NPD Loss -1.2169 (-1.2169) lr 1.3397e-04 eta 0:00:52
epoch [12/12] batch [10/50] time 0.729 (0.894) data 0.006 (0.205) loss 6.7676 (6.7309) loss_positive 4.8398 (4.6988) NIS Loss 6.8906 (6.8539) NND Loss -0.0263 (-0.0263) NPD Loss -1.2171 (-1.2169) lr 1.3397e-04 eta 0:00:35
epoch [12/12] batch [15/50] time 0.832 (0.880) data 0.017 (0.139) loss 6.6348 (6.7322) loss_positive 4.9609 (4.7286) NIS Loss 6.7578 (6.8552) NND Loss -0.0262 (-0.0263) NPD Loss -1.2173 (-1.2170) lr 1.3397e-04 eta 0:00:30
epoch [12/12] batch [20/50] time 0.873 (0.884) data 0.008 (0.107) loss 6.7285 (6.7244) loss_positive 4.6211 (4.7342) NIS Loss 6.8516 (6.8475) NND Loss -0.0262 (-0.0263) NPD Loss -1.2172 (-1.2171) lr 1.3397e-04 eta 0:00:26
epoch [12/12] batch [25/50] time 0.891 (0.880) data 0.004 (0.088) loss 6.8887 (6.7384) loss_positive 4.7227 (4.7294) NIS Loss 7.0117 (6.8614) NND Loss -0.0262 (-0.0263) NPD Loss -1.2171 (-1.2171) lr 1.3397e-04 eta 0:00:21
epoch [12/12] batch [30/50] time 0.569 (0.864) data 0.000 (0.073) loss 6.7324 (6.7413) loss_positive 4.7656 (4.7473) NIS Loss 6.8555 (6.8643) NND Loss -0.0262 (-0.0263) NPD Loss -1.2173 (-1.2171) lr 1.3397e-04 eta 0:00:17
epoch [12/12] batch [35/50] time 0.548 (0.823) data 0.001 (0.063) loss 6.6660 (6.7309) loss_positive 4.8359 (4.7393) NIS Loss 6.7891 (6.8539) NND Loss -0.0262 (-0.0262) NPD Loss -1.2176 (-1.2172) lr 1.3397e-04 eta 0:00:12
epoch [12/12] batch [40/50] time 0.547 (0.789) data 0.000 (0.055) loss 6.7129 (6.7287) loss_positive 4.5586 (4.7336) NIS Loss 6.8359 (6.8518) NND Loss -0.0262 (-0.0262) NPD Loss -1.2175 (-1.2172) lr 1.3397e-04 eta 0:00:07
epoch [12/12] batch [45/50] time 0.566 (0.763) data 0.000 (0.049) loss 6.6660 (6.7285) loss_positive 4.7578 (4.7260) NIS Loss 6.7891 (6.8516) NND Loss -0.0262 (-0.0262) NPD Loss -1.2175 (-1.2172) lr 1.3397e-04 eta 0:00:03
epoch [12/12] batch [50/50] time 0.653 (0.746) data 0.001 (0.044) loss 6.7285 (6.7319) loss_positive 4.5273 (4.7220) NIS Loss 6.8516 (6.8549) NND Loss -0.0261 (-0.0262) NPD Loss -1.2177 (-1.2173) lr 3.4074e-05 eta 0:00:00
Checkpoint saved to output/imagenet_openood/NegPrompt/vit_b16_ep12_16shots/nctx16nega_ctx2/seed1/prompt_learner/model.pth.tar-12
Finish training
Deploy the last-epoch model
Calling CoOp_works\CoOp\trainers\negprompt.NegPrompt.test
Shape of _pred_k:  (5000, 100)
Shape of _pred_u:  (17632, 100)
AUROC: 0.89727, AUPR: 0.72356, FPR95: 0.42843
Elapsed: 0:10:05
